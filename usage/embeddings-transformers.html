<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="sitemap" type="application/xml" href="/sitemap.xml"/><link rel="shortcut icon" href="/icons/icon-192x192.png"/><link rel="manifest" href="/manifest.webmanifest"/><meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=5.0, shrink-to-fit=no, viewport-fit=cover"/><meta name="theme-color" content="#09a3d5"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png"/><title>Embeddings, Transformers and Transfer Learning ¬∑ spaCy Usage Documentation</title><meta name="description" content="Using transformer embeddings like BERT in spaCy"/><meta property="og:title" content="Embeddings, Transformers and Transfer Learning ¬∑ spaCy Usage Documentation"/><meta property="og:description" content="Using transformer embeddings like BERT in spaCy"/><meta property="og:type" content="website"/><meta property="og:site_name" content="Embeddings, Transformers and Transfer Learning"/><meta property="og:image" content="https://spacy.io/_next/static/media/social_default.96b04585.jpg"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image" content="https://spacy.io/_next/static/media/social_default.96b04585.jpg"/><meta name="twitter:creator" content="@spacy_io"/><meta name="twitter:site" content="@spacy_io"/><meta name="twitter:title" content="Embeddings, Transformers and Transfer Learning ¬∑ spaCy Usage Documentation"/><meta name="twitter:description" content="Using transformer embeddings like BERT in spaCy"/><meta name="docsearch:language" content="en"/><meta name="next-head-count" content="24"/><link rel="preload" href="/_next/static/css/8f0b94edbc18d62d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8f0b94edbc18d62d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e6995e0e8addcf99.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e6995e0e8addcf99.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script defer="" src="/_next/static/chunks/262.c647d33d06232ef6.js"></script><script defer="" src="/_next/static/chunks/728.cf6ba0da2700fa1b.js"></script><script src="/_next/static/chunks/webpack-8161fc2bb14cec39.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-a0f603ce323043fd.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eb3ea1261af64e73.js" defer=""></script><script src="/_next/static/chunks/94-57434c8b7a6c3878.js" defer=""></script><script src="/_next/static/chunks/128-76b45627a109219b.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...listPathPage%5D-45eea57fe8c2902c.js" defer=""></script><script src="/_next/static/Ugre-usgT1EZhnSeYcBR9/_buildManifest.js" defer=""></script><script src="/_next/static/Ugre-usgT1EZhnSeYcBR9/_ssgManifest.js" defer=""></script></head><body class="theme-blue"><div id="__next"><div class="theme-blue"><nav class="navigation_root__yPL8O"><span class="navigation_has-alert__s0Drf"><a class="link_root__1Me7D link_no-link-layout__RPvod" aria-label="spaCy" href="/"><h1 class="navigation_title__pm49s">spaCy</h1></a> <span class="navigation_alert__ZOXon"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/usage/v3-5"><strong>üí• Out now:</strong> spaCy v3.5</a></span></span><div class="navigation_menu__ZMJxN"><select class="dropdown_root__3uiQq navigation_dropdown__4j4pI"><option value="title" disabled="">Menu</option><option value="/usage" selected="">Usage</option><option value="/models">Models</option><option value="/api">API</option><option value="/universe">Universe</option></select><ul class="navigation_list__DCzqi"><li class="navigation_item__ln1O1 navigation_is-active__RjVJG"><a class="link_root__1Me7D link_no-link-layout__RPvod" tabindex="-1" href="/usage">Usage</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/models">Models</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/api">API</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/universe">Universe</a></li><li class="navigation_item__ln1O1 navigation_github__MpFNv"><span><a href="https://github.com/explosion/spaCy" data-size="large" data-show-count="true" aria-label="Star spaCy on GitHub"></a></span></li></ul><div class="navigation_search__BKZCn"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div><progress class="progress_root__9huWN" value="0" max="100"></progress></nav><menu class="sidebar sidebar_root__s2No7"><h1 hidden="" aria-hidden="true" class="h0 sidebar_active-heading___dkf9">Guides</h1><div class="sidebar_dropdown__vyqjz"><select class="dropdown_root__3uiQq sidebar_dropdown-select__Nwbq9"><option disabled="">Select page...</option><option value="/usage">Get started<!-- --> ‚Ä∫ <!-- -->Installation</option><option value="/usage/models">Get started<!-- --> ‚Ä∫ <!-- -->Models &amp; Languages</option><option value="/usage/facts-figures">Get started<!-- --> ‚Ä∫ <!-- -->Facts &amp; Figures</option><option value="/usage/spacy-101">Get started<!-- --> ‚Ä∫ <!-- -->spaCy 101</option><option value="/usage/v3">Get started<!-- --> ‚Ä∫ <!-- -->New in v3.0</option><option value="/usage/v3-1">Get started<!-- --> ‚Ä∫ <!-- -->New in v3.1</option><option value="/usage/v3-2">Get started<!-- --> ‚Ä∫ <!-- -->New in v3.2</option><option value="/usage/v3-3">Get started<!-- --> ‚Ä∫ <!-- -->New in v3.3</option><option value="/usage/v3-4">Get started<!-- --> ‚Ä∫ <!-- -->New in v3.4</option><option value="/usage/v3-5">Get started<!-- --> ‚Ä∫ <!-- -->New in v3.5</option><option value="/usage/linguistic-features">Guides<!-- --> ‚Ä∫ <!-- -->Linguistic Features</option><option value="/usage/rule-based-matching">Guides<!-- --> ‚Ä∫ <!-- -->Rule-based Matching</option><option value="/usage/processing-pipelines">Guides<!-- --> ‚Ä∫ <!-- -->Processing Pipelines</option><option value="/usage/embeddings-transformers" selected="">Guides<!-- --> ‚Ä∫ <!-- -->Embeddings &amp; Transformers</option><option value="/usage/training">Guides<!-- --> ‚Ä∫ <!-- -->Training Models</option><option value="/usage/layers-architectures">Guides<!-- --> ‚Ä∫ <!-- -->Layers &amp; Model Architectures</option><option value="/usage/projects">Guides<!-- --> ‚Ä∫ <!-- -->spaCy Projects</option><option value="/usage/saving-loading">Guides<!-- --> ‚Ä∫ <!-- -->Saving &amp; Loading</option><option value="/usage/visualizers">Guides<!-- --> ‚Ä∫ <!-- -->Visualizers</option><option value="https://github.com/explosion/projects">Resources<!-- --> ‚Ä∫ <!-- -->Project Templates</option><option value="https://v2.spacy.io">Resources<!-- --> ‚Ä∫ <!-- -->v2.x Documentation</option><option value="https://explosion.ai/custom-solutions">Resources<!-- --> ‚Ä∫ <!-- -->Custom Solutions</option></select></div><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Get started</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage">Installation</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/models">Models &amp; Languages</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/facts-figures">Facts &amp; Figures</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/spacy-101">spaCy 101</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3">New in v3.0</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-1">New in v3.1</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-2">New in v3.2</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-3">New in v3.3</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-4">New in v3.4</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-5">New in v3.5</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Guides</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/linguistic-features">Linguistic Features</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/rule-based-matching">Rule-based Matching</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/processing-pipelines">Processing Pipelines</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP sidebar_is-active__yVTtL is-active" href="/usage/embeddings-transformers">Embeddings &amp; Transformers<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a><ul class="sidebar_crumbs__NhM2y"><li class="sidebar_crumb__tiiDl sidebar_crumb-active__zq8BI"><a href="#embedding-layers">Embedding Layers</a></li><li class="sidebar_crumb__tiiDl"><a href="#transformers">Transformers</a></li><li class="sidebar_crumb__tiiDl"><a href="#static-vectors">Static Vectors</a></li><li class="sidebar_crumb__tiiDl"><a href="#pretraining">Pretraining</a></li></ul></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/training">Training Models<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/layers-architectures">Layers &amp; Model Architectures<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/projects">spaCy Projects<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/saving-loading">Saving &amp; Loading</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/visualizers">Visualizers</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Resources</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/projects">Project Templates</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="https://v2.spacy.io">v2.x Documentation</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="https://explosion.ai/custom-solutions">Custom Solutions</a></li></ul></menu><main class="main_root__7f6Tj main_with-sidebar__uH1df main_with-asides__ikQT6"><article class="main_content__8zFCH"><header class="title_root__pS2WQ"><h1 id="_title" class="typography_heading__D82WZ typography_h1__b7dt9 title_h1__l3CW1"><span class="heading-text">Embeddings, Transformers and Transfer Learning<!-- --> </span></h1><div class="heading-teaser title_teaser__QhwCH">Using transformer embeddings like BERT in spaCy</div></header><section class="section_root__k1hUl"><p>spaCy supports a number of <strong>transfer and multi-task learning</strong> workflows that
can often help improve your pipeline‚Äôs efficiency or accuracy. Transfer learning
refers to techniques such as word vector tables and language model pretraining.
These techniques can be used to import knowledge from raw text into your
pipeline, so that your models are able to generalize better from your annotated
examples.</p><p>You can convert <strong>word vectors</strong> from popular tools like
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://fasttext.cc">FastText</a> and <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://radimrehurek.com/gensim">Gensim</a>,
or you can load in any pretrained <strong>transformer model</strong> if you install
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spacy-transformers"><code class="code_inline-code__Bq7ot">spacy-transformers</code></a>. You can
also do your own language model pretraining via the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#pretrain"><code class="code_inline-code__Bq7ot">spacy pretrain</code></a> command. You can even <strong>share</strong> your
transformer or other contextual embedding model across multiple components,
which can make long pipelines several times more efficient. To use transfer
learning, you‚Äôll need at least a few annotated examples for what you‚Äôre trying
to predict. Otherwise, you could try using a ‚Äúone-shot learning‚Äù approach using
<a class="link_root__1Me7D" href="/usage/linguistic-features#vectors-similarity">vectors and similarity</a>.</p><section class="accordion" id="vectors-vs-language-models"><div class="accordion_root__pPltq"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">What‚Äôs the difference between word vectors and language models?</span><a class="link_root__1Me7D accordion_anchor__kidBh link_no-link-layout__RPvod" href="/usage/embeddings-transformers#vectors-vs-language-models">¬∂</a></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><p><a class="link_root__1Me7D" href="/usage/embeddings-transformers#transformers">Transformers</a> are large and powerful neural networks that give
you better accuracy, but are harder to deploy in production, as they require a
GPU to run effectively. <a class="link_root__1Me7D" href="/usage/embeddings-transformers#word-vectors">Word vectors</a> are a slightly older
technique that can give your models a smaller improvement in accuracy, and can
also provide some additional capabilities.</p><p>The key difference between word-vectors and contextual language models such as
transformers is that word vectors model <strong>lexical types</strong>, rather than <em>tokens</em>.
If you have a list of terms with no context around them, a transformer model
like BERT can‚Äôt really help you. BERT is designed to understand language <strong>in
context</strong>, which isn‚Äôt what you have. A word vectors table will be a much better
fit for your task. However, if you do have words in context ‚Äì whole sentences or
paragraphs of running text ‚Äì word vectors will only provide a very rough
approximation of what the text is about.</p><p>Word vectors are also very computationally efficient, as they map a word to a
vector with a single indexing operation. Word vectors are therefore useful as a
way to <strong>improve the accuracy</strong> of neural network models, especially models that
are small or have received little or no pretraining. In spaCy, word vector
tables are only used as <strong>static features</strong>. spaCy does not backpropagate
gradients to the pretrained word vectors table. The static vectors table is
usually used in combination with a smaller table of learned task-specific
embeddings.</p></div></div></section><section class="accordion"><div class="accordion_root__pPltq"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">When should I add word vectors to my model?</span></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><p>Word vectors are not compatible with most <a class="link_root__1Me7D" href="/usage/embeddings-transformers#transformers">transformer models</a>,
but if you‚Äôre training another type of NLP network, it‚Äôs almost always worth
adding word vectors to your model. As well as improving your final accuracy,
word vectors often make experiments more consistent, as the accuracy you reach
will be less sensitive to how the network is randomly initialized. High variance
due to random chance can slow down your progress significantly, as you need to
run many experiments to filter the signal from the noise.</p><p>Word vector features need to be enabled prior to training, and the same word
vectors table will need to be available at runtime as well. You cannot add word
vector features once the model has already been trained, and you usually cannot
replace one word vectors table with another without causing a significant loss
of performance.</p></div></div></section></section>
<section id="section-embedding-layers" class="section_root__k1hUl"><h2 id="embedding-layers" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#embedding-layers" class="heading-text typography_permalink__UiIRy">Shared embedding layers <!-- --> </a></h2><p>spaCy lets you share a single transformer or other token-to-vector (‚Äútok2vec‚Äù)
embedding layer between multiple components. You can even update the shared
layer, performing <strong>multi-task learning</strong>. Reusing the tok2vec layer between
components can make your pipeline run a lot faster and result in much smaller
models. However, it can make the pipeline less modular and make it more
difficult to swap components or retrain parts of the pipeline. Multi-task
learning can affect your accuracy (either positively or negatively), and may
require some retuning of your hyper-parameters.</p><figure class="gatsby-resp-image-figure"><img class="embed_image__mSQUH" src="/images/tok2vec.svg" alt="Pipeline components using a shared embedding component vs. independent embedding layers" width="650" height="auto"/></figure><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Shared</th><th class="table_th__QJ9F8">Independent</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>smaller:</strong> models only need to include a single copy of the embeddings</td><td class="table_td__rmpJx"><strong>larger:</strong> models need to include the embeddings for each component</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>faster:</strong> embed the documents once for your whole pipeline</td><td class="table_td__rmpJx"><strong>slower:</strong> rerun the embedding for each component</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>less composable:</strong> all components require the same embedding component in the pipeline</td><td class="table_td__rmpJx"><strong>modular:</strong> components can be moved and swapped freely</td></tr></tbody></table><p>You can share a single transformer or other tok2vec model between multiple
components by adding a <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/transformer"><code class="code_inline-code__Bq7ot">Transformer</code></a> or
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/tok2vec"><code class="code_inline-code__Bq7ot">Tok2Vec</code></a> component near the start of your pipeline. Components
later in the pipeline can ‚Äúconnect‚Äù to it by including a <strong>listener layer</strong> like
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#Tok2VecListener"><span class="link_source-text__VDP74">Tok2VecListener</span></a> within their model.</p><figure class="gatsby-resp-image-figure"><img class="embed_image__mSQUH" src="/images/tok2vec-listener.svg" alt="Pipeline components listening to shared embedding component" width="650" height="auto"/></figure><p>At the beginning of training, the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/tok2vec"><code class="code_inline-code__Bq7ot">Tok2Vec</code></a> component will grab
a reference to the relevant listener layers in the rest of your pipeline. When
it processes a batch of documents, it will pass forward its predictions to the
listeners, allowing the listeners to <strong>reuse the predictions</strong> when they are
eventually called. A similar mechanism is used to pass gradients from the
listeners back to the model. The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/transformer"><code class="code_inline-code__Bq7ot">Transformer</code></a> component and
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#TransformerListener"><span class="link_source-text__VDP74">TransformerListener</span></a> layer do the same
thing for transformer models, but the <code class="code_inline-code__Bq7ot">Transformer</code> component will also save the
transformer outputs to the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/transformer#custom_attributes"><code class="code_inline-code__Bq7ot">Doc._.trf_data</code></a> extension attribute,
giving you access to them after the pipeline has finished running.</p><h3 id="embedding-layers-config" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#embedding-layers-config" class="heading-text typography_permalink__UiIRy">Example: Shared vs. independent config <!-- --> </a></h3><p>The <a class="link_root__1Me7D" href="/usage/training#config">config system</a> lets you express model configuration
for both shared and independent embedding layers. The shared setup uses a single
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/tok2vec"><code class="code_inline-code__Bq7ot">Tok2Vec</code></a> component with the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#Tok2Vec"><span class="link_source-text__VDP74">Tok2Vec</span></a> architecture. All other components, like
the entity recognizer, use a
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#Tok2VecListener"><span class="link_source-text__VDP74">Tok2VecListener</span></a> layer as their model‚Äôs
<code class="code_inline-code__Bq7ot">tok2vec</code> argument, which connects to the <code class="code_inline-code__Bq7ot">tok2vec</code> component model.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Shared</h4><code class="code_code__CILJL language-ini language-ini code_wrap__b41os"></code></pre><p>In the independent setup, the entity recognizer component defines its own
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#Tok2Vec"><span class="link_source-text__VDP74">Tok2Vec</span></a> instance. Other components will do the
same. This makes them fully independent and doesn‚Äôt require an upstream
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/tok2vec"><code class="code_inline-code__Bq7ot">Tok2Vec</code></a> component to be present in the pipeline.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Independent</h4><code class="code_code__CILJL language-ini language-ini code_wrap__b41os"></code></pre></section>
<section id="section-transformers" class="section_root__k1hUl"><h2 id="transformers" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#transformers" class="heading-text typography_permalink__UiIRy">Using transformer models <!-- --> </a></h2><p>Transformers are a family of neural network architectures that compute <strong>dense,
context-sensitive representations</strong> for the tokens in your documents. Downstream
models in your pipeline can then use these representations as input features to
<strong>improve their predictions</strong>. You can connect multiple components to a single
transformer model, with any or all of those components giving feedback to the
transformer to fine-tune it to your tasks. spaCy‚Äôs transformer support
interoperates with <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://pytorch.org">PyTorch</a> and the
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://huggingface.co/transformers/">HuggingFace <code class="code_inline-code__Bq7ot">transformers</code></a> library,
giving you access to thousands of pretrained models for your pipelines. There
are many <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="http://jalammar.github.io/illustrated-transformer/">great guides</a> to
transformer models, but for practical purposes, you can simply think of them as
drop-in replacements that let you achieve <strong>higher accuracy</strong> in exchange for
<strong>higher training and runtime costs</strong>.</p><h3 id="transformers-installation" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#transformers-installation" class="heading-text typography_permalink__UiIRy">Setup and installation <!-- --> </a></h3><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">System requirements<!-- --> </span></h4>
<p>We recommend an NVIDIA <strong>GPU</strong> with at least <strong>10GB of memory</strong> in order to
work with transformer models. Make sure your GPU drivers are up to date and
you have <strong>CUDA v9+</strong> installed.</p>
</div></div></aside><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<p>The exact requirements will depend on the transformer model. Training a
transformer-based model without a GPU will be too slow for most practical
purposes.</p>
<p>Provisioning a new machine will require about <strong>5GB</strong> of data to be
downloaded: 3GB CUDA runtime, 800MB PyTorch, 400MB CuPy, 500MB weights, 200MB
spaCy and dependencies.</p>
</div></div></aside><p>Once you have CUDA installed, we recommend installing PyTorch following the
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://pytorch.org/get-started/locally/">PyTorch installation guidelines</a> for
your package manager and CUDA version. If you skip this step, pip will install
PyTorch as a dependency below, but it may not find the best version for your
setup.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Example: Install PyTorch 1.11.0 for CUDA 11.3 with pip</h4><code class="code_code__CILJL language-bash language-bash"></code></pre><p>Next, install spaCy with the extras for your CUDA version and transformers. The
CUDA extra (e.g., <code class="code_inline-code__Bq7ot">cuda102</code>, <code class="code_inline-code__Bq7ot">cuda113</code>) installs the correct version of
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.cupy.dev/en/stable/install.html#installing-cupy"><code class="code_inline-code__Bq7ot">cupy</code></a>, which is
just like <code class="code_inline-code__Bq7ot">numpy</code>, but for GPU. You may also need to set the <code class="code_inline-code__Bq7ot">CUDA_PATH</code>
environment variable if your CUDA runtime is installed in a non-standard
location. Putting it all together, if you had installed CUDA 11.3 in
<code class="code_inline-code__Bq7ot">/opt/nvidia/cuda</code>, you would run:</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Installation with CUDA</h4><code class="code_code__CILJL language-bash language-bash"></code></pre><p>For <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://huggingface.co/transformers/"><code class="code_inline-code__Bq7ot">transformers</code></a> v4.0.0+ and models
that require <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/google/sentencepiece"><code class="code_inline-code__Bq7ot">SentencePiece</code></a> (e.g.,
ALBERT, CamemBERT, XLNet, Marian, and T5), install the additional dependencies
with:</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Install sentencepiece</h4><code class="code_code__CILJL language-bash language-bash"></code></pre><h3 id="transformers-runtime" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#transformers-runtime" class="heading-text typography_permalink__UiIRy">Runtime usage <!-- --> </a></h3><p>Transformer models can be used as <strong>drop-in replacements</strong> for other types of
neural networks, so your spaCy pipeline can include them in a way that‚Äôs
completely invisible to the user. Users will download, load and use the model in
the standard way, like any other spaCy pipeline. Instead of using the
transformers as subnetworks directly, you can also use them via the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/transformer"><code class="code_inline-code__Bq7ot">Transformer</code></a> pipeline component.</p><figure class="gatsby-resp-image-figure"><img class="embed_image__mSQUH" src="/images/pipeline_transformer.svg" alt="The processing pipeline with the transformer component" width="650" height="auto"/></figure><p>The <code class="code_inline-code__Bq7ot">Transformer</code> component sets the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/transformer#custom_attributes"><code class="code_inline-code__Bq7ot">Doc._.trf_data</code></a> extension attribute,
which lets you access the transformers outputs at runtime. The trained
transformer-based <a class="link_root__1Me7D" href="/models">pipelines</a> provided by spaCy end on <code class="code_inline-code__Bq7ot">_trf</code>, e.g.
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/models/en#en_core_web_trf"><code class="code_inline-code__Bq7ot">en_core_web_trf</code></a>.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Example</h4><code class="code_code__CILJL language-python language-python"></code></pre><p>You can also customize how the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/transformer"><code class="code_inline-code__Bq7ot">Transformer</code></a> component sets
annotations onto the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> by specifying a custom
<code class="code_inline-code__Bq7ot">set_extra_annotations</code> function. This callback will be called with the raw
input and output data for the whole batch, along with the batch of <code class="code_inline-code__Bq7ot">Doc</code>
objects, allowing you to implement whatever you need. The annotation setter is
called with a batch of <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> objects and a
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/transformer#fulltransformerbatch"><code class="code_inline-code__Bq7ot">FullTransformerBatch</code></a> containing the
transformers data for the batch.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><h3 id="transformers-training" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#transformers-training" class="heading-text typography_permalink__UiIRy">Training usage <!-- --> </a></h3><p>The recommended workflow for training is to use spaCy‚Äôs
<a class="link_root__1Me7D" href="/usage/training#config">config system</a>, usually via the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a> command. The training config defines all
component settings and hyperparameters in one place and lets you describe a tree
of objects by referring to creation functions, including functions you register
yourself. For details on how to get started with training your own model, check
out the <a class="link_root__1Me7D" href="/usage/training#quickstart">training quickstart</a>.</p><p>The <code class="code_inline-code__Bq7ot">[components]</code> section in the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/data-formats#config"><code class="code_inline-code__Bq7ot">config.cfg</code></a>
describes the pipeline components and the settings used to construct them,
including their model implementation. Here‚Äôs a config snippet for the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/transformer"><code class="code_inline-code__Bq7ot">Transformer</code></a> component, along with matching Python code. In
this case, the <code class="code_inline-code__Bq7ot">[components.transformer]</code> block describes the <code class="code_inline-code__Bq7ot">transformer</code>
component:</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Python equivalent<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">config.cfg</h4><code class="code_code__CILJL language-ini language-ini"></code></pre><p>The <code class="code_inline-code__Bq7ot code_wrap__b41os">[components.transformer.model]</code> block describes the <code class="code_inline-code__Bq7ot">model</code> argument passed
to the transformer component. It‚Äôs a Thinc
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-model"><code class="code_inline-code__Bq7ot">Model</code></a> object that will be passed into the
component. Here, it references the function
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#TransformerModel"><span class="link_source-text__VDP74">spacy-transformers.TransformerModel.v3</span></a>
registered in the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#registry"><code class="code_inline-code__Bq7ot">architectures</code> registry</a>. If a key
in a block starts with <code class="code_inline-code__Bq7ot">@</code>, it‚Äôs <strong>resolved to a function</strong> and all other
settings are passed to the function as arguments. In this case, <code class="code_inline-code__Bq7ot">name</code>,
<code class="code_inline-code__Bq7ot">tokenizer_config</code> and <code class="code_inline-code__Bq7ot">get_spans</code>.</p><p><code class="code_inline-code__Bq7ot">get_spans</code> is a function that takes a batch of <code class="code_inline-code__Bq7ot">Doc</code> objects and returns lists
of potentially overlapping <code class="code_inline-code__Bq7ot">Span</code> objects to process by the transformer. Several
<a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/transformer#span_getters"><span class="link_source-text__VDP74">built-in functions</span></a> are available ‚Äì for example,
to process the whole document or individual sentences. When the config is
resolved, the function is created and passed into the model as an argument.</p><p>The <code class="code_inline-code__Bq7ot">name</code> value is the name of any <a class="link_root__1Me7D" href="/huggingface-models">HuggingFace model</a>,
which will be downloaded automatically the first time it‚Äôs used. You can also
use a local file path. For full details, see the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#TransformerModel"><code class="code_inline-code__Bq7ot">TransformerModel</code> docs</a>.</p><p>A wide variety of PyTorch models are supported, but some might not work. If a
model doesn‚Äôt seem to work feel free to open an
<a class="link_root__1Me7D link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spacy/issues"><span class="link_source-text__VDP74">issue</span></a>. Additionally note that
Transformers loaded in spaCy can only be used for tensors, and pretrained
task-specific heads or text generation features cannot be used as part of the
<code class="code_inline-code__Bq7ot">transformer</code> pipeline component.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><p>Remember that the <code class="code_inline-code__Bq7ot">config.cfg</code> used for training should contain <strong>no missing
values</strong> and requires all settings to be defined. You don‚Äôt want any hidden
defaults creeping in and changing your results! spaCy will tell you if settings
are missing, and you can run
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-fill-config"><code class="code_inline-code__Bq7ot">spacy init fill-config</code></a> to automatically fill in
all defaults.</p></aside><h3 id="transformers-training-custom-settings" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#transformers-training-custom-settings" class="heading-text typography_permalink__UiIRy">Customizing the settings <!-- --> </a></h3><p>To change any of the settings, you can edit the <code class="code_inline-code__Bq7ot">config.cfg</code> and re-run the
training. To change any of the functions, like the span getter, you can replace
the name of the referenced function ‚Äì e.g.
<code class="code_inline-code__Bq7ot code_wrap__b41os">@span_getters = &quot;spacy-transformers.sent_spans.v1&quot;</code> to process sentences. You
can also register your own functions using the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#registry"><code class="code_inline-code__Bq7ot">span_getters</code> registry</a>. For instance, the following
custom function returns <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/span"><code class="code_inline-code__Bq7ot">Span</code></a> objects following sentence
boundaries, unless a sentence succeeds a certain amount of tokens, in which case
subsentences of at most <code class="code_inline-code__Bq7ot">max_length</code> tokens are returned.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">config.cfg<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">code.py</h4><code class="code_code__CILJL language-python language-python"></code></pre><p>To resolve the config during training, spaCy needs to know about your custom
function. You can make it available via the <code class="code_inline-code__Bq7ot">--code</code> argument that can point to
a Python file. For more details on training with custom code, see the
<a class="link_root__1Me7D" href="/usage/training#custom-functions">training documentation</a>.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><h3 id="training-custom-model" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#training-custom-model" class="heading-text typography_permalink__UiIRy">Customizing the model implementations <!-- --> </a></h3><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/transformer"><code class="code_inline-code__Bq7ot">Transformer</code></a> component expects a Thinc
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-model"><code class="code_inline-code__Bq7ot">Model</code></a> object to be passed in as its <code class="code_inline-code__Bq7ot">model</code>
argument. You‚Äôre not limited to the implementation provided by
<code class="code_inline-code__Bq7ot">spacy-transformers</code> ‚Äì the only requirement is that your registered function
must return an object of type <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-model">Model</a><span class="code_cli-arg-subtle__IgB5m">[</span>List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" href="/api/doc">Doc</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span><a class="link_root__1Me7D" href="/api/transformer#fulltransformerbatch">FullTransformerBatch</a><span class="code_cli-arg-subtle__IgB5m">]</span></span>: that
is, a Thinc model that takes a list of <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> objects, and returns a
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/transformer#fulltransformerbatch"><code class="code_inline-code__Bq7ot">FullTransformerBatch</code></a> object with the
transformer data.</p><p>The same idea applies to task models that power the <strong>downstream components</strong>.
Most of spaCy‚Äôs built-in model creation functions support a <code class="code_inline-code__Bq7ot">tok2vec</code> argument,
which should be a Thinc layer of type <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-model">Model</a><span class="code_cli-arg-subtle__IgB5m">[</span>List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" href="/api/doc">Doc</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-types#types">Floats2d</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span>. This
is where we‚Äôll plug in our transformer model, using the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#TransformerListener"><span class="link_source-text__VDP74">TransformerListener</span></a> layer, which
sneakily delegates to the <code class="code_inline-code__Bq7ot">Transformer</code> pipeline component.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">config.cfg (excerpt)</h4><code class="code_code__CILJL language-ini language-ini code_wrap__b41os"></code></pre><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#TransformerListener"><span class="link_source-text__VDP74">TransformerListener</span></a> layer expects
a <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-layers#reduction-ops">pooling layer</a> as the
argument <code class="code_inline-code__Bq7ot">pooling</code>, which needs to be of type <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-model">Model</a><span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-types#ragged">Ragged</a><span class="code_cli-arg-subtle__IgB5m">,</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-types#types">Floats2d</a><span class="code_cli-arg-subtle__IgB5m">]</span></span>. This
layer determines how the vector for each spaCy token will be computed from the
zero or more source rows the token is aligned against. Here we use the
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-layers#reduce_mean"><code class="code_inline-code__Bq7ot">reduce_mean</code></a> layer, which
averages the wordpiece rows. We could instead use
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-layers#reduce_max"><code class="code_inline-code__Bq7ot">reduce_max</code></a>, or a custom
function you write yourself.</p><p>You can have multiple components all listening to the same transformer model,
and all passing gradients back to it. By default, all of the gradients will be
<strong>equally weighted</strong>. You can control this with the <code class="code_inline-code__Bq7ot">grad_factor</code> setting, which
lets you reweight the gradients from the different listeners. For instance,
setting <code class="code_inline-code__Bq7ot">grad_factor = 0</code> would disable gradients from one of the listeners,
while <code class="code_inline-code__Bq7ot">grad_factor = 2.0</code> would multiply them by 2. This is similar to having a
custom learning rate for each component. Instead of a constant, you can also
provide a schedule, allowing you to freeze the shared parameters at the start of
training.</p></section>
<section id="section-static-vectors" class="section_root__k1hUl"><h2 id="static-vectors" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#static-vectors" class="heading-text typography_permalink__UiIRy">Static vectors <!-- --> </a></h2><p>If your pipeline includes a <strong>word vectors table</strong>, you‚Äôll be able to use the
<code class="code_inline-code__Bq7ot">.similarity()</code> method on the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a>, <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/span"><code class="code_inline-code__Bq7ot">Span</code></a>,
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token"><code class="code_inline-code__Bq7ot">Token</code></a> and <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/lexeme"><code class="code_inline-code__Bq7ot">Lexeme</code></a> objects. You‚Äôll also be able
to access the vectors using the <code class="code_inline-code__Bq7ot">.vector</code> attribute, or you can look up one or
more vectors directly using the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/vocab"><code class="code_inline-code__Bq7ot">Vocab</code></a> object. Pipelines with
word vectors can also <strong>use the vectors as features</strong> for the statistical
models, which can <strong>improve the accuracy</strong> of your components.</p><p>Word vectors in spaCy are ‚Äústatic‚Äù in the sense that they are not learned
parameters of the statistical models, and spaCy itself does not feature any
algorithms for learning word vector tables. You can train a word vectors table
using tools such as <a class="link_root__1Me7D link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/floret"><span class="link_source-text__VDP74">floret</span></a>,
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://radimrehurek.com/gensim/">Gensim</a>, <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://fasttext.cc/">FastText</a> or
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://nlp.stanford.edu/projects/glove/">GloVe</a>, or download existing
pretrained vectors. The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-vectors"><code class="code_inline-code__Bq7ot">init vectors</code></a> command lets you
convert vectors for use with spaCy and will give you a directory you can load or
refer to in your <a class="link_root__1Me7D" href="/usage/training#config">training configs</a>.</p><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">üìñ</span>Word vectors and similarity</span></h4><p>For more details on loading word vectors into spaCy, using them for similarity
and improving word vector coverage by truncating and pruning the vectors, see
the usage guide on
<a class="link_root__1Me7D" href="/usage/linguistic-features#vectors-similarity">word vectors and similarity</a>.</p></aside><h3 id="word-vectors-models" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#word-vectors-models" class="heading-text typography_permalink__UiIRy">Using word vectors in your models <!-- --> </a></h3><p>Many neural network models are able to use word vector tables as additional
features, which sometimes results in significant improvements in accuracy.
spaCy‚Äôs built-in embedding layer,
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#MultiHashEmbed"><span class="link_source-text__VDP74">MultiHashEmbed</span></a>, can be configured to use
word vector tables using the <code class="code_inline-code__Bq7ot">include_static_vectors</code> flag.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">üí°</span>How it works</span></h4><p>The configuration system will look up the string <code class="code_inline-code__Bq7ot">&quot;spacy.MultiHashEmbed.v2&quot;</code> in
the <code class="code_inline-code__Bq7ot">architectures</code> <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/top-level#registry"><span class="link_source-text__VDP74">registry</span></a>, and call the returned
object with the rest of the arguments from the block. This will result in a call
to the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spacy/tree/develop/spacy/ml/models/tok2vec.py"><code class="code_inline-code__Bq7ot">MultiHashEmbed</code></a>
function, which will return a <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai">Thinc</a> model object with the
type signature <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-model">Model</a><span class="code_cli-arg-subtle__IgB5m">[</span>List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" href="/api/doc">Doc</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-types#types">Floats2d</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span>. Because the embedding layer
takes a list of <code class="code_inline-code__Bq7ot">Doc</code> objects as input, it does not need to store a copy of the
vectors table. The vectors will be retrieved from the <code class="code_inline-code__Bq7ot">Doc</code> objects that are
passed in, via the <code class="code_inline-code__Bq7ot">doc.vocab.vectors</code> attribute. This part of the process is
handled by the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#StaticVectors"><span class="link_source-text__VDP74">StaticVectors</span></a> layer.</p></aside><h4 id="custom-embedding-layer" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#custom-embedding-layer" class="heading-text typography_permalink__UiIRy">Creating a custom embedding layer <!-- --> </a></h4><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#StaticVectors"><span class="link_source-text__VDP74">MultiHashEmbed</span></a> layer is spaCy‚Äôs
recommended strategy for constructing initial word representations for your
neural network models, but you can also implement your own. You can register any
function to a string name, and then reference that function within your config
(see the <a class="link_root__1Me7D" href="/usage/training">training docs</a> for more details). To try this out,
you can save the following little example to a new Python file:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>If you pass the path to your file to the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a> command
using the <code class="code_inline-code__Bq7ot">--code</code> argument, your file will be imported, which means the
decorator registering the function will be run. Your function is now on equal
footing with any of spaCy‚Äôs built-ins, so you can drop it in instead of any
other model with the same input and output signature. For instance, you could
use it in the tagger model as follows:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre><p>Now that you have a custom function wired into the network, you can start
implementing the logic you‚Äôre interested in. For example, let‚Äôs say you want to
try a relatively simple embedding strategy that makes use of static word
vectors, but combines them via summation with a smaller table of learned
embeddings.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre></section>
<section id="section-pretraining" class="section_root__k1hUl"><h2 id="pretraining" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#pretraining" class="heading-text typography_permalink__UiIRy">Pretraining <!-- --> </a></h2><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#pretrain"><code class="code_inline-code__Bq7ot">spacy pretrain</code></a> command lets you initialize your
models with <strong>information from raw text</strong>. Without pretraining, the models for
your components will usually be initialized randomly. The idea behind
pretraining is simple: random probably isn‚Äôt optimal, so if we have some text to
learn from, we can probably find a way to get the model off to a better start.</p><p>Pretraining uses the same <a class="link_root__1Me7D" href="/usage/training#config"><code class="code_inline-code__Bq7ot">config.cfg</code></a> file as the
regular training, which helps keep the settings and hyperparameters consistent.
The additional <code class="code_inline-code__Bq7ot">[pretraining]</code> section has several configuration subsections
that are familiar from the training block: the <code class="code_inline-code__Bq7ot">[pretraining.batcher]</code>,
<code class="code_inline-code__Bq7ot">[pretraining.optimizer]</code> and <code class="code_inline-code__Bq7ot">[pretraining.corpus]</code> all work the same way and
expect the same types of objects, although for pretraining your corpus does not
need to have any annotations, so you will often use a different reader, such as
the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#jsonlcorpus"><code class="code_inline-code__Bq7ot">JsonlCorpus</code></a>.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Raw text format<!-- --> </span></h4>
<p>The raw text can be provided in spaCy‚Äôs
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/data-formats#training">binary <code class="code_inline-code__Bq7ot">.spacy</code> format</a> consisting of serialized
<code class="code_inline-code__Bq7ot">Doc</code> objects or as a JSONL (newline-delimited JSON) with a key <code class="code_inline-code__Bq7ot">&quot;text&quot;</code> per
entry. This allows the data to be read in line by line, while also allowing
you to include newlines in the texts.</p>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-json language-json"></code></pre>
<p>You can also use your own custom corpus loader instead.</p>
</div></div></aside><p>You can add a <code class="code_inline-code__Bq7ot">[pretraining]</code> block to your config by setting the
<code class="code_inline-code__Bq7ot">--pretraining</code> flag on <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-config"><code class="code_inline-code__Bq7ot">init config</code></a> or
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-fill-config"><code class="code_inline-code__Bq7ot">init fill-config</code></a>:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><p>You can then run <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#pretrain"><code class="code_inline-code__Bq7ot">spacy pretrain</code></a> with the updated config
and pass in optional config overrides, like the path to the raw text file:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><p>The following defaults are used for the <code class="code_inline-code__Bq7ot">[pretraining]</code> block and merged into
your existing config when you run <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-config"><code class="code_inline-code__Bq7ot">init config</code></a> or
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-fill-config"><code class="code_inline-code__Bq7ot">init fill-config</code></a> with <code class="code_inline-code__Bq7ot">--pretraining</code>. If needed,
you can <a class="link_root__1Me7D" href="/usage/embeddings-transformers#pretraining-configure">configure</a> the settings and hyperparameters or
change the <a class="link_root__1Me7D" href="/usage/embeddings-transformers#pretraining-objectives">objective</a>.</p><pre class="code_pre__kzg60"><header class="code_header__Jc0Q1"><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/default_config_pretraining.cfg"><code class="code_inline-code__Bq7ot code_inline-code-dark__ZEbch">explosion/spaCy/master/spacy/default_config_pretraining.cfg</code></a></header><code class="code_code__CILJL code_code__CILJL code_max-height__093k6 code_code__CILJL language-ini language-ini code_max-height__093k6 language-ini"></code></pre><h3 id="pretraining-details" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#pretraining-details" class="heading-text typography_permalink__UiIRy">How pretraining works <!-- --> </a></h3><p>The impact of <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#pretrain"><code class="code_inline-code__Bq7ot">spacy pretrain</code></a> varies, but it will usually
be worth trying if you‚Äôre <strong>not using a transformer</strong> model and you have
<strong>relatively little training data</strong> (for instance, fewer than 5,000 sentences).
A good rule of thumb is that pretraining will generally give you a similar
accuracy improvement to using word vectors in your model. If word vectors have
given you a 10% error reduction, pretraining with spaCy might give you another
10%, for a 20% error reduction in total.</p><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#pretrain"><code class="code_inline-code__Bq7ot">spacy pretrain</code></a> command will take a <strong>specific
subnetwork</strong> within one of your components, and add additional layers to build a
network for a temporary task that forces the model to learn something about
sentence structure and word cooccurrence statistics.</p><p>Pretraining produces a <strong>binary weights file</strong> that can be loaded back in at the
start of training, using the configuration option <code class="code_inline-code__Bq7ot">initialize.init_tok2vec</code>. The
weights file specifies an initial set of weights. Training then proceeds as
normal.</p><p>You can only pretrain one subnetwork from your pipeline at a time, and the
subnetwork must be typed <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-model">Model</a><span class="code_cli-arg-subtle__IgB5m">[</span>List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" href="/api/doc">Doc</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-types#types">Floats2d</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span> (i.e. it has to be
a ‚Äútok2vec‚Äù layer). The most common workflow is to use the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/tok2vec"><code class="code_inline-code__Bq7ot">Tok2Vec</code></a> component to create a shared token-to-vector layer for
several components of your pipeline, and apply pretraining to its whole model.</p><h4 id="pretraining-configure" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#pretraining-configure" class="heading-text typography_permalink__UiIRy">Configuring the pretraining <!-- --> </a></h4><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#pretrain"><code class="code_inline-code__Bq7ot">spacy pretrain</code></a> command is configured using the
<code class="code_inline-code__Bq7ot">[pretraining]</code> section of your <a class="link_root__1Me7D" href="/usage/training#config">config file</a>. The
<code class="code_inline-code__Bq7ot">component</code> and <code class="code_inline-code__Bq7ot">layer</code> settings tell spaCy how to <strong>find the subnetwork</strong> to
pretrain. The <code class="code_inline-code__Bq7ot">layer</code> setting should be either the empty string (to use the
whole model), or a
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/usage-models#model-state">node reference</a>. Most of
spaCy‚Äôs built-in model architectures have a reference named <code class="code_inline-code__Bq7ot">&quot;tok2vec&quot;</code> that
will refer to the right layer.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">config.cfg</h4><code class="code_code__CILJL language-ini language-ini"></code></pre><h4 id="pretraining-training" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#pretraining-training" class="heading-text typography_permalink__UiIRy">Connecting pretraining to training <!-- --> </a></h4><p>To benefit from pretraining, your training step needs to know to initialize its
<code class="code_inline-code__Bq7ot">tok2vec</code> component with the weights learned from the pretraining step. You do
this by setting <code class="code_inline-code__Bq7ot">initialize.init_tok2vec</code> to the filename of the <code class="code_inline-code__Bq7ot">.bin</code> file
that you want to use from pretraining.</p><p>A pretraining step that runs for 5 epochs with an output path of <code class="code_inline-code__Bq7ot">pretrain/</code>, as
an example, produces <code class="code_inline-code__Bq7ot">pretrain/model0.bin</code> through <code class="code_inline-code__Bq7ot">pretrain/model4.bin</code>. To
make use of the final output, you could fill in this value in your config file:</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">config.cfg</h4><code class="code_code__CILJL language-ini language-ini"></code></pre><aside class="infobox_root__yNIMg infobox_warning__SKl67"><p>The outputs of <code class="code_inline-code__Bq7ot">spacy pretrain</code> are not the same data format as the pre-packaged
static word vectors that would go into
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/data-formats#config-initialize"><code class="code_inline-code__Bq7ot">initialize.vectors</code></a>. The pretraining
output consists of the weights that the <code class="code_inline-code__Bq7ot">tok2vec</code> component should start with in
an existing pipeline, so it goes in <code class="code_inline-code__Bq7ot">initialize.init_tok2vec</code>.</p></aside><h4 id="pretraining-objectives" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#pretraining-objectives" class="heading-text typography_permalink__UiIRy">Pretraining objectives <!-- --> </a></h4><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Characters objective</h4><code class="code_code__CILJL language-ini language-ini"></code></pre>
<pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Vectors objective</h4><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><p>Two pretraining objectives are available, both of which are variants of the
cloze task <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://arxiv.org/abs/1810.04805">Devlin et al. (2018)</a> introduced
for BERT. The objective can be defined and configured via the
<code class="code_inline-code__Bq7ot">[pretraining.objective]</code> config block.</p><ul class="list_ul__fe_HF">
<li class="list_li__sfx_z">
<p><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#pretrain_chars"><code class="code_inline-code__Bq7ot">PretrainCharacters</code></a>: The <code class="code_inline-code__Bq7ot">&quot;characters&quot;</code>
objective asks the model to predict some number of leading and trailing UTF-8
bytes for the words. For instance, setting <code class="code_inline-code__Bq7ot">n_characters = 2</code>, the model will
try to predict the first two and last two characters of the word.</p>
</li>
<li class="list_li__sfx_z">
<p><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#pretrain_vectors"><code class="code_inline-code__Bq7ot">PretrainVectors</code></a>: The <code class="code_inline-code__Bq7ot">&quot;vectors&quot;</code>
objective asks the model to predict the word‚Äôs vector, from a static
embeddings table. This requires a word vectors model to be trained and loaded.
The vectors objective can optimize either a cosine or an L2 loss. We‚Äôve
generally found cosine loss to perform better.</p>
</li>
</ul><p>These pretraining objectives use a trick that we term <strong>language modelling with
approximate outputs (LMAO)</strong>. The motivation for the trick is that predicting an
exact word ID introduces a lot of incidental complexity. You need a large output
layer, and even then, the vocabulary is too large, which motivates tokenization
schemes that do not align to actual word boundaries. At the end of training, the
output layer will be thrown away regardless: we just want a task that forces the
network to model something about word cooccurrence statistics. Predicting
leading and trailing characters does that more than adequately, as the exact
word sequence could be recovered with high accuracy if the initial and trailing
characters are predicted accurately. With the vectors objective, the pretraining
uses the embedding space learned by an algorithm such as
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://nlp.stanford.edu/projects/glove/">GloVe</a> or
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://code.google.com/archive/p/word2vec/">Word2vec</a>, allowing the model to
focus on the contextual modelling we actual care about.</p></section><div class="grid_root__EfDZl grid_spacing__fhBCv grid_half__xoJZs"><div style="margin-top:var(--spacing-lg)"><a class="link_root__1Me7D button_root__jwipc button_secondary__ukZAk" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/website/docs/usage/embeddings-transformers.mdx">Suggest edits</a></div><a class="link_root__1Me7D readnext_root__JNzwZ link_no-link-layout__RPvod" href="/usage/training"><span><span class="typography_label__l_oVJ">Read next</span>Training Models</span><span class="readnext_icon__jfRnJ"></span></a></div></article><div class="main_asides__RITE5" style="background-image:url(/_next/static/media/pattern_blue.d167bed5.png"></div><footer class="footer_root__zlkjP"><div class="grid_root__EfDZl footer_content__LaE1F grid_narrow__x_6xS grid_spacing__fhBCv grid_third__edHuB"><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">spaCy</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/usage">Usage</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/models">Models</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/api">API Reference</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://course.spacy.io">Online Course</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/custom-solutions">Custom Solutions</a></li></ul></section><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Community</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/universe">Universe</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/discussions">GitHub Discussions</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/issues">Issue Tracker</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="http://stackoverflow.com/questions/tagged/spacy">Stack Overflow</a></li></ul></section><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Connect</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://twitter.com/spacy_io">Twitter</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy">GitHub</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://youtube.com/c/ExplosionAI">YouTube</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/blog">Blog</a></li></ul></section><section class="footer_full___icln"><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Stay in the loop!</li><li>Receive updates about new releases, tutorials and more.</li><li><form id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" action="//spacy.us12.list-manage.com/subscribe/post?u=83b0498b1e7fa3c91ce68c3f1&amp;amp;id=ecc82e0493" method="post" target="_blank" novalidate=""><div style="position:absolute;left:-5000px" aria-hidden="true"><input type="text" name="b_83b0498b1e7fa3c91ce68c3f1_ecc82e0493" tabindex="-1" value=""/></div><div class="newsletter_root__uh6MU"><input class="newsletter_input___SMSB" id="mce-EMAIL" type="email" name="EMAIL" placeholder="Your email" aria-label="Your email"/><button class="newsletter_button__gKW8E" id="mc-embedded-subscribe" type="submit" name="subscribe">Sign up</button></div></form></li></ul></section></div><div class="footer_content__LaE1F footer_copy__rbjvc"><span>¬© 2016-<!-- -->2023<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai">Explosion</a></span><a class="link_root__1Me7D footer_logo__BthsJ link_no-link-layout__RPvod" aria-label="Explosion" href="https://explosion.ai"></a><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/legal">Legal / Imprint</a></div></footer></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"title":"Embeddings, Transformers and Transfer Learning","teaser":"Using transformer embeddings like BERT in spaCy","menu":[["Embedding Layers","embedding-layers"],["Transformers","transformers"],["Static Vectors","static-vectors"],["Pretraining","pretraining"]],"next":{"slug":"/usage/training","title":"Training Models"},"slug":"/usage/embeddings-transformers","mdx":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\n/*TODO: Once rehearsal is tested, mention it here.*/\n/*TODO: \u003cProject id=\"pipelines/transformers\"\u003e*/\n/*The easiest way to get started is to clone a transformers-based project*/\n/*template. Swap in your data, edit the settings and hyperparameters and train,*/\n/*evaluate, package and visualize your model.*/\n/*\u003c/Project\u003e*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    section: \"section\",\n    p: \"p\",\n    strong: \"strong\",\n    a: \"a\",\n    em: \"em\",\n    h2: \"h2\",\n    img: \"img\",\n    table: \"table\",\n    thead: \"thead\",\n    tr: \"tr\",\n    th: \"th\",\n    tbody: \"tbody\",\n    td: \"td\",\n    h3: \"h3\",\n    pre: \"pre\",\n    code: \"code\",\n    blockquote: \"blockquote\",\n    h4: \"h4\",\n    del: \"del\",\n    ul: \"ul\",\n    li: \"li\"\n  }, _provideComponents(), props.components), {InlineCode, Accordion, Infobox} = _components;\n  if (!Accordion) _missingMdxReference(\"Accordion\", true);\n  if (!Infobox) _missingMdxReference(\"Infobox\", true);\n  if (!InlineCode) _missingMdxReference(\"InlineCode\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.section, {\n      children: [_jsxs(_components.p, {\n        children: [\"spaCy supports a number of \", _jsx(_components.strong, {\n          children: \"transfer and multi-task learning\"\n        }), \" workflows that\\ncan often help improve your pipeline‚Äôs efficiency or accuracy. Transfer learning\\nrefers to techniques such as word vector tables and language model pretraining.\\nThese techniques can be used to import knowledge from raw text into your\\npipeline, so that your models are able to generalize better from your annotated\\nexamples.\"]\n      }), _jsxs(_components.p, {\n        children: [\"You can convert \", _jsx(_components.strong, {\n          children: \"word vectors\"\n        }), \" from popular tools like\\n\", _jsx(_components.a, {\n          href: \"https://fasttext.cc\",\n          children: \"FastText\"\n        }), \" and \", _jsx(_components.a, {\n          href: \"https://radimrehurek.com/gensim\",\n          children: \"Gensim\"\n        }), \",\\nor you can load in any pretrained \", _jsx(_components.strong, {\n          children: \"transformer model\"\n        }), \" if you install\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spacy-transformers\",\n          children: _jsx(InlineCode, {\n            children: \"spacy-transformers\"\n          })\n        }), \". You can\\nalso do your own language model pretraining via the\\n\", _jsx(_components.a, {\n          href: \"/api/cli#pretrain\",\n          children: _jsx(InlineCode, {\n            children: \"spacy pretrain\"\n          })\n        }), \" command. You can even \", _jsx(_components.strong, {\n          children: \"share\"\n        }), \" your\\ntransformer or other contextual embedding model across multiple components,\\nwhich can make long pipelines several times more efficient. To use transfer\\nlearning, you‚Äôll need at least a few annotated examples for what you‚Äôre trying\\nto predict. Otherwise, you could try using a ‚Äúone-shot learning‚Äù approach using\\n\", _jsx(_components.a, {\n          href: \"/usage/linguistic-features#vectors-similarity\",\n          children: \"vectors and similarity\"\n        }), \".\"]\n      }), _jsxs(Accordion, {\n        title: \"What‚Äôs the difference between word vectors and language models?\",\n        id: \"vectors-vs-language-models\",\n        children: [_jsxs(_components.p, {\n          children: [_jsx(_components.a, {\n            href: \"#transformers\",\n            children: \"Transformers\"\n          }), \" are large and powerful neural networks that give\\nyou better accuracy, but are harder to deploy in production, as they require a\\nGPU to run effectively. \", _jsx(_components.a, {\n            href: \"#word-vectors\",\n            children: \"Word vectors\"\n          }), \" are a slightly older\\ntechnique that can give your models a smaller improvement in accuracy, and can\\nalso provide some additional capabilities.\"]\n        }), _jsxs(_components.p, {\n          children: [\"The key difference between word-vectors and contextual language models such as\\ntransformers is that word vectors model \", _jsx(_components.strong, {\n            children: \"lexical types\"\n          }), \", rather than \", _jsx(_components.em, {\n            children: \"tokens\"\n          }), \".\\nIf you have a list of terms with no context around them, a transformer model\\nlike BERT can‚Äôt really help you. BERT is designed to understand language \", _jsx(_components.strong, {\n            children: \"in\\ncontext\"\n          }), \", which isn‚Äôt what you have. A word vectors table will be a much better\\nfit for your task. However, if you do have words in context ‚Äì whole sentences or\\nparagraphs of running text ‚Äì word vectors will only provide a very rough\\napproximation of what the text is about.\"]\n        }), _jsxs(_components.p, {\n          children: [\"Word vectors are also very computationally efficient, as they map a word to a\\nvector with a single indexing operation. Word vectors are therefore useful as a\\nway to \", _jsx(_components.strong, {\n            children: \"improve the accuracy\"\n          }), \" of neural network models, especially models that\\nare small or have received little or no pretraining. In spaCy, word vector\\ntables are only used as \", _jsx(_components.strong, {\n            children: \"static features\"\n          }), \". spaCy does not backpropagate\\ngradients to the pretrained word vectors table. The static vectors table is\\nusually used in combination with a smaller table of learned task-specific\\nembeddings.\"]\n        })]\n      }), _jsxs(Accordion, {\n        title: \"When should I add word vectors to my model?\",\n        children: [_jsxs(_components.p, {\n          children: [\"Word vectors are not compatible with most \", _jsx(_components.a, {\n            href: \"#transformers\",\n            children: \"transformer models\"\n          }), \",\\nbut if you‚Äôre training another type of NLP network, it‚Äôs almost always worth\\nadding word vectors to your model. As well as improving your final accuracy,\\nword vectors often make experiments more consistent, as the accuracy you reach\\nwill be less sensitive to how the network is randomly initialized. High variance\\ndue to random chance can slow down your progress significantly, as you need to\\nrun many experiments to filter the signal from the noise.\"]\n        }), _jsx(_components.p, {\n          children: \"Word vector features need to be enabled prior to training, and the same word\\nvectors table will need to be available at runtime as well. You cannot add word\\nvector features once the model has already been trained, and you usually cannot\\nreplace one word vectors table with another without causing a significant loss\\nof performance.\"\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-embedding-layers\",\n      children: [_jsx(_components.h2, {\n        id: \"embedding-layers\",\n        children: \"Shared embedding layers \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy lets you share a single transformer or other token-to-vector (‚Äútok2vec‚Äù)\\nembedding layer between multiple components. You can even update the shared\\nlayer, performing \", _jsx(_components.strong, {\n          children: \"multi-task learning\"\n        }), \". Reusing the tok2vec layer between\\ncomponents can make your pipeline run a lot faster and result in much smaller\\nmodels. However, it can make the pipeline less modular and make it more\\ndifficult to swap components or retrain parts of the pipeline. Multi-task\\nlearning can affect your accuracy (either positively or negatively), and may\\nrequire some retuning of your hyper-parameters.\"]\n      }), _jsx(_components.img, {\n        src: \"/images/tok2vec.svg\",\n        alt: \"Pipeline components using a shared embedding component vs. independent embedding layers\"\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Shared\"\n            }), _jsx(_components.th, {\n              children: \"Independent\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [\"‚úÖ \", _jsx(_components.strong, {\n                children: \"smaller:\"\n              }), \" models only need to include a single copy of the embeddings\"]\n            }), _jsxs(_components.td, {\n              children: [\"‚ùå \", _jsx(_components.strong, {\n                children: \"larger:\"\n              }), \" models need to include the embeddings for each component\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [\"‚úÖ \", _jsx(_components.strong, {\n                children: \"faster:\"\n              }), \" embed the documents once for your whole pipeline\"]\n            }), _jsxs(_components.td, {\n              children: [\"‚ùå \", _jsx(_components.strong, {\n                children: \"slower:\"\n              }), \" rerun the embedding for each component\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [\"‚ùå \", _jsx(_components.strong, {\n                children: \"less composable:\"\n              }), \" all components require the same embedding component in the pipeline\"]\n            }), _jsxs(_components.td, {\n              children: [\"‚úÖ \", _jsx(_components.strong, {\n                children: \"modular:\"\n              }), \" components can be moved and swapped freely\"]\n            })]\n          })]\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"You can share a single transformer or other tok2vec model between multiple\\ncomponents by adding a \", _jsx(_components.a, {\n          href: \"/api/transformer\",\n          children: _jsx(InlineCode, {\n            children: \"Transformer\"\n          })\n        }), \" or\\n\", _jsx(_components.a, {\n          href: \"/api/tok2vec\",\n          children: _jsx(InlineCode, {\n            children: \"Tok2Vec\"\n          })\n        }), \" component near the start of your pipeline. Components\\nlater in the pipeline can ‚Äúconnect‚Äù to it by including a \", _jsx(_components.strong, {\n          children: \"listener layer\"\n        }), \" like\\n\", _jsx(_components.a, {\n          href: \"/api/architectures#Tok2VecListener\",\n          children: \"Tok2VecListener\"\n        }), \" within their model.\"]\n      }), _jsx(_components.img, {\n        src: \"/images/tok2vec-listener.svg\",\n        alt: \"Pipeline components listening to shared embedding component\"\n      }), _jsxs(_components.p, {\n        children: [\"At the beginning of training, the \", _jsx(_components.a, {\n          href: \"/api/tok2vec\",\n          children: _jsx(InlineCode, {\n            children: \"Tok2Vec\"\n          })\n        }), \" component will grab\\na reference to the relevant listener layers in the rest of your pipeline. When\\nit processes a batch of documents, it will pass forward its predictions to the\\nlisteners, allowing the listeners to \", _jsx(_components.strong, {\n          children: \"reuse the predictions\"\n        }), \" when they are\\neventually called. A similar mechanism is used to pass gradients from the\\nlisteners back to the model. The \", _jsx(_components.a, {\n          href: \"/api/transformer\",\n          children: _jsx(InlineCode, {\n            children: \"Transformer\"\n          })\n        }), \" component and\\n\", _jsx(_components.a, {\n          href: \"/api/architectures#TransformerListener\",\n          children: \"TransformerListener\"\n        }), \" layer do the same\\nthing for transformer models, but the \", _jsx(InlineCode, {\n          children: \"Transformer\"\n        }), \" component will also save the\\ntransformer outputs to the\\n\", _jsx(_components.a, {\n          href: \"/api/transformer#custom_attributes\",\n          children: _jsx(InlineCode, {\n            children: \"Doc._.trf_data\"\n          })\n        }), \" extension attribute,\\ngiving you access to them after the pipeline has finished running.\"]\n      }), _jsx(_components.h3, {\n        id: \"embedding-layers-config\",\n        children: \"Example: Shared vs. independent config \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/usage/training#config\",\n          children: \"config system\"\n        }), \" lets you express model configuration\\nfor both shared and independent embedding layers. The shared setup uses a single\\n\", _jsx(_components.a, {\n          href: \"/api/tok2vec\",\n          children: _jsx(InlineCode, {\n            children: \"Tok2Vec\"\n          })\n        }), \" component with the\\n\", _jsx(_components.a, {\n          href: \"/api/architectures#Tok2Vec\",\n          children: \"Tok2Vec\"\n        }), \" architecture. All other components, like\\nthe entity recognizer, use a\\n\", _jsx(_components.a, {\n          href: \"/api/architectures#Tok2VecListener\",\n          children: \"Tok2VecListener\"\n        }), \" layer as their model‚Äôs\\n\", _jsx(InlineCode, {\n          children: \"tok2vec\"\n        }), \" argument, which connects to the \", _jsx(InlineCode, {\n          children: \"tok2vec\"\n        }), \" component model.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"Shared\",\n          highlight: \"1-2,4-5,19-20\",\n          children: \"[components.tok2vec]\\nfactory = \\\"tok2vec\\\"\\n\\n[components.tok2vec.model]\\n@architectures = \\\"spacy.Tok2Vec.v2\\\"\\n\\n[components.tok2vec.model.embed]\\n@architectures = \\\"spacy.MultiHashEmbed.v2\\\"\\n\\n[components.tok2vec.model.encode]\\n@architectures = \\\"spacy.MaxoutWindowEncoder.v2\\\"\\n\\n[components.ner]\\nfactory = \\\"ner\\\"\\n\\n[components.ner.model]\\n@architectures = \\\"spacy.TransitionBasedParser.v1\\\"\\n\\n[components.ner.model.tok2vec]\\n@architectures = \\\"spacy.Tok2VecListener.v1\\\"\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"In the independent setup, the entity recognizer component defines its own\\n\", _jsx(_components.a, {\n          href: \"/api/architectures#Tok2Vec\",\n          children: \"Tok2Vec\"\n        }), \" instance. Other components will do the\\nsame. This makes them fully independent and doesn‚Äôt require an upstream\\n\", _jsx(_components.a, {\n          href: \"/api/tok2vec\",\n          children: _jsx(InlineCode, {\n            children: \"Tok2Vec\"\n          })\n        }), \" component to be present in the pipeline.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"Independent\",\n          highlight: \"7-8\",\n          children: \"[components.ner]\\nfactory = \\\"ner\\\"\\n\\n[components.ner.model]\\n@architectures = \\\"spacy.TransitionBasedParser.v1\\\"\\n\\n[components.ner.model.tok2vec]\\n@architectures = \\\"spacy.Tok2Vec.v2\\\"\\n\\n[components.ner.model.tok2vec.embed]\\n@architectures = \\\"spacy.MultiHashEmbed.v2\\\"\\n\\n[components.ner.model.tok2vec.encode]\\n@architectures = \\\"spacy.MaxoutWindowEncoder.v2\\\"\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-transformers\",\n      children: [_jsx(_components.h2, {\n        id: \"transformers\",\n        children: \"Using transformer models \"\n      }), _jsxs(_components.p, {\n        children: [\"Transformers are a family of neural network architectures that compute \", _jsx(_components.strong, {\n          children: \"dense,\\ncontext-sensitive representations\"\n        }), \" for the tokens in your documents. Downstream\\nmodels in your pipeline can then use these representations as input features to\\n\", _jsx(_components.strong, {\n          children: \"improve their predictions\"\n        }), \". You can connect multiple components to a single\\ntransformer model, with any or all of those components giving feedback to the\\ntransformer to fine-tune it to your tasks. spaCy‚Äôs transformer support\\ninteroperates with \", _jsx(_components.a, {\n          href: \"https://pytorch.org\",\n          children: \"PyTorch\"\n        }), \" and the\\n\", _jsxs(_components.a, {\n          href: \"https://huggingface.co/transformers/\",\n          children: [\"HuggingFace \", _jsx(InlineCode, {\n            children: \"transformers\"\n          })]\n        }), \" library,\\ngiving you access to thousands of pretrained models for your pipelines. There\\nare many \", _jsx(_components.a, {\n          href: \"http://jalammar.github.io/illustrated-transformer/\",\n          children: \"great guides\"\n        }), \" to\\ntransformer models, but for practical purposes, you can simply think of them as\\ndrop-in replacements that let you achieve \", _jsx(_components.strong, {\n          children: \"higher accuracy\"\n        }), \" in exchange for\\n\", _jsx(_components.strong, {\n          children: \"higher training and runtime costs\"\n        }), \".\"]\n      }), _jsx(_components.h3, {\n        id: \"transformers-installation\",\n        children: \"Setup and installation \"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"System requirements\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"We recommend an NVIDIA \", _jsx(_components.strong, {\n            children: \"GPU\"\n          }), \" with at least \", _jsx(_components.strong, {\n            children: \"10GB of memory\"\n          }), \" in order to\\nwork with transformer models. Make sure your GPU drivers are up to date and\\nyou have \", _jsx(_components.strong, {\n            children: \"CUDA v9+\"\n          }), \" installed.\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"The exact requirements will depend on the transformer model. Training a\\ntransformer-based model without a GPU will be too slow for most practical\\npurposes.\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"Provisioning a new machine will require about \", _jsx(_components.strong, {\n            children: \"5GB\"\n          }), \" of data to be\\ndownloaded: 3GB CUDA runtime, 800MB PyTorch, 400MB CuPy, 500MB weights, 200MB\\nspaCy and dependencies.\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"Once you have CUDA installed, we recommend installing PyTorch following the\\n\", _jsx(_components.a, {\n          href: \"https://pytorch.org/get-started/locally/\",\n          children: \"PyTorch installation guidelines\"\n        }), \" for\\nyour package manager and CUDA version. If you skip this step, pip will install\\nPyTorch as a dependency below, but it may not find the best version for your\\nsetup.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          title: \"Example: Install PyTorch 1.11.0 for CUDA 11.3 with pip\",\n          children: \"# See: https://pytorch.org/get-started/locally/\\n$ pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Next, install spaCy with the extras for your CUDA version and transformers. The\\nCUDA extra (e.g., \", _jsx(InlineCode, {\n          children: \"cuda102\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"cuda113\"\n        }), \") installs the correct version of\\n\", _jsx(_components.a, {\n          href: \"https://docs.cupy.dev/en/stable/install.html#installing-cupy\",\n          children: _jsx(InlineCode, {\n            children: \"cupy\"\n          })\n        }), \", which is\\njust like \", _jsx(InlineCode, {\n          children: \"numpy\"\n        }), \", but for GPU. You may also need to set the \", _jsx(InlineCode, {\n          children: \"CUDA_PATH\"\n        }), \"\\nenvironment variable if your CUDA runtime is installed in a non-standard\\nlocation. Putting it all together, if you had installed CUDA 11.3 in\\n\", _jsx(InlineCode, {\n          children: \"/opt/nvidia/cuda\"\n        }), \", you would run:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          title: \"Installation with CUDA\",\n          children: \"$ export CUDA_PATH=\\\"/opt/nvidia/cuda\\\"\\n$ pip install -U spacy[cuda113,transformers]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"For \", _jsx(_components.a, {\n          href: \"https://huggingface.co/transformers/\",\n          children: _jsx(InlineCode, {\n            children: \"transformers\"\n          })\n        }), \" v4.0.0+ and models\\nthat require \", _jsx(_components.a, {\n          href: \"https://github.com/google/sentencepiece\",\n          children: _jsx(InlineCode, {\n            children: \"SentencePiece\"\n          })\n        }), \" (e.g.,\\nALBERT, CamemBERT, XLNet, Marian, and T5), install the additional dependencies\\nwith:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          title: \"Install sentencepiece\",\n          children: \"$ pip install transformers[sentencepiece]\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"transformers-runtime\",\n        children: \"Runtime usage \"\n      }), _jsxs(_components.p, {\n        children: [\"Transformer models can be used as \", _jsx(_components.strong, {\n          children: \"drop-in replacements\"\n        }), \" for other types of\\nneural networks, so your spaCy pipeline can include them in a way that‚Äôs\\ncompletely invisible to the user. Users will download, load and use the model in\\nthe standard way, like any other spaCy pipeline. Instead of using the\\ntransformers as subnetworks directly, you can also use them via the\\n\", _jsx(_components.a, {\n          href: \"/api/transformer\",\n          children: _jsx(InlineCode, {\n            children: \"Transformer\"\n          })\n        }), \" pipeline component.\"]\n      }), _jsx(_components.img, {\n        src: \"/images/pipeline_transformer.svg\",\n        alt: \"The processing pipeline with the transformer component\"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"Transformer\"\n        }), \" component sets the\\n\", _jsx(_components.a, {\n          href: \"/api/transformer#custom_attributes\",\n          children: _jsx(InlineCode, {\n            children: \"Doc._.trf_data\"\n          })\n        }), \" extension attribute,\\nwhich lets you access the transformers outputs at runtime. The trained\\ntransformer-based \", _jsx(_components.a, {\n          href: \"/models\",\n          children: \"pipelines\"\n        }), \" provided by spaCy end on \", _jsx(InlineCode, {\n          children: \"_trf\"\n        }), \", e.g.\\n\", _jsx(_components.a, {\n          href: \"/models/en#en_core_web_trf\",\n          children: _jsx(InlineCode, {\n            children: \"en_core_web_trf\"\n          })\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ python -m spacy download en_core_web_trf\\n\"\n        })\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Example\",\n          children: \"import spacy\\nfrom thinc.api import set_gpu_allocator, require_gpu\\n\\n# Use the GPU, with memory allocations directed via PyTorch.\\n# This prevents out-of-memory errors that would otherwise occur from competing\\n# memory pools.\\nset_gpu_allocator(\\\"pytorch\\\")\\nrequire_gpu(0)\\n\\nnlp = spacy.load(\\\"en_core_web_trf\\\")\\nfor doc in nlp.pipe([\\\"some text\\\", \\\"some other text\\\"]):\\n    tokvecs = doc._.trf_data.tensors[-1]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"You can also customize how the \", _jsx(_components.a, {\n          href: \"/api/transformer\",\n          children: _jsx(InlineCode, {\n            children: \"Transformer\"\n          })\n        }), \" component sets\\nannotations onto the \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" by specifying a custom\\n\", _jsx(InlineCode, {\n          children: \"set_extra_annotations\"\n        }), \" function. This callback will be called with the raw\\ninput and output data for the whole batch, along with the batch of \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \"\\nobjects, allowing you to implement whatever you need. The annotation setter is\\ncalled with a batch of \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" objects and a\\n\", _jsx(_components.a, {\n          href: \"/api/transformer#fulltransformerbatch\",\n          children: _jsx(InlineCode, {\n            children: \"FullTransformerBatch\"\n          })\n        }), \" containing the\\ntransformers data for the batch.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"def custom_annotation_setter(docs, trf_data):\\n    doc_data = list(trf_data.doc_data)\\n    for doc, data in zip(docs, doc_data):\\n        doc._.custom_attr = data\\n\\nnlp = spacy.load(\\\"en_core_web_trf\\\")\\nnlp.get_pipe(\\\"transformer\\\").set_extra_annotations = custom_annotation_setter\\ndoc = nlp(\\\"This is a text\\\")\\nassert isinstance(doc._.custom_attr, TransformerData)\\nprint(doc._.custom_attr.tensors)\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"transformers-training\",\n        children: \"Training usage \"\n      }), _jsxs(_components.p, {\n        children: [\"The recommended workflow for training is to use spaCy‚Äôs\\n\", _jsx(_components.a, {\n          href: \"/usage/training#config\",\n          children: \"config system\"\n        }), \", usually via the\\n\", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \" command. The training config defines all\\ncomponent settings and hyperparameters in one place and lets you describe a tree\\nof objects by referring to creation functions, including functions you register\\nyourself. For details on how to get started with training your own model, check\\nout the \", _jsx(_components.a, {\n          href: \"/usage/training#quickstart\",\n          children: \"training quickstart\"\n        }), \".\"]\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"[components]\"\n        }), \" section in the \", _jsx(_components.a, {\n          href: \"/api/data-formats#config\",\n          children: _jsx(InlineCode, {\n            children: \"config.cfg\"\n          })\n        }), \"\\ndescribes the pipeline components and the settings used to construct them,\\nincluding their model implementation. Here‚Äôs a config snippet for the\\n\", _jsx(_components.a, {\n          href: \"/api/transformer\",\n          children: _jsx(InlineCode, {\n            children: \"Transformer\"\n          })\n        }), \" component, along with matching Python code. In\\nthis case, the \", _jsx(InlineCode, {\n          children: \"[components.transformer]\"\n        }), \" block describes the \", _jsx(InlineCode, {\n          children: \"transformer\"\n        }), \"\\ncomponent:\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Python equivalent\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"from spacy_transformers import Transformer, TransformerModel\\nfrom spacy_transformers.annotation_setters import null_annotation_setter\\nfrom spacy_transformers.span_getters import get_doc_spans\\n\\ntrf = Transformer(\\n    nlp.vocab,\\n    TransformerModel(\\n        \\\"bert-base-cased\\\",\\n        get_spans=get_doc_spans,\\n        tokenizer_config={\\\"use_fast\\\": True},\\n    ),\\n    set_extra_annotations=null_annotation_setter,\\n    max_batch_items=4096,\\n)\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"config.cfg\",\n          excerpt: \"true\",\n          children: \"[components.transformer]\\nfactory = \\\"transformer\\\"\\nmax_batch_items = 4096\\n\\n[components.transformer.model]\\n@architectures = \\\"spacy-transformers.TransformerModel.v3\\\"\\nname = \\\"bert-base-cased\\\"\\ntokenizer_config = {\\\"use_fast\\\": true}\\n\\n[components.transformer.model.get_spans]\\n@span_getters = \\\"spacy-transformers.doc_spans.v1\\\"\\n\\n[components.transformer.set_extra_annotations]\\n@annotation_setters = \\\"spacy-transformers.null_annotation_setter.v1\\\"\\n\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"[components.transformer.model]\"\n        }), \" block describes the \", _jsx(InlineCode, {\n          children: \"model\"\n        }), \" argument passed\\nto the transformer component. It‚Äôs a Thinc\\n\", _jsx(_components.a, {\n          href: \"https://thinc.ai/docs/api-model\",\n          children: _jsx(InlineCode, {\n            children: \"Model\"\n          })\n        }), \" object that will be passed into the\\ncomponent. Here, it references the function\\n\", _jsx(_components.a, {\n          href: \"/api/architectures#TransformerModel\",\n          children: \"spacy-transformers.TransformerModel.v3\"\n        }), \"\\nregistered in the \", _jsxs(_components.a, {\n          href: \"/api/top-level#registry\",\n          children: [_jsx(InlineCode, {\n            children: \"architectures\"\n          }), \" registry\"]\n        }), \". If a key\\nin a block starts with \", _jsx(InlineCode, {\n          children: \"@\"\n        }), \", it‚Äôs \", _jsx(_components.strong, {\n          children: \"resolved to a function\"\n        }), \" and all other\\nsettings are passed to the function as arguments. In this case, \", _jsx(InlineCode, {\n          children: \"name\"\n        }), \",\\n\", _jsx(InlineCode, {\n          children: \"tokenizer_config\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"get_spans\"\n        }), \".\"]\n      }), _jsxs(_components.p, {\n        children: [_jsx(InlineCode, {\n          children: \"get_spans\"\n        }), \" is a function that takes a batch of \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" objects and returns lists\\nof potentially overlapping \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" objects to process by the transformer. Several\\n\", _jsx(_components.a, {\n          href: \"/api/transformer#span_getters\",\n          children: \"built-in functions\"\n        }), \" are available ‚Äì for example,\\nto process the whole document or individual sentences. When the config is\\nresolved, the function is created and passed into the model as an argument.\"]\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"name\"\n        }), \" value is the name of any \", _jsx(_components.a, {\n          href: \"huggingface-models\",\n          children: \"HuggingFace model\"\n        }), \",\\nwhich will be downloaded automatically the first time it‚Äôs used. You can also\\nuse a local file path. For full details, see the\\n\", _jsxs(_components.a, {\n          href: \"/api/architectures#TransformerModel\",\n          children: [_jsx(InlineCode, {\n            children: \"TransformerModel\"\n          }), \" docs\"]\n        }), \".\"]\n      }), _jsxs(_components.p, {\n        children: [\"A wide variety of PyTorch models are supported, but some might not work. If a\\nmodel doesn‚Äôt seem to work feel free to open an\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spacy/issues\",\n          children: \"issue\"\n        }), \". Additionally note that\\nTransformers loaded in spaCy can only be used for tensors, and pretrained\\ntask-specific heads or text generation features cannot be used as part of the\\n\", _jsx(InlineCode, {\n          children: \"transformer\"\n        }), \" pipeline component.\"]\n      }), _jsx(Infobox, {\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Remember that the \", _jsx(InlineCode, {\n            children: \"config.cfg\"\n          }), \" used for training should contain \", _jsx(_components.strong, {\n            children: \"no missing\\nvalues\"\n          }), \" and requires all settings to be defined. You don‚Äôt want any hidden\\ndefaults creeping in and changing your results! spaCy will tell you if settings\\nare missing, and you can run\\n\", _jsx(_components.a, {\n            href: \"/api/cli#init-fill-config\",\n            children: _jsx(InlineCode, {\n              children: \"spacy init fill-config\"\n            })\n          }), \" to automatically fill in\\nall defaults.\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"transformers-training-custom-settings\",\n        children: \"Customizing the settings \"\n      }), _jsxs(_components.p, {\n        children: [\"To change any of the settings, you can edit the \", _jsx(InlineCode, {\n          children: \"config.cfg\"\n        }), \" and re-run the\\ntraining. To change any of the functions, like the span getter, you can replace\\nthe name of the referenced function ‚Äì e.g.\\n\", _jsx(InlineCode, {\n          children: \"@span_getters = \\\"spacy-transformers.sent_spans.v1\\\"\"\n        }), \" to process sentences. You\\ncan also register your own functions using the\\n\", _jsxs(_components.a, {\n          href: \"/api/top-level#registry\",\n          children: [_jsx(InlineCode, {\n            children: \"span_getters\"\n          }), \" registry\"]\n        }), \". For instance, the following\\ncustom function returns \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \" objects following sentence\\nboundaries, unless a sentence succeeds a certain amount of tokens, in which case\\nsubsentences of at most \", _jsx(InlineCode, {\n          children: \"max_length\"\n        }), \" tokens are returned.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[components.transformer.model.get_spans]\\n@span_getters = \\\"custom_sent_spans\\\"\\nmax_length = 25\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"code.py\",\n          children: \"import spacy_transformers\\n\\n@spacy_transformers.registry.span_getters(\\\"custom_sent_spans\\\")\\ndef configure_custom_sent_spans(max_length: int):\\n    def get_custom_sent_spans(docs):\\n        spans = []\\n        for doc in docs:\\n            spans.append([])\\n            for sent in doc.sents:\\n                start = 0\\n                end = max_length\\n                while end \u003c= len(sent):\\n                    spans[-1].append(sent[start:end])\\n                    start += max_length\\n                    end += max_length\\n                if start \u003c len(sent):\\n                    spans[-1].append(sent[start:len(sent)])\\n        return spans\\n\\n    return get_custom_sent_spans\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"To resolve the config during training, spaCy needs to know about your custom\\nfunction. You can make it available via the \", _jsx(InlineCode, {\n          children: \"--code\"\n        }), \" argument that can point to\\na Python file. For more details on training with custom code, see the\\n\", _jsx(_components.a, {\n          href: \"/usage/training#custom-functions\",\n          children: \"training documentation\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"python -m spacy train ./config.cfg --code ./code.py\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"training-custom-model\",\n        children: \"Customizing the model implementations \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/transformer\",\n          children: _jsx(InlineCode, {\n            children: \"Transformer\"\n          })\n        }), \" component expects a Thinc\\n\", _jsx(_components.a, {\n          href: \"https://thinc.ai/docs/api-model\",\n          children: _jsx(InlineCode, {\n            children: \"Model\"\n          })\n        }), \" object to be passed in as its \", _jsx(InlineCode, {\n          children: \"model\"\n        }), \"\\nargument. You‚Äôre not limited to the implementation provided by\\n\", _jsx(InlineCode, {\n          children: \"spacy-transformers\"\n        }), \" ‚Äì the only requirement is that your registered function\\nmust return an object of type \", _jsx(_components.del, {\n          children: \"Model[List[Doc], FullTransformerBatch]\"\n        }), \": that\\nis, a Thinc model that takes a list of \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" objects, and returns a\\n\", _jsx(_components.a, {\n          href: \"/api/transformer#fulltransformerbatch\",\n          children: _jsx(InlineCode, {\n            children: \"FullTransformerBatch\"\n          })\n        }), \" object with the\\ntransformer data.\"]\n      }), _jsxs(_components.p, {\n        children: [\"The same idea applies to task models that power the \", _jsx(_components.strong, {\n          children: \"downstream components\"\n        }), \".\\nMost of spaCy‚Äôs built-in model creation functions support a \", _jsx(InlineCode, {\n          children: \"tok2vec\"\n        }), \" argument,\\nwhich should be a Thinc layer of type \", _jsx(_components.del, {\n          children: \"Model[List[Doc], List[Floats2d]]\"\n        }), \". This\\nis where we‚Äôll plug in our transformer model, using the\\n\", _jsx(_components.a, {\n          href: \"/api/architectures#TransformerListener\",\n          children: \"TransformerListener\"\n        }), \" layer, which\\nsneakily delegates to the \", _jsx(InlineCode, {\n          children: \"Transformer\"\n        }), \" pipeline component.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"config.cfg (excerpt)\",\n          highlight: \"12\",\n          children: \"[components.ner]\\nfactory = \\\"ner\\\"\\n\\n[nlp.pipeline.ner.model]\\n@architectures = \\\"spacy.TransitionBasedParser.v1\\\"\\nstate_type = \\\"ner\\\"\\nextra_state_tokens = false\\nhidden_width = 128\\nmaxout_pieces = 3\\nuse_upper = false\\n\\n[nlp.pipeline.ner.model.tok2vec]\\n@architectures = \\\"spacy-transformers.TransformerListener.v1\\\"\\ngrad_factor = 1.0\\n\\n[nlp.pipeline.ner.model.tok2vec.pooling]\\n@layers = \\\"reduce_mean.v1\\\"\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/architectures#TransformerListener\",\n          children: \"TransformerListener\"\n        }), \" layer expects\\na \", _jsx(_components.a, {\n          href: \"https://thinc.ai/docs/api-layers#reduction-ops\",\n          children: \"pooling layer\"\n        }), \" as the\\nargument \", _jsx(InlineCode, {\n          children: \"pooling\"\n        }), \", which needs to be of type \", _jsx(_components.del, {\n          children: \"Model[Ragged, Floats2d]\"\n        }), \". This\\nlayer determines how the vector for each spaCy token will be computed from the\\nzero or more source rows the token is aligned against. Here we use the\\n\", _jsx(_components.a, {\n          href: \"https://thinc.ai/docs/api-layers#reduce_mean\",\n          children: _jsx(InlineCode, {\n            children: \"reduce_mean\"\n          })\n        }), \" layer, which\\naverages the wordpiece rows. We could instead use\\n\", _jsx(_components.a, {\n          href: \"https://thinc.ai/docs/api-layers#reduce_max\",\n          children: _jsx(InlineCode, {\n            children: \"reduce_max\"\n          })\n        }), \", or a custom\\nfunction you write yourself.\"]\n      }), _jsxs(_components.p, {\n        children: [\"You can have multiple components all listening to the same transformer model,\\nand all passing gradients back to it. By default, all of the gradients will be\\n\", _jsx(_components.strong, {\n          children: \"equally weighted\"\n        }), \". You can control this with the \", _jsx(InlineCode, {\n          children: \"grad_factor\"\n        }), \" setting, which\\nlets you reweight the gradients from the different listeners. For instance,\\nsetting \", _jsx(InlineCode, {\n          children: \"grad_factor = 0\"\n        }), \" would disable gradients from one of the listeners,\\nwhile \", _jsx(InlineCode, {\n          children: \"grad_factor = 2.0\"\n        }), \" would multiply them by 2. This is similar to having a\\ncustom learning rate for each component. Instead of a constant, you can also\\nprovide a schedule, allowing you to freeze the shared parameters at the start of\\ntraining.\"]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-static-vectors\",\n      children: [_jsx(_components.h2, {\n        id: \"static-vectors\",\n        children: \"Static vectors \"\n      }), _jsxs(_components.p, {\n        children: [\"If your pipeline includes a \", _jsx(_components.strong, {\n          children: \"word vectors table\"\n        }), \", you‚Äôll be able to use the\\n\", _jsx(InlineCode, {\n          children: \".similarity()\"\n        }), \" method on the \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \", \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \",\\n\", _jsx(_components.a, {\n          href: \"/api/token\",\n          children: _jsx(InlineCode, {\n            children: \"Token\"\n          })\n        }), \" and \", _jsx(_components.a, {\n          href: \"/api/lexeme\",\n          children: _jsx(InlineCode, {\n            children: \"Lexeme\"\n          })\n        }), \" objects. You‚Äôll also be able\\nto access the vectors using the \", _jsx(InlineCode, {\n          children: \".vector\"\n        }), \" attribute, or you can look up one or\\nmore vectors directly using the \", _jsx(_components.a, {\n          href: \"/api/vocab\",\n          children: _jsx(InlineCode, {\n            children: \"Vocab\"\n          })\n        }), \" object. Pipelines with\\nword vectors can also \", _jsx(_components.strong, {\n          children: \"use the vectors as features\"\n        }), \" for the statistical\\nmodels, which can \", _jsx(_components.strong, {\n          children: \"improve the accuracy\"\n        }), \" of your components.\"]\n      }), _jsxs(_components.p, {\n        children: [\"Word vectors in spaCy are ‚Äústatic‚Äù in the sense that they are not learned\\nparameters of the statistical models, and spaCy itself does not feature any\\nalgorithms for learning word vector tables. You can train a word vectors table\\nusing tools such as \", _jsx(_components.a, {\n          href: \"https://github.com/explosion/floret\",\n          children: \"floret\"\n        }), \",\\n\", _jsx(_components.a, {\n          href: \"https://radimrehurek.com/gensim/\",\n          children: \"Gensim\"\n        }), \", \", _jsx(_components.a, {\n          href: \"https://fasttext.cc/\",\n          children: \"FastText\"\n        }), \" or\\n\", _jsx(_components.a, {\n          href: \"https://nlp.stanford.edu/projects/glove/\",\n          children: \"GloVe\"\n        }), \", or download existing\\npretrained vectors. The \", _jsx(_components.a, {\n          href: \"/api/cli#init-vectors\",\n          children: _jsx(InlineCode, {\n            children: \"init vectors\"\n          })\n        }), \" command lets you\\nconvert vectors for use with spaCy and will give you a directory you can load or\\nrefer to in your \", _jsx(_components.a, {\n          href: \"/usage/training#config\",\n          children: \"training configs\"\n        }), \".\"]\n      }), _jsx(Infobox, {\n        title: \"Word vectors and similarity\",\n        emoji: \"üìñ\",\n        children: _jsxs(_components.p, {\n          children: [\"For more details on loading word vectors into spaCy, using them for similarity\\nand improving word vector coverage by truncating and pruning the vectors, see\\nthe usage guide on\\n\", _jsx(_components.a, {\n            href: \"/usage/linguistic-features#vectors-similarity\",\n            children: \"word vectors and similarity\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"word-vectors-models\",\n        children: \"Using word vectors in your models \"\n      }), _jsxs(_components.p, {\n        children: [\"Many neural network models are able to use word vector tables as additional\\nfeatures, which sometimes results in significant improvements in accuracy.\\nspaCy‚Äôs built-in embedding layer,\\n\", _jsx(_components.a, {\n          href: \"/api/architectures#MultiHashEmbed\",\n          children: \"MultiHashEmbed\"\n        }), \", can be configured to use\\nword vector tables using the \", _jsx(InlineCode, {\n          children: \"include_static_vectors\"\n        }), \" flag.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          children: \"[tagger.model.tok2vec.embed]\\n@architectures = \\\"spacy.MultiHashEmbed.v2\\\"\\nwidth = 128\\nattrs = [\\\"LOWER\\\",\\\"PREFIX\\\",\\\"SUFFIX\\\",\\\"SHAPE\\\"]\\nrows = [5000,2500,2500,2500]\\ninclude_static_vectors = true\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"How it works\",\n        emoji: \"üí°\",\n        children: _jsxs(_components.p, {\n          children: [\"The configuration system will look up the string \", _jsx(InlineCode, {\n            children: \"\\\"spacy.MultiHashEmbed.v2\\\"\"\n          }), \" in\\nthe \", _jsx(InlineCode, {\n            children: \"architectures\"\n          }), \" \", _jsx(_components.a, {\n            href: \"/api/top-level#registry\",\n            children: \"registry\"\n          }), \", and call the returned\\nobject with the rest of the arguments from the block. This will result in a call\\nto the\\n\", _jsx(_components.a, {\n            href: \"https://github.com/explosion/spacy/tree/develop/spacy/ml/models/tok2vec.py\",\n            children: _jsx(InlineCode, {\n              children: \"MultiHashEmbed\"\n            })\n          }), \"\\nfunction, which will return a \", _jsx(_components.a, {\n            href: \"https://thinc.ai\",\n            children: \"Thinc\"\n          }), \" model object with the\\ntype signature \", _jsx(_components.del, {\n            children: \"Model[List[Doc], List[Floats2d]]\"\n          }), \". Because the embedding layer\\ntakes a list of \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \" objects as input, it does not need to store a copy of the\\nvectors table. The vectors will be retrieved from the \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \" objects that are\\npassed in, via the \", _jsx(InlineCode, {\n            children: \"doc.vocab.vectors\"\n          }), \" attribute. This part of the process is\\nhandled by the \", _jsx(_components.a, {\n            href: \"/api/architectures#StaticVectors\",\n            children: \"StaticVectors\"\n          }), \" layer.\"]\n        })\n      }), _jsx(_components.h4, {\n        id: \"custom-embedding-layer\",\n        children: \"Creating a custom embedding layer \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/architectures#StaticVectors\",\n          children: \"MultiHashEmbed\"\n        }), \" layer is spaCy‚Äôs\\nrecommended strategy for constructing initial word representations for your\\nneural network models, but you can also implement your own. You can register any\\nfunction to a string name, and then reference that function within your config\\n(see the \", _jsx(_components.a, {\n          href: \"/usage/training\",\n          children: \"training docs\"\n        }), \" for more details). To try this out,\\nyou can save the following little example to a new Python file:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"from spacy.ml.staticvectors import StaticVectors\\nfrom spacy.util import registry\\n\\nprint(\\\"I was imported!\\\")\\n\\n@registry.architectures(\\\"my_example.MyEmbedding.v1\\\")\\ndef MyEmbedding(output_width: int) -\u003e Model[List[Doc], List[Floats2d]]:\\n    print(\\\"I was called!\\\")\\n    return StaticVectors(nO=output_width)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"If you pass the path to your file to the \", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \" command\\nusing the \", _jsx(InlineCode, {\n          children: \"--code\"\n        }), \" argument, your file will be imported, which means the\\ndecorator registering the function will be run. Your function is now on equal\\nfooting with any of spaCy‚Äôs built-ins, so you can drop it in instead of any\\nother model with the same input and output signature. For instance, you could\\nuse it in the tagger model as follows:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          children: \"[tagger.model.tok2vec.embed]\\n@architectures = \\\"my_example.MyEmbedding.v1\\\"\\noutput_width = 128\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"Now that you have a custom function wired into the network, you can start\\nimplementing the logic you‚Äôre interested in. For example, let‚Äôs say you want to\\ntry a relatively simple embedding strategy that makes use of static word\\nvectors, but combines them via summation with a smaller table of learned\\nembeddings.\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"from thinc.api import add, chain, remap_ids, Embed\\nfrom spacy.ml.staticvectors import StaticVectors\\nfrom spacy.ml.featureextractor import FeatureExtractor\\nfrom spacy.util import registry\\n\\n@registry.architectures(\\\"my_example.MyEmbedding.v1\\\")\\ndef MyCustomVectors(\\n    output_width: int,\\n    vector_width: int,\\n    embed_rows: int,\\n    key2row: Dict[int, int]\\n) -\u003e Model[List[Doc], List[Floats2d]]:\\n    return add(\\n        StaticVectors(nO=output_width),\\n        chain(\\n           FeatureExtractor([\\\"ORTH\\\"]),\\n           remap_ids(key2row),\\n           Embed(nO=output_width, nV=embed_rows)\\n        )\\n    )\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-pretraining\",\n      children: [_jsx(_components.h2, {\n        id: \"pretraining\",\n        children: \"Pretraining \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/cli#pretrain\",\n          children: _jsx(InlineCode, {\n            children: \"spacy pretrain\"\n          })\n        }), \" command lets you initialize your\\nmodels with \", _jsx(_components.strong, {\n          children: \"information from raw text\"\n        }), \". Without pretraining, the models for\\nyour components will usually be initialized randomly. The idea behind\\npretraining is simple: random probably isn‚Äôt optimal, so if we have some text to\\nlearn from, we can probably find a way to get the model off to a better start.\"]\n      }), _jsxs(_components.p, {\n        children: [\"Pretraining uses the same \", _jsx(_components.a, {\n          href: \"/usage/training#config\",\n          children: _jsx(InlineCode, {\n            children: \"config.cfg\"\n          })\n        }), \" file as the\\nregular training, which helps keep the settings and hyperparameters consistent.\\nThe additional \", _jsx(InlineCode, {\n          children: \"[pretraining]\"\n        }), \" section has several configuration subsections\\nthat are familiar from the training block: the \", _jsx(InlineCode, {\n          children: \"[pretraining.batcher]\"\n        }), \",\\n\", _jsx(InlineCode, {\n          children: \"[pretraining.optimizer]\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"[pretraining.corpus]\"\n        }), \" all work the same way and\\nexpect the same types of objects, although for pretraining your corpus does not\\nneed to have any annotations, so you will often use a different reader, such as\\nthe \", _jsx(_components.a, {\n          href: \"/api/top-level#jsonlcorpus\",\n          children: _jsx(InlineCode, {\n            children: \"JsonlCorpus\"\n          })\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Raw text format\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"The raw text can be provided in spaCy‚Äôs\\n\", _jsxs(_components.a, {\n            href: \"/api/data-formats#training\",\n            children: [\"binary \", _jsx(InlineCode, {\n              children: \".spacy\"\n            }), \" format\"]\n          }), \" consisting of serialized\\n\", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \" objects or as a JSONL (newline-delimited JSON) with a key \", _jsx(InlineCode, {\n            children: \"\\\"text\\\"\"\n          }), \" per\\nentry. This allows the data to be read in line by line, while also allowing\\nyou to include newlines in the texts.\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-json\",\n            lang: \"json\",\n            children: \"{\\\"text\\\": \\\"Can I ask where you work now and what you do, and if you enjoy it?\\\"}\\n{\\\"text\\\": \\\"They may just pull out of the Seattle market completely, at least until they have autonomous vehicles.\\\"}\\n\"\n          })\n        }), \"\\n\", _jsx(_components.p, {\n          children: \"You can also use your own custom corpus loader instead.\"\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"You can add a \", _jsx(InlineCode, {\n          children: \"[pretraining]\"\n        }), \" block to your config by setting the\\n\", _jsx(InlineCode, {\n          children: \"--pretraining\"\n        }), \" flag on \", _jsx(_components.a, {\n          href: \"/api/cli#init-config\",\n          children: _jsx(InlineCode, {\n            children: \"init config\"\n          })\n        }), \" or\\n\", _jsx(_components.a, {\n          href: \"/api/cli#init-fill-config\",\n          children: _jsx(InlineCode, {\n            children: \"init fill-config\"\n          })\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ python -m spacy init fill-config config.cfg config_pretrain.cfg --pretraining\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"You can then run \", _jsx(_components.a, {\n          href: \"/api/cli#pretrain\",\n          children: _jsx(InlineCode, {\n            children: \"spacy pretrain\"\n          })\n        }), \" with the updated config\\nand pass in optional config overrides, like the path to the raw text file:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ python -m spacy pretrain config_pretrain.cfg ./output --paths.raw_text text.jsonl\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The following defaults are used for the \", _jsx(InlineCode, {\n          children: \"[pretraining]\"\n        }), \" block and merged into\\nyour existing config when you run \", _jsx(_components.a, {\n          href: \"/api/cli#init-config\",\n          children: _jsx(InlineCode, {\n            children: \"init config\"\n          })\n        }), \" or\\n\", _jsx(_components.a, {\n          href: \"/api/cli#init-fill-config\",\n          children: _jsx(InlineCode, {\n            children: \"init fill-config\"\n          })\n        }), \" with \", _jsx(InlineCode, {\n          children: \"--pretraining\"\n        }), \". If needed,\\nyou can \", _jsx(_components.a, {\n          href: \"#pretraining-configure\",\n          children: \"configure\"\n        }), \" the settings and hyperparameters or\\nchange the \", _jsx(_components.a, {\n          href: \"#pretraining-objectives\",\n          children: \"objective\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          github: \"https://github.com/explosion/spaCy/tree/master/spacy/default_config_pretraining.cfg\",\n          children: \"https://github.com/explosion/spaCy/tree/master/spacy/default_config_pretraining.cfg\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"pretraining-details\",\n        children: \"How pretraining works \"\n      }), _jsxs(_components.p, {\n        children: [\"The impact of \", _jsx(_components.a, {\n          href: \"/api/cli#pretrain\",\n          children: _jsx(InlineCode, {\n            children: \"spacy pretrain\"\n          })\n        }), \" varies, but it will usually\\nbe worth trying if you‚Äôre \", _jsx(_components.strong, {\n          children: \"not using a transformer\"\n        }), \" model and you have\\n\", _jsx(_components.strong, {\n          children: \"relatively little training data\"\n        }), \" (for instance, fewer than 5,000 sentences).\\nA good rule of thumb is that pretraining will generally give you a similar\\naccuracy improvement to using word vectors in your model. If word vectors have\\ngiven you a 10% error reduction, pretraining with spaCy might give you another\\n10%, for a 20% error reduction in total.\"]\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/cli#pretrain\",\n          children: _jsx(InlineCode, {\n            children: \"spacy pretrain\"\n          })\n        }), \" command will take a \", _jsx(_components.strong, {\n          children: \"specific\\nsubnetwork\"\n        }), \" within one of your components, and add additional layers to build a\\nnetwork for a temporary task that forces the model to learn something about\\nsentence structure and word cooccurrence statistics.\"]\n      }), _jsxs(_components.p, {\n        children: [\"Pretraining produces a \", _jsx(_components.strong, {\n          children: \"binary weights file\"\n        }), \" that can be loaded back in at the\\nstart of training, using the configuration option \", _jsx(InlineCode, {\n          children: \"initialize.init_tok2vec\"\n        }), \". The\\nweights file specifies an initial set of weights. Training then proceeds as\\nnormal.\"]\n      }), _jsxs(_components.p, {\n        children: [\"You can only pretrain one subnetwork from your pipeline at a time, and the\\nsubnetwork must be typed \", _jsx(_components.del, {\n          children: \"Model[List[Doc], List[Floats2d]]\"\n        }), \" (i.e. it has to be\\na ‚Äútok2vec‚Äù layer). The most common workflow is to use the\\n\", _jsx(_components.a, {\n          href: \"/api/tok2vec\",\n          children: _jsx(InlineCode, {\n            children: \"Tok2Vec\"\n          })\n        }), \" component to create a shared token-to-vector layer for\\nseveral components of your pipeline, and apply pretraining to its whole model.\"]\n      }), _jsx(_components.h4, {\n        id: \"pretraining-configure\",\n        children: \"Configuring the pretraining \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/cli#pretrain\",\n          children: _jsx(InlineCode, {\n            children: \"spacy pretrain\"\n          })\n        }), \" command is configured using the\\n\", _jsx(InlineCode, {\n          children: \"[pretraining]\"\n        }), \" section of your \", _jsx(_components.a, {\n          href: \"/usage/training#config\",\n          children: \"config file\"\n        }), \". The\\n\", _jsx(InlineCode, {\n          children: \"component\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"layer\"\n        }), \" settings tell spaCy how to \", _jsx(_components.strong, {\n          children: \"find the subnetwork\"\n        }), \" to\\npretrain. The \", _jsx(InlineCode, {\n          children: \"layer\"\n        }), \" setting should be either the empty string (to use the\\nwhole model), or a\\n\", _jsx(_components.a, {\n          href: \"https://thinc.ai/docs/usage-models#model-state\",\n          children: \"node reference\"\n        }), \". Most of\\nspaCy‚Äôs built-in model architectures have a reference named \", _jsx(InlineCode, {\n          children: \"\\\"tok2vec\\\"\"\n        }), \" that\\nwill refer to the right layer.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"config.cfg\",\n          children: \"# 1. Use the whole model of the \\\"tok2vec\\\" component\\n[pretraining]\\ncomponent = \\\"tok2vec\\\"\\nlayer = \\\"\\\"\\n\\n# 2. Pretrain the \\\"tok2vec\\\" node of the \\\"textcat\\\" component\\n[pretraining]\\ncomponent = \\\"textcat\\\"\\nlayer = \\\"tok2vec\\\"\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"pretraining-training\",\n        children: \"Connecting pretraining to training \"\n      }), _jsxs(_components.p, {\n        children: [\"To benefit from pretraining, your training step needs to know to initialize its\\n\", _jsx(InlineCode, {\n          children: \"tok2vec\"\n        }), \" component with the weights learned from the pretraining step. You do\\nthis by setting \", _jsx(InlineCode, {\n          children: \"initialize.init_tok2vec\"\n        }), \" to the filename of the \", _jsx(InlineCode, {\n          children: \".bin\"\n        }), \" file\\nthat you want to use from pretraining.\"]\n      }), _jsxs(_components.p, {\n        children: [\"A pretraining step that runs for 5 epochs with an output path of \", _jsx(InlineCode, {\n          children: \"pretrain/\"\n        }), \", as\\nan example, produces \", _jsx(InlineCode, {\n          children: \"pretrain/model0.bin\"\n        }), \" through \", _jsx(InlineCode, {\n          children: \"pretrain/model4.bin\"\n        }), \". To\\nmake use of the final output, you could fill in this value in your config file:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"config.cfg\",\n          children: \"\\n[paths]\\ninit_tok2vec = \\\"pretrain/model4.bin\\\"\\n\\n[initialize]\\ninit_tok2vec = ${paths.init_tok2vec}\\n\"\n        })\n      }), _jsx(Infobox, {\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"The outputs of \", _jsx(InlineCode, {\n            children: \"spacy pretrain\"\n          }), \" are not the same data format as the pre-packaged\\nstatic word vectors that would go into\\n\", _jsx(_components.a, {\n            href: \"/api/data-formats#config-initialize\",\n            children: _jsx(InlineCode, {\n              children: \"initialize.vectors\"\n            })\n          }), \". The pretraining\\noutput consists of the weights that the \", _jsx(InlineCode, {\n            children: \"tok2vec\"\n          }), \" component should start with in\\nan existing pipeline, so it goes in \", _jsx(InlineCode, {\n            children: \"initialize.init_tok2vec\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h4, {\n        id: \"pretraining-objectives\",\n        children: \"Pretraining objectives \"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            title: \"Characters objective\",\n            children: \"[pretraining.objective]\\n@architectures = \\\"spacy.PretrainCharacters.v1\\\"\\nmaxout_pieces = 3\\nhidden_size = 300\\nn_characters = 4\\n\"\n          })\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            title: \"Vectors objective\",\n            children: \"[pretraining.objective]\\n@architectures = \\\"spacy.PretrainVectors.v1\\\"\\nmaxout_pieces = 3\\nhidden_size = 300\\nloss = \\\"cosine\\\"\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"Two pretraining objectives are available, both of which are variants of the\\ncloze task \", _jsx(_components.a, {\n          href: \"https://arxiv.org/abs/1810.04805\",\n          children: \"Devlin et al. (2018)\"\n        }), \" introduced\\nfor BERT. The objective can be defined and configured via the\\n\", _jsx(InlineCode, {\n          children: \"[pretraining.objective]\"\n        }), \" config block.\"]\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.a, {\n              href: \"/api/architectures#pretrain_chars\",\n              children: _jsx(InlineCode, {\n                children: \"PretrainCharacters\"\n              })\n            }), \": The \", _jsx(InlineCode, {\n              children: \"\\\"characters\\\"\"\n            }), \"\\nobjective asks the model to predict some number of leading and trailing UTF-8\\nbytes for the words. For instance, setting \", _jsx(InlineCode, {\n              children: \"n_characters = 2\"\n            }), \", the model will\\ntry to predict the first two and last two characters of the word.\"]\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.a, {\n              href: \"/api/architectures#pretrain_vectors\",\n              children: _jsx(InlineCode, {\n                children: \"PretrainVectors\"\n              })\n            }), \": The \", _jsx(InlineCode, {\n              children: \"\\\"vectors\\\"\"\n            }), \"\\nobjective asks the model to predict the word‚Äôs vector, from a static\\nembeddings table. This requires a word vectors model to be trained and loaded.\\nThe vectors objective can optimize either a cosine or an L2 loss. We‚Äôve\\ngenerally found cosine loss to perform better.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"These pretraining objectives use a trick that we term \", _jsx(_components.strong, {\n          children: \"language modelling with\\napproximate outputs (LMAO)\"\n        }), \". The motivation for the trick is that predicting an\\nexact word ID introduces a lot of incidental complexity. You need a large output\\nlayer, and even then, the vocabulary is too large, which motivates tokenization\\nschemes that do not align to actual word boundaries. At the end of training, the\\noutput layer will be thrown away regardless: we just want a task that forces the\\nnetwork to model something about word cooccurrence statistics. Predicting\\nleading and trailing characters does that more than adequately, as the exact\\nword sequence could be recovered with high accuracy if the initial and trailing\\ncharacters are predicted accurately. With the vectors objective, the pretraining\\nuses the embedding space learned by an algorithm such as\\n\", _jsx(_components.a, {\n          href: \"https://nlp.stanford.edu/projects/glove/\",\n          children: \"GloVe\"\n        }), \" or\\n\", _jsx(_components.a, {\n          href: \"https://code.google.com/archive/p/word2vec/\",\n          children: \"Word2vec\"\n        }), \", allowing the model to\\nfocus on the contextual modelling we actual care about.\"]\n      })]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{"title":"Embeddings, Transformers and Transfer Learning","teaser":"Using transformer embeddings like BERT in spaCy","menu":[["Embedding Layers","embedding-layers"],["Transformers","transformers"],["Static Vectors","static-vectors"],["Pretraining","pretraining"]],"next":"/usage/training"},"scope":{}},"sectionTitle":"Usage Documentation","theme":"blue","section":"usage","apiDetails":{"stringName":null,"baseClass":null,"trainable":null},"isIndex":false},"__N_SSG":true},"page":"/[...listPathPage]","query":{"listPathPage":["usage","embeddings-transformers"]},"buildId":"Ugre-usgT1EZhnSeYcBR9","isFallback":false,"dynamicIds":[728],"gsp":true,"scriptLoader":[]}</script></body></html>