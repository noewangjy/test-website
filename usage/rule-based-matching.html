<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="sitemap" type="application/xml" href="/sitemap.xml"/><link rel="shortcut icon" href="/icons/icon-192x192.png"/><link rel="manifest" href="/manifest.webmanifest"/><meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=5.0, shrink-to-fit=no, viewport-fit=cover"/><meta name="theme-color" content="#09a3d5"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png"/><title>Rule-based matching · spaCy Usage Documentation</title><meta name="description" content="Find phrases and tokens, and match entities"/><meta property="og:title" content="Rule-based matching · spaCy Usage Documentation"/><meta property="og:description" content="Find phrases and tokens, and match entities"/><meta property="og:type" content="website"/><meta property="og:site_name" content="Rule-based matching"/><meta property="og:image" content="https://noewangjy.github.io/test-website//_next/static/media/social_default.96b04585.jpg"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image" content="https://noewangjy.github.io/test-website//_next/static/media/social_default.96b04585.jpg"/><meta name="twitter:creator" content="@spacy_io"/><meta name="twitter:site" content="@spacy_io"/><meta name="twitter:title" content="Rule-based matching · spaCy Usage Documentation"/><meta name="twitter:description" content="Find phrases and tokens, and match entities"/><meta name="docsearch:language" content="en"/><meta name="next-head-count" content="24"/><link rel="preload" href="/_next/static/css/8f0b94edbc18d62d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8f0b94edbc18d62d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e6995e0e8addcf99.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e6995e0e8addcf99.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script defer="" src="/_next/static/chunks/262.c647d33d06232ef6.js"></script><script defer="" src="/_next/static/chunks/728.cf6ba0da2700fa1b.js"></script><script defer="" src="/_next/static/chunks/4ad82c5e.9f71e347f3d5ee0a.js"></script><script defer="" src="/_next/static/chunks/fec483df.e5c4c2e1905c02db.js"></script><script defer="" src="/_next/static/chunks/d9c63220.b8e7d78d95edcd53.js"></script><script defer="" src="/_next/static/chunks/876.15142163f03ee62c.js"></script><script defer="" src="/_next/static/chunks/661.267b5ccf1a1d90d0.js"></script><script defer="" src="/_next/static/chunks/492.0a716e87ad804aae.js"></script><script defer="" src="/_next/static/chunks/ff39441c.a546763fcfb2e100.js"></script><script defer="" src="/_next/static/chunks/d848df63.589bf0a30c292ffe.js"></script><script defer="" src="/_next/static/chunks/294.7690b28271fa0539.js"></script><script defer="" src="/_next/static/chunks/561.47ba5672af9b582d.js"></script><script src="/_next/static/chunks/webpack-8161fc2bb14cec39.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-a0f603ce323043fd.js" defer=""></script><script src="/_next/static/chunks/pages/_app-ee1fab0e25bb912e.js" defer=""></script><script src="/_next/static/chunks/94-57434c8b7a6c3878.js" defer=""></script><script src="/_next/static/chunks/128-76b45627a109219b.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...listPathPage%5D-45eea57fe8c2902c.js" defer=""></script><script src="/_next/static/2lY2cUyEfZosk4VLgkHb2/_buildManifest.js" defer=""></script><script src="/_next/static/2lY2cUyEfZosk4VLgkHb2/_ssgManifest.js" defer=""></script></head><body class="theme-blue"><div id="__next"><div class="theme-blue"><nav class="navigation_root__yPL8O"><span class="navigation_has-alert__s0Drf"><a class="link_root__1Me7D link_no-link-layout__RPvod" aria-label="spaCy" href="/"><h1 class="navigation_title__pm49s">spaCy</h1></a> <span class="navigation_alert__ZOXon"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/usage/v3-5"><strong>💥 Out now:</strong> spaCy v3.5</a></span></span><div class="navigation_menu__ZMJxN"><select class="dropdown_root__3uiQq navigation_dropdown__4j4pI"><option value="title" disabled="">Menu</option><option value="/usage" selected="">Usage</option><option value="/models">Models</option><option value="/api">API</option><option value="/universe">Universe</option></select><ul class="navigation_list__DCzqi"><li class="navigation_item__ln1O1 navigation_is-active__RjVJG"><a class="link_root__1Me7D link_no-link-layout__RPvod" tabindex="-1" href="/usage">Usage</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/models">Models</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/api">API</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/universe">Universe</a></li><li class="navigation_item__ln1O1 navigation_github__MpFNv"><span><a href="https://github.com/explosion/spaCy" data-size="large" data-show-count="true" aria-label="Star spaCy on GitHub"></a></span></li></ul><div class="navigation_search__BKZCn"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div><progress class="progress_root__9huWN" value="0" max="100"></progress></nav><menu class="sidebar sidebar_root__s2No7"><h1 hidden="" aria-hidden="true" class="h0 sidebar_active-heading___dkf9">Guides</h1><div class="sidebar_dropdown__vyqjz"><select class="dropdown_root__3uiQq sidebar_dropdown-select__Nwbq9"><option disabled="">Select page...</option><option value="/usage">Get started<!-- --> › <!-- -->Installation</option><option value="/usage/models">Get started<!-- --> › <!-- -->Models &amp; Languages</option><option value="/usage/facts-figures">Get started<!-- --> › <!-- -->Facts &amp; Figures</option><option value="/usage/spacy-101">Get started<!-- --> › <!-- -->spaCy 101</option><option value="/usage/v3">Get started<!-- --> › <!-- -->New in v3.0</option><option value="/usage/v3-1">Get started<!-- --> › <!-- -->New in v3.1</option><option value="/usage/v3-2">Get started<!-- --> › <!-- -->New in v3.2</option><option value="/usage/v3-3">Get started<!-- --> › <!-- -->New in v3.3</option><option value="/usage/v3-4">Get started<!-- --> › <!-- -->New in v3.4</option><option value="/usage/v3-5">Get started<!-- --> › <!-- -->New in v3.5</option><option value="/usage/linguistic-features">Guides<!-- --> › <!-- -->Linguistic Features</option><option value="/usage/rule-based-matching" selected="">Guides<!-- --> › <!-- -->Rule-based Matching</option><option value="/usage/processing-pipelines">Guides<!-- --> › <!-- -->Processing Pipelines</option><option value="/usage/embeddings-transformers">Guides<!-- --> › <!-- -->Embeddings &amp; Transformers</option><option value="/usage/training">Guides<!-- --> › <!-- -->Training Models</option><option value="/usage/layers-architectures">Guides<!-- --> › <!-- -->Layers &amp; Model Architectures</option><option value="/usage/projects">Guides<!-- --> › <!-- -->spaCy Projects</option><option value="/usage/saving-loading">Guides<!-- --> › <!-- -->Saving &amp; Loading</option><option value="/usage/visualizers">Guides<!-- --> › <!-- -->Visualizers</option><option value="https://github.com/explosion/projects">Resources<!-- --> › <!-- -->Project Templates</option><option value="https://v2.spacy.io">Resources<!-- --> › <!-- -->v2.x Documentation</option><option value="https://explosion.ai/custom-solutions">Resources<!-- --> › <!-- -->Custom Solutions</option></select></div><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Get started</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage">Installation</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/models">Models &amp; Languages</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/facts-figures">Facts &amp; Figures</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/spacy-101">spaCy 101</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3">New in v3.0</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-1">New in v3.1</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-2">New in v3.2</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-3">New in v3.3</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-4">New in v3.4</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-5">New in v3.5</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Guides</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/linguistic-features">Linguistic Features</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP sidebar_is-active__yVTtL is-active" href="/usage/rule-based-matching">Rule-based Matching</a><ul class="sidebar_crumbs__NhM2y"><li class="sidebar_crumb__tiiDl sidebar_crumb-active__zq8BI"><a href="#matcher">Token Matcher</a></li><li class="sidebar_crumb__tiiDl"><a href="#phrasematcher">Phrase Matcher</a></li><li class="sidebar_crumb__tiiDl"><a href="#dependencymatcher">Dependency Matcher</a></li><li class="sidebar_crumb__tiiDl"><a href="#entityruler">Entity Ruler</a></li><li class="sidebar_crumb__tiiDl"><a href="#spanruler">Span Ruler</a></li><li class="sidebar_crumb__tiiDl"><a href="#models-rules">Models &amp; Rules</a></li></ul></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/processing-pipelines">Processing Pipelines</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/embeddings-transformers">Embeddings &amp; Transformers<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/training">Training Models<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/layers-architectures">Layers &amp; Model Architectures<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/projects">spaCy Projects<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/saving-loading">Saving &amp; Loading</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/visualizers">Visualizers</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Resources</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/projects">Project Templates</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="https://v2.spacy.io">v2.x Documentation</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="https://explosion.ai/custom-solutions">Custom Solutions</a></li></ul></menu><main class="main_root__7f6Tj main_with-sidebar__uH1df main_with-asides__ikQT6"><article class="main_content__8zFCH"><header class="title_root__pS2WQ"><h1 id="_title" class="typography_heading__D82WZ typography_h1__b7dt9 title_h1__l3CW1"><span class="heading-text">Rule-based matching<!-- --> </span></h1><div class="heading-teaser title_teaser__QhwCH">Find phrases and tokens, and match entities</div></header><section class="section_root__k1hUl"><p>Compared to using regular expressions on raw text, spaCy’s rule-based matcher
engines and components not only let you find the words and phrases you’re
looking for – they also give you access to the tokens within the document and
their relationships. This means you can easily access and analyze the
surrounding tokens, merge spans into single tokens or add entries to the named
entities in <code class="code_inline-code__Bq7ot">doc.ents</code>.</p><section class="accordion" id="rules-vs-model"><div class="accordion_root__pPltq"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">Should I use rules or train a model?</span><a class="link_root__1Me7D accordion_anchor__kidBh link_no-link-layout__RPvod" href="/usage/rule-based-matching#rules-vs-model">¶</a></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><p>For complex tasks, it’s usually better to train a statistical entity recognition
model. However, statistical models require training data, so for many
situations, rule-based approaches are more practical. This is especially true at
the start of a project: you can use a rule-based approach as part of a data
collection process, to help you “bootstrap” a statistical model.</p><p>Training a model is useful if you have some examples and you want your system to
be able to <strong>generalize</strong> based on those examples. It works especially well if
there are clues in the <em>local context</em>. For instance, if you’re trying to detect
person or company names, your application may benefit from a statistical named
entity recognition model.</p><p>Rule-based systems are a good choice if there’s a more or less <strong>finite number</strong>
of examples that you want to find in the data, or if there’s a very <strong>clear,
structured pattern</strong> you can express with token rules or regular expressions.
For instance, country names, IP addresses or URLs are things you might be able
to handle well with a purely rule-based approach.</p><p>You can also combine both approaches and improve a statistical model with rules
to handle very specific cases and boost accuracy. For details, see the section
on <a class="link_root__1Me7D" href="/usage/rule-based-matching#entityruler">rule-based entity recognition</a>.</p></div></div></section><section class="accordion" id="matcher-vs-phrase-matcher"><div class="accordion_root__pPltq"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">When should I use the token matcher vs. the phrase matcher?</span><a class="link_root__1Me7D accordion_anchor__kidBh link_no-link-layout__RPvod" href="/usage/rule-based-matching#matcher-vs-phrase-matcher">¶</a></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><p>The <code class="code_inline-code__Bq7ot">PhraseMatcher</code> is useful if you already have a large terminology list or
gazetteer consisting of single or multi-token phrases that you want to find
exact instances of in your data. As of spaCy v2.1.0, you can also match on the
<code class="code_inline-code__Bq7ot">LOWER</code> attribute for fast and case-insensitive matching.</p><p>The <code class="code_inline-code__Bq7ot">Matcher</code> isn’t as blazing fast as the <code class="code_inline-code__Bq7ot">PhraseMatcher</code>, since it compares
across individual token attributes. However, it allows you to write very
abstract representations of the tokens you’re looking for, using lexical
attributes, linguistic features predicted by the model, operators, set
membership and rich comparison. For example, you can find a noun, followed by a
verb with the lemma “love” or “like”, followed by an optional determiner and
another token that’s at least 10 characters long.</p></div></div></section></section>
<section id="section-matcher" class="section_root__k1hUl"><h2 id="matcher" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#matcher" class="heading-text typography_permalink__UiIRy">Token-based matching <!-- --> </a></h2><p>spaCy features a rule-matching engine, the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/matcher"><code class="code_inline-code__Bq7ot">Matcher</code></a>, that
operates over tokens, similar to regular expressions. The rules can refer to
token annotations (e.g. the token <code class="code_inline-code__Bq7ot">text</code> or <code class="code_inline-code__Bq7ot">tag_</code>, and flags like <code class="code_inline-code__Bq7ot">IS_PUNCT</code>).
The rule matcher also lets you pass in a custom callback to act on matches – for
example, to merge entities and apply custom labels. You can also associate
patterns with entity IDs, to allow some basic entity linking or disambiguation.
To match large terminology lists, you can use the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/phrasematcher"><code class="code_inline-code__Bq7ot">PhraseMatcher</code></a>, which accepts <code class="code_inline-code__Bq7ot">Doc</code> objects as match
patterns.</p><h3 id="adding-patterns" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#adding-patterns" class="heading-text typography_permalink__UiIRy">Adding patterns <!-- --> </a></h3><p>Let’s say we want to enable spaCy to find a combination of three tokens:</p><ol class="list_ol__aclSa">
<li class="list_li__sfx_z">A token whose <strong>lowercase form matches “hello”</strong>, e.g. “Hello” or “HELLO”.</li>
<li class="list_li__sfx_z">A token whose <strong><code class="code_inline-code__Bq7ot">is_punct</code> flag is set to <code class="code_inline-code__Bq7ot">True</code></strong>, i.e. any punctuation.</li>
<li class="list_li__sfx_z">A token whose <strong>lowercase form matches “world”</strong>, e.g. “World” or “WORLD”.</li>
</ol><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><aside class="infobox_root__yNIMg infobox_danger__a_J18"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>When writing patterns, keep in mind that <strong>each dictionary</strong> represents <strong>one
token</strong>. If spaCy’s tokenization doesn’t match the tokens defined in a pattern,
the pattern is not going to produce any results. When developing complex
patterns, make sure to check examples against spaCy’s tokenization:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre></aside><p>First, we initialize the <code class="code_inline-code__Bq7ot">Matcher</code> with a vocab. The matcher must always share
the same vocab with the documents it will operate on. We can now call
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/matcher#add"><code class="code_inline-code__Bq7ot">matcher.add()</code></a> with an ID and a list of patterns.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>The matcher returns a list of <code class="code_inline-code__Bq7ot">(match_id, start, end)</code> tuples – in this case,
<code class="code_inline-code__Bq7ot code_wrap__b41os">[(&#x27;15578876784678163569&#x27;, 0, 3)]</code>, which maps to the span <code class="code_inline-code__Bq7ot">doc[0:3]</code> of our
original document. The <code class="code_inline-code__Bq7ot">match_id</code> is the <a class="link_root__1Me7D" href="/usage/spacy-101#vocab">hash value</a> of
the string ID “HelloWorld”. To get the string value, you can look up the ID in
the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/stringstore"><code class="code_inline-code__Bq7ot">StringStore</code></a>.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>Optionally, we could also choose to add more than one pattern, for example to
also match sequences without punctuation between “hello” and “world”:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>By default, the matcher will only return the matches and <strong>not do anything
else</strong>, like merge entities or assign labels. This is all up to you and can be
defined individually for each pattern, by passing in a callback function as the
<code class="code_inline-code__Bq7ot">on_match</code> argument on <code class="code_inline-code__Bq7ot">add()</code>. This is useful, because it lets you write
entirely custom and <strong>pattern-specific logic</strong>. For example, you might want to
merge <em>some</em> patterns into one token, while adding entity labels for other
pattern types. You shouldn’t have to create different matchers for each of those
processes.</p><h4 id="adding-patterns-attributes" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#adding-patterns-attributes" class="heading-text typography_permalink__UiIRy">Available token attributes <!-- --> </a></h4><p>The available token pattern keys correspond to a number of
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#attributes"><code class="code_inline-code__Bq7ot">Token</code> attributes</a>. The supported attributes for
rule-based matching are:</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Attribute</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">ORTH</code></td><td class="table_td__rmpJx">The exact verbatim text of a token. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">TEXT</code></td><td class="table_td__rmpJx">The exact verbatim text of a token. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">NORM</code></td><td class="table_td__rmpJx">The normalized form of the token text. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">LOWER</code></td><td class="table_td__rmpJx">The lowercase form of the token text. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">LENGTH</code></td><td class="table_td__rmpJx">The length of the token text. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">int</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">IS_ALPHA</code>, <code class="code_inline-code__Bq7ot">IS_ASCII</code>, <code class="code_inline-code__Bq7ot">IS_DIGIT</code></td><td class="table_td__rmpJx">Token text consists of alphabetic characters, ASCII characters, digits. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">bool</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">IS_LOWER</code>, <code class="code_inline-code__Bq7ot">IS_UPPER</code>, <code class="code_inline-code__Bq7ot">IS_TITLE</code></td><td class="table_td__rmpJx">Token text is in lowercase, uppercase, titlecase. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">bool</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">IS_PUNCT</code>, <code class="code_inline-code__Bq7ot">IS_SPACE</code>, <code class="code_inline-code__Bq7ot">IS_STOP</code></td><td class="table_td__rmpJx">Token is punctuation, whitespace, stop word. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">bool</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">IS_SENT_START</code></td><td class="table_td__rmpJx">Token is start of sentence. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">bool</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">LIKE_NUM</code>, <code class="code_inline-code__Bq7ot">LIKE_URL</code>, <code class="code_inline-code__Bq7ot">LIKE_EMAIL</code></td><td class="table_td__rmpJx">Token text resembles a number, URL, email. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">bool</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">SPACY</code></td><td class="table_td__rmpJx">Token has a trailing space. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">bool</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">POS</code>, <code class="code_inline-code__Bq7ot">TAG</code>, <code class="code_inline-code__Bq7ot">MORPH</code>, <code class="code_inline-code__Bq7ot">DEP</code>, <code class="code_inline-code__Bq7ot">LEMMA</code>, <code class="code_inline-code__Bq7ot">SHAPE</code></td><td class="table_td__rmpJx">The token’s simple and extended part-of-speech tag, morphological analysis, dependency label, lemma, shape. Note that the values of these attributes are case-sensitive. For a list of available part-of-speech tags and dependency labels, see the <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/annotation"><span class="link_source-text__VDP74">Annotation Specifications</span></a>. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">ENT_TYPE</code></td><td class="table_td__rmpJx">The token’s entity label. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">_</code></td><td class="table_td__rmpJx">Properties in <a class="link_root__1Me7D" href="/usage/processing-pipelines#custom-components-attributes">custom extension attributes</a>. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Dict<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">,</span> Any<span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">OP</code></td><td class="table_td__rmpJx"><a class="link_root__1Me7D" href="/usage/rule-based-matching#quantifiers">Operator or quantifier</a> to determine how often to match a token pattern. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr></tbody></table><section class="accordion"><div class="accordion_root__pPltq"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">Does it matter if the attribute names are uppercase or lowercase?</span></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><p>No, it shouldn’t. spaCy will normalize the names internally and
<code class="code_inline-code__Bq7ot">{&quot;LOWER&quot;: &quot;text&quot;}</code> and <code class="code_inline-code__Bq7ot">{&quot;lower&quot;: &quot;text&quot;}</code> will both produce the same result.
Using the uppercase version is mostly a convention to make it clear that the
attributes are “special” and don’t exactly map to the token attributes like
<code class="code_inline-code__Bq7ot">Token.lower</code> and <code class="code_inline-code__Bq7ot">Token.lower_</code>.</p></div></div></section><section class="accordion"><div class="accordion_root__pPltq"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">Why are not all token attributes supported?</span></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><p>spaCy can’t provide access to all of the attributes because the <code class="code_inline-code__Bq7ot">Matcher</code> loops
over the Cython data, not the Python objects. Inside the matcher, we’re dealing
with a <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cython-structs#tokenc"><code class="code_inline-code__Bq7ot">TokenC</code> struct</a> – we don’t have an instance
of <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token"><code class="code_inline-code__Bq7ot">Token</code></a>. This means that all of the attributes that refer to
computed properties can’t be accessed.</p><p>The uppercase attribute names like <code class="code_inline-code__Bq7ot">LOWER</code> or <code class="code_inline-code__Bq7ot">IS_PUNCT</code> refer to symbols from
the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/attrs.pyx"><code class="code_inline-code__Bq7ot">spacy.attrs</code></a> enum table. They’re passed
into a function that essentially is a big case/switch statement, to figure out
which struct field to return. The same attribute identifiers are used in
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc#to_array"><code class="code_inline-code__Bq7ot">Doc.to_array</code></a>, and a few other places in the code where
you need to describe fields like this.</p></div></div></section><hr class="section_hr__07Hes"/><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span>Tip: Try the interactive matcher explorer</span></h4><figure class="gatsby-resp-image-figure"><a class="link_root__1Me7D gatsby-resp-image-link embed_image-link__loaAa link_no-link-layout__RPvod" href="https://explosion.ai/demos/matcher"><img class="embed_image__mSQUH" src="/images/matcher-demo.jpg" alt="Matcher demo" width="650" height="auto"/></a></figure><p>The <a class="link_root__1Me7D" href="https://explosion.ai/demos/matcher">Matcher Explorer</a> lets you test the
rule-based <code class="code_inline-code__Bq7ot">Matcher</code> by creating token patterns interactively and running them
over your text. Each token can set multiple attributes like text value,
part-of-speech tag or boolean flags. The token-based view lets you explore how
spaCy processes your text – and why your pattern matches, or why it doesn’t.</p></aside><h4 id="adding-patterns-attributes-extended" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#adding-patterns-attributes-extended" class="heading-text typography_permalink__UiIRy">Extended pattern syntax and attributes <!-- --> </a></h4><p>Instead of mapping to a single value, token patterns can also map to a
<strong>dictionary of properties</strong>. For example, to specify that the value of a lemma
should be part of a list of values, or to set a minimum character length. The
following rich comparison attributes are available:</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Attribute</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">IN</code></td><td class="table_td__rmpJx">Attribute value is member of a list. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Any</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">NOT_IN</code></td><td class="table_td__rmpJx">Attribute value is <em>not</em> member of a list. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Any</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">IS_SUBSET</code></td><td class="table_td__rmpJx">Attribute value (for <code class="code_inline-code__Bq7ot">MORPH</code> or custom list attributes) is a subset of a list. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Any</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">IS_SUPERSET</code></td><td class="table_td__rmpJx">Attribute value (for <code class="code_inline-code__Bq7ot">MORPH</code> or custom list attributes) is a superset of a list. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Any</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">INTERSECTS</code></td><td class="table_td__rmpJx">Attribute value (for <code class="code_inline-code__Bq7ot">MORPH</code> or custom list attributes) has a non-empty intersection with a list. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Any</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">==</code>, <code class="code_inline-code__Bq7ot">&gt;=</code>, <code class="code_inline-code__Bq7ot">&lt;=</code>, <code class="code_inline-code__Bq7ot">&gt;</code>, <code class="code_inline-code__Bq7ot">&lt;</code></td><td class="table_td__rmpJx">Attribute value is equal, greater or equal, smaller or equal, greater or smaller. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Union<span class="code_cli-arg-subtle__IgB5m">[</span>int<span class="code_cli-arg-subtle__IgB5m">,</span> float<span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr></tbody></table><h4 id="regex" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#regex" class="heading-text typography_permalink__UiIRy">Regular expressions <!-- --> </a></h4><p>In some cases, only matching tokens and token attributes isn’t enough – for
example, you might want to match different spellings of a word, without having
to add a new pattern for each spelling.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>The <code class="code_inline-code__Bq7ot">REGEX</code> operator allows defining rules for any attribute string value,
including custom attributes. It always needs to be applied to an attribute like
<code class="code_inline-code__Bq7ot">TEXT</code>, <code class="code_inline-code__Bq7ot">LOWER</code> or <code class="code_inline-code__Bq7ot">TAG</code>:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>When using the <code class="code_inline-code__Bq7ot">REGEX</code> operator, keep in mind that it operates on <strong>single
tokens</strong>, not the whole text. Each expression you provide will be matched on a
token. If you need to match on the whole text instead, see the details on
<a class="link_root__1Me7D" href="/usage/rule-based-matching#regex-text">regex matching on the whole text</a>.</p></aside><h5 id="regex-text" class="typography_heading__D82WZ typography_h5__JbE6d"><a href="#regex-text" class="heading-text typography_permalink__UiIRy">Matching regular expressions on the full text <!-- --> </a></h5><p>If your expressions apply to multiple tokens, a simple solution is to match on
the <code class="code_inline-code__Bq7ot">doc.text</code> with <code class="code_inline-code__Bq7ot">re.finditer</code> and use the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc#char_span"><code class="code_inline-code__Bq7ot">Doc.char_span</code></a> method to create a <code class="code_inline-code__Bq7ot">Span</code> from the
character indices of the match. If the matched characters don’t map to one or
more valid tokens, <code class="code_inline-code__Bq7ot">Doc.char_span</code> returns <code class="code_inline-code__Bq7ot">None</code>.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">What’s a valid token sequence?<!-- --> </span></h4>
<p>In the example, the expression will also match <code class="code_inline-code__Bq7ot">&quot;US&quot;</code> in <code class="code_inline-code__Bq7ot">&quot;USA&quot;</code>. However,
<code class="code_inline-code__Bq7ot">&quot;USA&quot;</code> is a single token and <code class="code_inline-code__Bq7ot">Span</code> objects are <strong>sequences of tokens</strong>. So
<code class="code_inline-code__Bq7ot">&quot;US&quot;</code> cannot be its own span, because it does not end on a token boundary.</p>
</div></div></aside><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><section class="accordion"><div class="accordion_root__pPltq"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">How can I expand the match to a valid token sequence?</span></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><p>In some cases, you might want to expand the match to the closest token
boundaries, so you can create a <code class="code_inline-code__Bq7ot">Span</code> for <code class="code_inline-code__Bq7ot">&quot;USA&quot;</code>, even though only the
substring <code class="code_inline-code__Bq7ot">&quot;US&quot;</code> is matched. You can calculate this using the character offsets
of the tokens in the document, available as
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#attributes"><code class="code_inline-code__Bq7ot">Token.idx</code></a>. This lets you create a list of valid token
start and end boundaries and leaves you with a rather basic algorithmic problem:
Given a number, find the next lowest (start token) or the next highest (end
token) number that’s part of a given list of numbers. This will be the closest
valid token boundary.</p><p>There are many ways to do this and the most straightforward one is to create a
dict keyed by characters in the <code class="code_inline-code__Bq7ot">Doc</code>, mapped to the token they’re part of. It’s
easy to write and less error-prone, and gives you a constant lookup time: you
only ever need to create the dict once per <code class="code_inline-code__Bq7ot">Doc</code>.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>You can then look up character at a given position, and get the index of the
corresponding token that the character is part of. Your span would then be
<code class="code_inline-code__Bq7ot">doc[token_start:token_end]</code>. If a character isn’t in the dict, it means it’s
the (white)space tokens are split on. That hopefully shouldn’t happen, though,
because it’d mean your regex is producing matches with leading or trailing
whitespace.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python code_wrap__b41os"></code></pre></div></div></section><h4 id="fuzzy" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#fuzzy" class="heading-text typography_permalink__UiIRy">Fuzzy matching <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="This feature is new and was introduced in spaCy v3.5">v<!-- -->3.5</span></h4><p>Fuzzy matching allows you to match tokens with alternate spellings, typos, etc.
without specifying every possible variant.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>The <code class="code_inline-code__Bq7ot">FUZZY</code> attribute allows fuzzy matches for any attribute string value,
including custom attributes. Just like <code class="code_inline-code__Bq7ot">REGEX</code>, it always needs to be applied to
an attribute like <code class="code_inline-code__Bq7ot">TEXT</code> or <code class="code_inline-code__Bq7ot">LOWER</code>. By default <code class="code_inline-code__Bq7ot">FUZZY</code> allows a Levenshtein
edit distance of at least 2 and up to 30% of the pattern string length. Using
the more specific attributes <code class="code_inline-code__Bq7ot">FUZZY1</code>..<code class="code_inline-code__Bq7ot">FUZZY9</code> you can specify the maximum
allowed edit distance directly.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><h4 id="regex-fuzzy-lists" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#regex-fuzzy-lists" class="heading-text typography_permalink__UiIRy">Regex and fuzzy matching with lists <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="This feature is new and was introduced in spaCy v3.5">v<!-- -->3.5</span></h4><p>Starting in spaCy v3.5, both <code class="code_inline-code__Bq7ot">REGEX</code> and <code class="code_inline-code__Bq7ot">FUZZY</code> can be combined with the
attributes <code class="code_inline-code__Bq7ot">IN</code> and <code class="code_inline-code__Bq7ot">NOT_IN</code>:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><hr class="section_hr__07Hes"/><h4 id="quantifiers" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#quantifiers" class="heading-text typography_permalink__UiIRy">Operators and quantifiers <!-- --> </a></h4><p>The matcher also lets you use quantifiers, specified as the <code class="code_inline-code__Bq7ot">&#x27;OP&#x27;</code> key.
Quantifiers let you define sequences of tokens to be matched, e.g. one or more
punctuation marks, or specify optional tokens. Note that there are no nested or
scoped quantifiers – instead, you can build those behaviors with <code class="code_inline-code__Bq7ot">on_match</code>
callbacks.</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">OP</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">!</code></td><td class="table_td__rmpJx">Negate the pattern, by requiring it to match exactly 0 times.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">?</code></td><td class="table_td__rmpJx">Make the pattern optional, by allowing it to match 0 or 1 times.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">+</code></td><td class="table_td__rmpJx">Require the pattern to match 1 or more times.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">*</code></td><td class="table_td__rmpJx">Allow the pattern to match zero or more times.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">{n}</code></td><td class="table_td__rmpJx">Require the pattern to match exactly <em>n</em> times.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">{n,m}</code></td><td class="table_td__rmpJx">Require the pattern to match at least <em>n</em> but not more than <em>m</em> times.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">{n,}</code></td><td class="table_td__rmpJx">Require the pattern to match at least <em>n</em> times.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">{,m}</code></td><td class="table_td__rmpJx">Require the pattern to match at most <em>m</em> times.</td></tr></tbody></table><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Note on operator behaviour</span></h4><p>In versions before v2.1.0, the semantics of the <code class="code_inline-code__Bq7ot">+</code> and <code class="code_inline-code__Bq7ot">*</code> operators behave
inconsistently. They were usually interpreted “greedily”, i.e. longer matches
are returned where possible. However, if you specify two <code class="code_inline-code__Bq7ot">+</code> and <code class="code_inline-code__Bq7ot">*</code> patterns in
a row and their matches overlap, the first operator will behave non-greedily.
This quirk in the semantics is corrected in spaCy v2.1.0.</p></aside><h4 id="adding-patterns-wildcard" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#adding-patterns-wildcard" class="heading-text typography_permalink__UiIRy">Using wildcard token patterns <!-- --> </a></h4><p>While the token attributes offer many options to write highly specific patterns,
you can also use an empty dictionary, <code class="code_inline-code__Bq7ot">{}</code> as a wildcard representing <strong>any
token</strong>. This is useful if you know the context of what you’re trying to match,
but very little about the specific token and its characters. For example, let’s
say you’re trying to extract people’s user names from your data. All you know is
that they are listed as “User name: {username}“. The name itself may contain
any character, but no whitespace – so you’ll know it will be handled as one
token.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><h4 id="pattern-validation" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#pattern-validation" class="heading-text typography_permalink__UiIRy">Validating and debugging patterns <!-- --> </a></h4><p>The <code class="code_inline-code__Bq7ot">Matcher</code> can validate patterns against a JSON schema with the option
<code class="code_inline-code__Bq7ot">validate=True</code>. This is useful for debugging patterns during development, in
particular for catching unsupported attributes.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h3 id="on_match" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#on_match" class="heading-text typography_permalink__UiIRy">Adding on_match rules <!-- --> </a></h3><p>To move on to a more realistic example, let’s say you’re working with a large
corpus of blog articles, and you want to match all mentions of “Google I/O”
(which spaCy tokenizes as <code class="code_inline-code__Bq7ot">[&#x27;Google&#x27;, &#x27;I&#x27;, &#x27;/&#x27;, &#x27;O&#x27;</code>]). To be safe, you only
match on the uppercase versions, avoiding matches with phrases such as “Google
i/o”.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>A very similar logic has been implemented in the built-in
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/entityruler"><code class="code_inline-code__Bq7ot">EntityRuler</code></a> by the way. It also takes care of handling
overlapping matches, which you would otherwise have to take care of yourself.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Tip: Visualizing matches<!-- --> </span></h4>
<p>When working with entities, you can use <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/top-level#displacy"><span class="link_source-text__VDP74">displaCy</span></a> to
quickly generate a NER visualization from your updated <code class="code_inline-code__Bq7ot">Doc</code>, which can be
exported as an HTML file:</p>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
<p>For more info and examples, see the usage guide on
<a class="link_root__1Me7D" href="/usage/visualizers">visualizing spaCy</a>.</p>
</div></div></aside><p>We can now call the matcher on our documents. The patterns will be matched in
the order they occur in the text. The matcher will then iterate over the
matches, look up the callback for the match ID that was matched, and invoke it.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>When the callback is invoked, it is passed four arguments: the matcher itself,
the document, the position of the current match, and the total list of matches.
This allows you to write callbacks that consider the entire set of matched
phrases, so that you can resolve overlaps and other conflicts in whatever way
you prefer.</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Argument</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">matcher</code></td><td class="table_td__rmpJx">The matcher instance. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" href="/api/matcher">Matcher</a></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">doc</code></td><td class="table_td__rmpJx">The document the matcher was used on. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" href="/api/doc">Doc</a></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">i</code></td><td class="table_td__rmpJx">Index of the current match (<code class="code_inline-code__Bq7ot">matches[i</code>]). <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">int</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">matches</code></td><td class="table_td__rmpJx">A list of <code class="code_inline-code__Bq7ot">(match_id, start, end)</code> tuples, describing the matches. A match tuple describes a span <code class="code_inline-code__Bq7ot">doc[start:end</code>]. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">List<span class="code_cli-arg-subtle__IgB5m">[</span>Tuple<span class="code_cli-arg-subtle__IgB5m">[</span>int<span class="code_cli-arg-subtle__IgB5m">,</span> int int<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr></tbody></table><h3 id="matcher-spans" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#matcher-spans" class="heading-text typography_permalink__UiIRy">Creating spans from matches <!-- --> </a></h3><p>Creating <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/span"><code class="code_inline-code__Bq7ot">Span</code></a> objects from the returned matches is a very common
use case. spaCy makes this easy by giving you access to the <code class="code_inline-code__Bq7ot">start</code> and <code class="code_inline-code__Bq7ot">end</code>
token of each match, which you can use to construct a new span with an optional
label. As of spaCy v3.0, you can also set <code class="code_inline-code__Bq7ot">as_spans=True</code> when calling the
matcher on a <code class="code_inline-code__Bq7ot">Doc</code>, which will return a list of <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/span"><code class="code_inline-code__Bq7ot">Span</code></a> objects
using the <code class="code_inline-code__Bq7ot">match_id</code> as the span label.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h3 id="matcher-pipeline" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#matcher-pipeline" class="heading-text typography_permalink__UiIRy">Using custom pipeline components <!-- --> </a></h3><p>Let’s say your data also contains some annoying pre-processing artifacts, like
leftover HTML line breaks (e.g. <code class="code_inline-code__Bq7ot">&lt;br&gt;</code> or <code class="code_inline-code__Bq7ot">&lt;BR/&gt;</code>). To make your text easier to
analyze, you want to merge those into one token and flag them, to make sure you
can ignore them later. Ideally, this should all be done automatically as you
process the text. You can achieve this by adding a
<a class="link_root__1Me7D" href="/usage/processing-pipelines#custom-components">custom pipeline component</a>
that’s called on each <code class="code_inline-code__Bq7ot">Doc</code> object, merges the leftover HTML spans and sets an
attribute <code class="code_inline-code__Bq7ot">bad_html</code> on the token.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>Instead of hard-coding the patterns into the component, you could also make it
take a path to a JSON file containing the patterns. This lets you reuse the
component with different patterns, depending on your application. When adding
the component to the pipeline with <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#add_pipe"><code class="code_inline-code__Bq7ot">nlp.add_pipe</code></a>, you
can pass in the argument via the <code class="code_inline-code__Bq7ot">config</code>:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">📖</span>Processing pipelines</span></h4><p>For more details and examples of how to <strong>create custom pipeline components</strong>
and <strong>extension attributes</strong>, see the
<a class="link_root__1Me7D" href="/usage/processing-pipelines">usage guide</a>.</p></aside><h3 id="example1" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#example1" class="heading-text typography_permalink__UiIRy">Example: Using linguistic annotations <!-- --> </a></h3><p>Let’s say you’re analyzing user comments and you want to find out what people
are saying about Facebook. You want to start off by finding adjectives following
“Facebook is” or “Facebook was”. This is obviously a very rudimentary solution,
but it’ll be fast, and a great way to get an idea for what’s in your data. Your
pattern could look like this:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>This translates to a token whose lowercase form matches “facebook” (like
Facebook, facebook or FACEBOOK), followed by a token with the lemma “be” (for
example, is, was, or ‘s), followed by an <strong>optional</strong> adverb, followed by an
adjective. Using the linguistic annotations here is especially useful, because
you can tell spaCy to match “Facebook’s annoying”, but <strong>not</strong> “Facebook’s
annoying ads”. The optional adverb makes sure you won’t miss adjectives with
intensifiers, like “pretty awful” or “very nice”.</p><p>To get a quick overview of the results, you could collect all sentences
containing a match and render them with the
<a class="link_root__1Me7D" href="/usage/visualizers">displaCy visualizer</a>. In the callback function, you’ll have
access to the <code class="code_inline-code__Bq7ot">start</code> and <code class="code_inline-code__Bq7ot">end</code> of each match, as well as the parent <code class="code_inline-code__Bq7ot">Doc</code>. This
lets you determine the sentence containing the match, <code class="code_inline-code__Bq7ot">doc[start:end].sent</code>, and
calculate the start and end of the matched span within the sentence. Using
displaCy in <a class="link_root__1Me7D" href="/usage/visualizers#manual-usage">“manual” mode</a> lets you pass in a
list of dictionaries containing the text and entities to render.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h3 id="example2" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#example2" class="heading-text typography_permalink__UiIRy">Example: Phone numbers <!-- --> </a></h3><p>Phone numbers can have many different formats and matching them is often tricky.
During tokenization, spaCy will leave sequences of numbers intact and only split
on whitespace and punctuation. This means that your match pattern will have to
look out for number sequences of a certain length, surrounded by specific
punctuation – depending on the
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/National_conventions_for_writing_telephone_numbers">national conventions</a>.</p><p>The <code class="code_inline-code__Bq7ot">IS_DIGIT</code> flag is not very helpful here, because it doesn’t tell us
anything about the length. However, you can use the <code class="code_inline-code__Bq7ot">SHAPE</code> flag, with each <code class="code_inline-code__Bq7ot">d</code>
representing a digit (up to 4 digits / characters):</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>This will match phone numbers of the format <strong>(123) 4567 8901</strong> or <strong>(123)
4567-8901</strong>. To also match formats like <strong>(123) 456 789</strong>, you can add a second
pattern using <code class="code_inline-code__Bq7ot">&#x27;ddd&#x27;</code> in place of <code class="code_inline-code__Bq7ot">&#x27;dddd&#x27;</code>. By hard-coding some values, you can
match only certain, country-specific numbers. For example, here’s a pattern to
match the most common formats of
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/National_conventions_for_writing_telephone_numbers#Germany">international German numbers</a>:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>Depending on the formats your application needs to match, creating an extensive
set of rules like this is often better than training a model. It’ll produce more
predictable results, is much easier to modify and extend, and doesn’t require
any training data – only a set of test cases.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h3 id="example3" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#example3" class="heading-text typography_permalink__UiIRy">Example: Hashtags and emoji on social media <!-- --> </a></h3><p>Social media posts, especially tweets, can be difficult to work with. They’re
very short and often contain various emoji and hashtags. By only looking at the
plain text, you’ll lose a lot of valuable semantic information.</p><p>Let’s say you’ve extracted a large sample of social media posts on a specific
topic, for example posts mentioning a brand name or product. As the first step
of your data exploration, you want to filter out posts containing certain emoji
and use them to assign a general sentiment score, based on whether the expressed
emotion is positive or negative, e.g. 😀 or 😞. You also want to find, merge and
label hashtags like <code class="code_inline-code__Bq7ot">#MondayMotivation</code>, to be able to ignore or analyze them
later.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Note on sentiment analysis<!-- --> </span></h4>
<p>Ultimately, sentiment analysis is not always <em>that</em> easy. In addition to the
emoji, you’ll also want to take specific words into account and check the
<code class="code_inline-code__Bq7ot">subtree</code> for intensifiers like “very”, to increase the sentiment score. At
some point, you might also want to train a sentiment model. However, the
approach described in this example is very useful for <strong>bootstrapping rules to
collect training data</strong>. It’s also an incredibly fast way to gather first
insights into your data – with about 1 million tweets, you’d be looking at a
processing time of <strong>under 1 minute</strong>.</p>
</div></div></aside><p>By default, spaCy’s tokenizer will split emoji into separate tokens. This means
that you can create a pattern for one or more emoji tokens. Valid hashtags
usually consist of a <code class="code_inline-code__Bq7ot">#</code>, plus a sequence of ASCII characters with no
whitespace, making them easy to match as well.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>Because the <code class="code_inline-code__Bq7ot">on_match</code> callback receives the ID of each match, you can use the
same function to handle the sentiment assignment for both the positive and
negative pattern. To keep it simple, we’ll either add or subtract <code class="code_inline-code__Bq7ot">0.1</code> points –
this way, the score will also reflect combinations of emoji, even positive <em>and</em>
negative ones.</p><p>With a library like <a class="link_root__1Me7D link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/bcongdon/python-emojipedia"><span class="link_source-text__VDP74">Emojipedia</span></a>,
we can also retrieve a short description for each emoji – for example, 😍‘s
official title is “Smiling Face With Heart-Eyes”. Assigning it to a
<a class="link_root__1Me7D" href="/usage/processing-pipelines#custom-components-attributes">custom attribute</a> on
the emoji span will make it available as <code class="code_inline-code__Bq7ot">span._.emoji_desc</code>.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>To label the hashtags, we can use a
<a class="link_root__1Me7D" href="/usage/processing-pipelines#custom-components-attributes">custom attribute</a> set
on the respective token:</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre></section>
<section id="section-phrasematcher" class="section_root__k1hUl"><h2 id="phrasematcher" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#phrasematcher" class="heading-text typography_permalink__UiIRy">Efficient phrase matching <!-- --> </a></h2><p>If you need to match large terminology lists, you can also use the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/phrasematcher"><code class="code_inline-code__Bq7ot">PhraseMatcher</code></a> and create <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> objects
instead of token patterns, which is much more efficient overall. The <code class="code_inline-code__Bq7ot">Doc</code>
patterns can contain single or multiple tokens.</p><h3 id="adding-phrase-patterns" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#adding-phrase-patterns" class="heading-text typography_permalink__UiIRy">Adding phrase patterns <!-- --> </a></h3><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>Since spaCy is used for processing both the patterns and the text to be matched,
you won’t have to worry about specific tokenization – for example, you can
simply pass in <code class="code_inline-code__Bq7ot">nlp(&quot;Washington, D.C.&quot;)</code> and won’t have to write a complex token
pattern covering the exact tokenization of the term.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note on creating patterns</span></h4><p>To create the patterns, each phrase has to be processed with the <code class="code_inline-code__Bq7ot">nlp</code> object.
If you have a trained pipeline loaded, doing this in a loop or list
comprehension can easily become inefficient and slow. If you <strong>only need the
tokenization and lexical attributes</strong>, you can run
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#make_doc"><code class="code_inline-code__Bq7ot">nlp.make_doc</code></a> instead, which will only run the
tokenizer. For an additional speed boost, you can also use the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/tokenizer#pipe"><code class="code_inline-code__Bq7ot">nlp.tokenizer.pipe</code></a> method, which will process the texts
as a stream.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-diff language-diff"></code></pre></aside><h3 id="phrasematcher-attrs" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#phrasematcher-attrs" class="heading-text typography_permalink__UiIRy">Matching on other token attributes <!-- --> </a></h3><p>By default, the <code class="code_inline-code__Bq7ot">PhraseMatcher</code> will match on the verbatim token text, e.g.
<code class="code_inline-code__Bq7ot">Token.text</code>. By setting the <code class="code_inline-code__Bq7ot">attr</code> argument on initialization, you can change
<strong>which token attribute the matcher should use</strong> when comparing the phrase
pattern to the matched <code class="code_inline-code__Bq7ot">Doc</code>. For example, using the attribute <code class="code_inline-code__Bq7ot">LOWER</code> lets you
match on <code class="code_inline-code__Bq7ot">Token.lower</code> and create case-insensitive match patterns:</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note on creating patterns</span></h4><p>The examples here use <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#make_doc"><code class="code_inline-code__Bq7ot">nlp.make_doc</code></a> to create <code class="code_inline-code__Bq7ot">Doc</code>
object patterns as efficiently as possible and without running any of the other
pipeline components. If the token attribute you want to match on is set by a
pipeline component, <strong>make sure that the pipeline component runs</strong> when you
create the pattern. For example, to match on <code class="code_inline-code__Bq7ot">POS</code> or <code class="code_inline-code__Bq7ot">LEMMA</code>, the pattern <code class="code_inline-code__Bq7ot">Doc</code>
objects need to have part-of-speech tags set by the <code class="code_inline-code__Bq7ot">tagger</code> or <code class="code_inline-code__Bq7ot">morphologizer</code>.
You can either call the <code class="code_inline-code__Bq7ot">nlp</code> object on your pattern texts instead of
<code class="code_inline-code__Bq7ot">nlp.make_doc</code>, or use <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#select_pipes"><code class="code_inline-code__Bq7ot">nlp.select_pipes</code></a> to
disable components selectively.</p></aside><p>Another possible use case is matching number tokens like IP addresses based on
their shape. This means that you won’t have to worry about how those strings
will be tokenized and you’ll be able to find tokens and combinations of tokens
based on a few examples. Here, we’re matching on the shapes <code class="code_inline-code__Bq7ot">ddd.d.d.d</code> and
<code class="code_inline-code__Bq7ot">ddd.ddd.d.d</code>:</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>In theory, the same also works for attributes like <code class="code_inline-code__Bq7ot">POS</code>. For example, a pattern
<code class="code_inline-code__Bq7ot">nlp(&quot;I like cats&quot;)</code> matched based on its part-of-speech tag would return a
match for “I love dogs”. You could also match on boolean flags like <code class="code_inline-code__Bq7ot">IS_PUNCT</code>
to match phrases with the same sequence of punctuation and non-punctuation
tokens as the pattern. But this can easily get confusing and doesn’t have much
of an advantage over writing one or two token patterns.</p></section>
<section id="section-dependencymatcher" class="section_root__k1hUl"><h2 id="dependencymatcher" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#dependencymatcher" class="heading-text typography_permalink__UiIRy">Dependency Matcher <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="This feature is new and was introduced in spaCy v3.0">v<!-- -->3.0</span><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="To use this functionality, spaCy needs a trained pipeline that supports the following capabilities: parser">Needs model</span></h2><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/dependencymatcher"><code class="code_inline-code__Bq7ot">DependencyMatcher</code></a> lets you match patterns within
the dependency parse using
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.html">Semgrex</a>
operators. It requires a model containing a parser such as the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/dependencyparser"><code class="code_inline-code__Bq7ot">DependencyParser</code></a>. Instead of defining a list of
adjacent tokens as in <code class="code_inline-code__Bq7ot">Matcher</code> patterns, the <code class="code_inline-code__Bq7ot">DependencyMatcher</code> patterns match
tokens in the dependency parse and specify the relations between them.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Example</h4><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><p>A pattern added to the dependency matcher consists of a <strong>list of
dictionaries</strong>, with each dictionary describing a <strong>token to match</strong> and its
<strong>relation to an existing token</strong> in the pattern. Except for the first
dictionary, which defines an anchor token using only <code class="code_inline-code__Bq7ot">RIGHT_ID</code> and
<code class="code_inline-code__Bq7ot">RIGHT_ATTRS</code>, each pattern should have the following keys:</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">LEFT_ID</code></td><td class="table_td__rmpJx">The name of the left-hand node in the relation, which has been defined in an earlier node. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">REL_OP</code></td><td class="table_td__rmpJx">An operator that describes how the two nodes are related. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">RIGHT_ID</code></td><td class="table_td__rmpJx">A unique name for the right-hand node in the relation. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">RIGHT_ATTRS</code></td><td class="table_td__rmpJx">The token attributes to match for the right-hand node in the same format as patterns provided to the regular token-based <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/matcher"><code class="code_inline-code__Bq7ot">Matcher</code></a>. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Dict<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">,</span> Any<span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr></tbody></table><p>Each additional token added to the pattern is linked to an existing token
<code class="code_inline-code__Bq7ot">LEFT_ID</code> by the relation <code class="code_inline-code__Bq7ot">REL_OP</code>. The new token is given the name <code class="code_inline-code__Bq7ot">RIGHT_ID</code>
and described by the attributes <code class="code_inline-code__Bq7ot">RIGHT_ATTRS</code>.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>Because the unique token <strong>names</strong> in <code class="code_inline-code__Bq7ot">LEFT_ID</code> and <code class="code_inline-code__Bq7ot">RIGHT_ID</code> are used to
identify tokens, the order of the dicts in the patterns is important: a token
name needs to be defined as <code class="code_inline-code__Bq7ot">RIGHT_ID</code> in one dict in the pattern <strong>before</strong> it
can be used as <code class="code_inline-code__Bq7ot">LEFT_ID</code> in another dict.</p></aside><h3 id="dependencymatcher-operators" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#dependencymatcher-operators" class="heading-text typography_permalink__UiIRy">Dependency matcher operators <!-- --> </a></h3><p>The following operators are supported by the <code class="code_inline-code__Bq7ot">DependencyMatcher</code>, most of which
come directly from
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.html">Semgrex</a>:</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Symbol</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A &lt; B</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A</code> is the immediate dependent of <code class="code_inline-code__Bq7ot">B</code>.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A &gt; B</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A</code> is the immediate head of <code class="code_inline-code__Bq7ot">B</code>.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A &lt;&lt; B</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A</code> is the dependent in a chain to <code class="code_inline-code__Bq7ot">B</code> following dep → head paths.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A &gt;&gt; B</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A</code> is the head in a chain to <code class="code_inline-code__Bq7ot">B</code> following head → dep paths.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A . B</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A</code> immediately precedes <code class="code_inline-code__Bq7ot">B</code>, i.e. <code class="code_inline-code__Bq7ot">A.i == B.i - 1</code>, and both are within the same dependency tree.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A .* B</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A</code> precedes <code class="code_inline-code__Bq7ot">B</code>, i.e. <code class="code_inline-code__Bq7ot">A.i &lt; B.i</code>, and both are within the same dependency tree <em>(not in Semgrex)</em>.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A ; B</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A</code> immediately follows <code class="code_inline-code__Bq7ot">B</code>, i.e. <code class="code_inline-code__Bq7ot">A.i == B.i + 1</code>, and both are within the same dependency tree <em>(not in Semgrex)</em>.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A ;* B</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A</code> follows <code class="code_inline-code__Bq7ot">B</code>, i.e. <code class="code_inline-code__Bq7ot">A.i &gt; B.i</code>, and both are within the same dependency tree <em>(not in Semgrex)</em>.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A $+ B</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">B</code> is a right immediate sibling of <code class="code_inline-code__Bq7ot">A</code>, i.e. <code class="code_inline-code__Bq7ot">A</code> and <code class="code_inline-code__Bq7ot">B</code> have the same parent and <code class="code_inline-code__Bq7ot">A.i == B.i - 1</code>.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A $- B</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">B</code> is a left immediate sibling of <code class="code_inline-code__Bq7ot">A</code>, i.e. <code class="code_inline-code__Bq7ot">A</code> and <code class="code_inline-code__Bq7ot">B</code> have the same parent and <code class="code_inline-code__Bq7ot">A.i == B.i + 1</code>.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A $++ B</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">B</code> is a right sibling of <code class="code_inline-code__Bq7ot">A</code>, i.e. <code class="code_inline-code__Bq7ot">A</code> and <code class="code_inline-code__Bq7ot">B</code> have the same parent and <code class="code_inline-code__Bq7ot">A.i &lt; B.i</code>.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">A $-- B</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">B</code> is a left sibling of <code class="code_inline-code__Bq7ot">A</code>, i.e. <code class="code_inline-code__Bq7ot">A</code> and <code class="code_inline-code__Bq7ot">B</code> have the same parent and <code class="code_inline-code__Bq7ot">A.i &gt; B.i</code>.</td></tr></tbody></table><h3 id="dependencymatcher-patterns" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#dependencymatcher-patterns" class="heading-text typography_permalink__UiIRy">Designing dependency matcher patterns <!-- --> </a></h3><p>Let’s say we want to find sentences describing who founded what kind of company:</p><ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><em>Smith founded a healthcare company in 2005.</em></li>
<li class="list_li__sfx_z"><em>Williams initially founded an insurance company in 1987.</em></li>
<li class="list_li__sfx_z"><em>Lee, an experienced CEO, has founded two AI startups.</em></li>
</ul><p>The dependency parse for “Smith founded a healthcare company” shows types of
relations and tokens we want to match:</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Visualizing the parse<!-- --> </span></h4>
<p>The <a class="link_root__1Me7D" href="/usage/visualizers"><code class="code_inline-code__Bq7ot">displacy</code> visualizer</a> lets you render <code class="code_inline-code__Bq7ot">Doc</code> objects
and their dependency parse and part-of-speech tags:</p>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><iframe class="embed_standalone__RHbIL" title="displaCy visualization of dependencies" src="/images/displacy-dep-founded.html" width="800" height="450" allowfullscreen="" frameBorder="0"></iframe><p>The relations we’re interested in are:</p><ul class="list_ul__fe_HF">
<li class="list_li__sfx_z">the founder is the <strong>subject</strong> (<code class="code_inline-code__Bq7ot">nsubj</code>) of the token with the text <code class="code_inline-code__Bq7ot">founded</code></li>
<li class="list_li__sfx_z">the company is the <strong>object</strong> (<code class="code_inline-code__Bq7ot">dobj</code>) of <code class="code_inline-code__Bq7ot">founded</code></li>
<li class="list_li__sfx_z">the kind of company may be an <strong>adjective</strong> (<code class="code_inline-code__Bq7ot">amod</code>, not shown above) or a
<strong>compound</strong> (<code class="code_inline-code__Bq7ot">compound</code>)</li>
</ul><p>The first step is to pick an <strong>anchor token</strong> for the pattern. Since it’s the
root of the dependency parse, <code class="code_inline-code__Bq7ot">founded</code> is a good choice here. It is often
easier to construct patterns when all dependency relation operators point from
the head to the children. In this example, we’ll only use <code class="code_inline-code__Bq7ot">&gt;</code>, which connects a
head to an immediate dependent as <code class="code_inline-code__Bq7ot">head &gt; child</code>.</p><p>The simplest dependency matcher pattern will identify and name a single token in
the tree:</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>Now that we have a named anchor token (<code class="code_inline-code__Bq7ot">anchor_founded</code>), we can add the founder
as the immediate dependent (<code class="code_inline-code__Bq7ot">&gt;</code>) of <code class="code_inline-code__Bq7ot">founded</code> with the dependency label <code class="code_inline-code__Bq7ot">nsubj</code>:</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Step 1</h4><code class="code_code__CILJL language-python language-python code_wrap__b41os"></code></pre><p>The direct object (<code class="code_inline-code__Bq7ot">dobj</code>) is added in the same way:</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Step 2</h4><code class="code_code__CILJL language-python language-python"></code></pre><p>When the subject and object tokens are added, they are required to have names
under the key <code class="code_inline-code__Bq7ot">RIGHT_ID</code>, which are allowed to be any unique string, e.g.
<code class="code_inline-code__Bq7ot">founded_subject</code>. These names can then be used as <code class="code_inline-code__Bq7ot">LEFT_ID</code> to <strong>link new
tokens into the pattern</strong>. For the final part of our pattern, we’ll specify that
the token <code class="code_inline-code__Bq7ot">founded_object</code> should have a modifier with the dependency relation
<code class="code_inline-code__Bq7ot">amod</code> or <code class="code_inline-code__Bq7ot">compound</code>:</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Step 3</h4><code class="code_code__CILJL language-python language-python code_wrap__b41os"></code></pre><p>You can picture the process of creating a dependency matcher pattern as defining
an anchor token on the left and building up the pattern by linking tokens
one-by-one on the right using relation operators. To create a valid pattern,
each new token needs to be linked to an existing token on its left. As for
<code class="code_inline-code__Bq7ot">founded</code> in this example, a token may be linked to more than one token on its
right:</p><figure class="gatsby-resp-image-figure"><img class="embed_image__mSQUH" src="/images/dep-match-diagram.svg" alt="Dependency matcher pattern" width="650" height="auto"/></figure><p>The full pattern comes together as shown in the example below:</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note on speed</span></h4><p>The dependency matcher may be slow when token patterns can potentially match
many tokens in the sentence or when relation operators allow longer paths in the
dependency parse, e.g. <code class="code_inline-code__Bq7ot">&lt;&lt;</code>, <code class="code_inline-code__Bq7ot">&gt;&gt;</code>, <code class="code_inline-code__Bq7ot">.*</code> and <code class="code_inline-code__Bq7ot">;*</code>.</p><p>To improve the matcher speed, try to make your token patterns and operators as
specific as possible. For example, use <code class="code_inline-code__Bq7ot">&gt;</code> instead of <code class="code_inline-code__Bq7ot">&gt;&gt;</code> if possible and use
token patterns that include dependency labels and other token attributes instead
of patterns such as <code class="code_inline-code__Bq7ot">{}</code> that match any token in the sentence.</p></aside></section>
<section id="section-entityruler" class="section_root__k1hUl"><h2 id="entityruler" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#entityruler" class="heading-text typography_permalink__UiIRy">Rule-based entity recognition <!-- --> </a></h2><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/entityruler"><code class="code_inline-code__Bq7ot">EntityRuler</code></a> is a component that lets you add named
entities based on pattern dictionaries, which makes it easy to combine
rule-based and statistical named entity recognition for even more powerful
pipelines.</p><h3 id="entityruler-patterns" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#entityruler-patterns" class="heading-text typography_permalink__UiIRy">Entity Patterns <!-- --> </a></h3><p>Entity patterns are dictionaries with two keys: <code class="code_inline-code__Bq7ot">&quot;label&quot;</code>, specifying the label
to assign to the entity if the pattern is matched, and <code class="code_inline-code__Bq7ot">&quot;pattern&quot;</code>, the match
pattern. The entity ruler accepts two types of patterns:</p><ol class="list_ol__aclSa">
<li class="list_li__sfx_z">
<p><strong>Phrase patterns</strong> for exact string matches (string).</p>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</li>
<li class="list_li__sfx_z">
<p><strong>Token patterns</strong> with one dictionary describing one token (list).</p>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</li>
</ol><h3 id="entityruler-usage" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#entityruler-usage" class="heading-text typography_permalink__UiIRy">Using the entity ruler <!-- --> </a></h3><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/entityruler"><code class="code_inline-code__Bq7ot">EntityRuler</code></a> is a pipeline component that’s typically
added via <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#add_pipe"><code class="code_inline-code__Bq7ot">nlp.add_pipe</code></a>. When the <code class="code_inline-code__Bq7ot">nlp</code> object is
called on a text, it will find matches in the <code class="code_inline-code__Bq7ot">doc</code> and add them as entities to
the <code class="code_inline-code__Bq7ot">doc.ents</code>, using the specified pattern label as the entity label. If any
matches were to overlap, the pattern matching most tokens takes priority. If
they also happen to be equally long, then the match occurring first in the <code class="code_inline-code__Bq7ot">Doc</code>
is chosen.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>The entity ruler is designed to integrate with spaCy’s existing pipeline
components and enhance the named entity recognizer. If it’s added <strong>before the
<code class="code_inline-code__Bq7ot">&quot;ner&quot;</code> component</strong>, the entity recognizer will respect the existing entity
spans and adjust its predictions around it. This can significantly improve
accuracy in some cases. If it’s added <strong>after the <code class="code_inline-code__Bq7ot">&quot;ner&quot;</code> component</strong>, the
entity ruler will only add spans to the <code class="code_inline-code__Bq7ot">doc.ents</code> if they don’t overlap with
existing entities predicted by the model. To overwrite overlapping entities, you
can set <code class="code_inline-code__Bq7ot">overwrite_ents=True</code> on initialization.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h4 id="entityruler-pattern-validation" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#entityruler-pattern-validation" class="heading-text typography_permalink__UiIRy">Validating and debugging EntityRuler patterns <!-- --> </a></h4><p>The entity ruler can validate patterns against a JSON schema with the config
setting <code class="code_inline-code__Bq7ot">&quot;validate&quot;</code>. See details under
<a class="link_root__1Me7D" href="/usage/rule-based-matching#pattern-validation">Validating and debugging patterns</a>.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><h3 id="entityruler-ent-ids" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#entityruler-ent-ids" class="heading-text typography_permalink__UiIRy">Adding IDs to patterns <!-- --> </a></h3><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/entityruler"><code class="code_inline-code__Bq7ot">EntityRuler</code></a> can also accept an <code class="code_inline-code__Bq7ot">id</code> attribute for each
pattern. Using the <code class="code_inline-code__Bq7ot">id</code> attribute allows multiple patterns to be associated with
the same entity.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>If the <code class="code_inline-code__Bq7ot">id</code> attribute is included in the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/entityruler"><code class="code_inline-code__Bq7ot">EntityRuler</code></a>
patterns, the <code class="code_inline-code__Bq7ot">ent_id_</code> property of the matched entity is set to the <code class="code_inline-code__Bq7ot">id</code> given
in the patterns. So in the example above it’s easy to identify that “San
Francisco” and “San Fran” are both the same entity.</p><h3 id="entityruler-files" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#entityruler-files" class="heading-text typography_permalink__UiIRy">Using pattern files <!-- --> </a></h3><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/entityruler#to_disk"><code class="code_inline-code__Bq7ot">to_disk</code></a> and
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/entityruler#from_disk"><code class="code_inline-code__Bq7ot">from_disk</code></a> let you save and load patterns to and
from JSONL (newline-delimited JSON) files, containing one pattern object per
line.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">patterns.jsonl</h4><code class="code_code__CILJL language-json language-json"></code></pre><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span>Integration with Prodigy</span></h4><p>If you’re using the <a class="link_root__1Me7D" href="https://prodi.gy">Prodigy</a> annotation tool, you might
recognize these pattern files from bootstrapping your named entity and text
classification labelling. The patterns for the <code class="code_inline-code__Bq7ot">EntityRuler</code> follow the same
syntax, so you can use your existing Prodigy pattern files in spaCy, and vice
versa.</p></aside><p>When you save out an <code class="code_inline-code__Bq7ot">nlp</code> object that has an <code class="code_inline-code__Bq7ot">EntityRuler</code> added to its
pipeline, its patterns are automatically exported to the pipeline directory:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>The saved pipeline now includes the <code class="code_inline-code__Bq7ot">&quot;entity_ruler&quot;</code> in its
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/data-formats#config"><code class="code_inline-code__Bq7ot">config.cfg</code></a> and the pipeline directory contains a
file <code class="code_inline-code__Bq7ot">patterns.jsonl</code> with the patterns. When you load the pipeline back in, all
pipeline components will be restored and deserialized – including the entity
ruler. This lets you ship powerful pipeline packages with binary weights <em>and</em>
rules included!</p><h3 id="entityruler-large-phrase-patterns" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#entityruler-large-phrase-patterns" class="heading-text typography_permalink__UiIRy">Using a large number of phrase patterns <!-- --> </a></h3><p>When using a large amount of <strong>phrase patterns</strong> (roughly &gt; 10000) it’s useful
to understand how the <code class="code_inline-code__Bq7ot">add_patterns</code> function of the entity ruler works. For
each <strong>phrase pattern</strong>, the EntityRuler calls the nlp object to construct a doc
object. This happens in case you try to add the EntityRuler at the end of an
existing pipeline with, for example, a POS tagger and want to extract matches
based on the pattern’s POS signature. In this case you would pass a config value
of <code class="code_inline-code__Bq7ot">&quot;phrase_matcher_attr&quot;: &quot;POS&quot;</code> for the entity ruler.</p><p>Running the full language pipeline across every pattern in a large list scales
linearly and can therefore take a long time on large amounts of phrase patterns.
As of spaCy v2.2.4 the <code class="code_inline-code__Bq7ot">add_patterns</code> function has been refactored to use
<code class="code_inline-code__Bq7ot">nlp.pipe</code> on all phrase patterns resulting in about a 10x-20x speed up with
5,000-100,000 phrase patterns respectively. Even with this speedup (but
especially if you’re using an older version) the <code class="code_inline-code__Bq7ot">add_patterns</code> function can
still take a long time. An easy workaround to make this function run faster is
disabling the other language pipes while adding the phrase patterns.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre></section>
<section id="section-spanruler" class="section_root__k1hUl"><h2 id="spanruler" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#spanruler" class="heading-text typography_permalink__UiIRy">Rule-based span matching <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="This feature is new and was introduced in spaCy v3.3.1">v<!-- -->3.3.1</span></h2><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/spanruler"><code class="code_inline-code__Bq7ot">SpanRuler</code></a> is a generalized version of the entity ruler
that lets you add spans to <code class="code_inline-code__Bq7ot">doc.spans</code> or <code class="code_inline-code__Bq7ot">doc.ents</code> based on pattern
dictionaries, which makes it easy to combine rule-based and statistical pipeline
components.</p><h3 id="spanruler-patterns" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#spanruler-patterns" class="heading-text typography_permalink__UiIRy">Span patterns <!-- --> </a></h3><p>The <a class="link_root__1Me7D" href="/usage/rule-based-matching#entityruler-patterns">pattern format</a> is the same as for the entity ruler:</p><ol class="list_ol__aclSa">
<li class="list_li__sfx_z">
<p><strong>Phrase patterns</strong> for exact string matches (string).</p>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</li>
<li class="list_li__sfx_z">
<p><strong>Token patterns</strong> with one dictionary describing one token (list).</p>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</li>
</ol><h3 id="spanruler-usage" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#spanruler-usage" class="heading-text typography_permalink__UiIRy">Using the span ruler <!-- --> </a></h3><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/spanruler"><code class="code_inline-code__Bq7ot">SpanRuler</code></a> is a pipeline component that’s typically added
via <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#add_pipe"><code class="code_inline-code__Bq7ot">nlp.add_pipe</code></a>. When the <code class="code_inline-code__Bq7ot">nlp</code> object is called on
a text, it will find matches in the <code class="code_inline-code__Bq7ot">doc</code> and add them as spans to
<code class="code_inline-code__Bq7ot">doc.spans[&quot;ruler&quot;]</code>, using the specified pattern label as the entity label.
Unlike in <code class="code_inline-code__Bq7ot">doc.ents</code>, overlapping matches are allowed in <code class="code_inline-code__Bq7ot">doc.spans</code>, so no
filtering is required, but optional filtering and sorting can be applied to the
spans before they’re saved.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>The span ruler is designed to integrate with spaCy’s existing pipeline
components and enhance the <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/spancat"><span class="link_source-text__VDP74">SpanCategorizer</span></a> and
<a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/entityrecognizer"><span class="link_source-text__VDP74">EntityRecognizer</span></a>. The <code class="code_inline-code__Bq7ot">overwrite</code> setting determines
whether the existing annotation in <code class="code_inline-code__Bq7ot">doc.spans</code> or <code class="code_inline-code__Bq7ot">doc.ents</code> is preserved.
Because overlapping entities are not allowed for <code class="code_inline-code__Bq7ot">doc.ents</code>, the entities are
always filtered, using <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#util.filter_spans"><code class="code_inline-code__Bq7ot">util.filter_spans</code></a>
by default. See the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/spanruler"><code class="code_inline-code__Bq7ot">SpanRuler</code> API docs</a> for more information
about how to customize the sorting and filtering of matched spans.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h3 id="spanruler-files" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#spanruler-files" class="heading-text typography_permalink__UiIRy">Using pattern files <!-- --> </a></h3><p>You can save patterns in a JSONL file (newline-delimited JSON) to load with
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/spanruler#initialize"><code class="code_inline-code__Bq7ot">SpanRuler.initialize</code></a> or
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/spanruler#add_patterns"><code class="code_inline-code__Bq7ot">SpanRuler.add_patterns</code></a>.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">patterns.jsonl</h4><code class="code_code__CILJL language-json language-json"></code></pre><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>Unlike the entity ruler, the span ruler cannot load patterns on initialization
with <code class="code_inline-code__Bq7ot">SpanRuler(patterns=patterns)</code> or directly from a JSONL file path with
<code class="code_inline-code__Bq7ot code_wrap__b41os">SpanRuler.from_disk(jsonl_path)</code>. Patterns should be loaded from the JSONL file
separately and then added through
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/spanruler#initialize%5D"><code class="code_inline-code__Bq7ot">SpanRuler.initialize</code></a> or
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/spanruler#add_patterns"><code class="code_inline-code__Bq7ot">SpanRuler.add_patterns</code></a> as shown above.</p></aside></section>
<section id="section-models-rules" class="section_root__k1hUl"><h2 id="models-rules" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#models-rules" class="heading-text typography_permalink__UiIRy">Combining models and rules <!-- --> </a></h2><p>You can combine statistical and rule-based components in a variety of ways.
Rule-based components can be used to improve the accuracy of statistical models,
by presetting tags, entities or sentence boundaries for specific tokens. The
statistical models will usually respect these preset annotations, which
sometimes improves the accuracy of other decisions. You can also use rule-based
components after a statistical model to correct common errors. Finally,
rule-based components can reference the attributes set by statistical models, in
order to implement more abstract logic.</p><h3 id="models-rules-ner" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#models-rules-ner" class="heading-text typography_permalink__UiIRy">Example: Expanding named entities <!-- --> </a></h3><p>When using a trained
<a class="link_root__1Me7D" href="/usage/linguistic-features#named-entities">named entity recognition</a> model to
extract information from your texts, you may find that the predicted span only
includes parts of the entity you’re looking for. Sometimes, this happens if
statistical model predicts entities incorrectly. Other times, it happens if the
way the entity type was defined in the original training corpus doesn’t match
what you need for your application.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Where corpora come from<!-- --> </span></h4>
<p>Corpora used to train pipelines from scratch are often produced in academia.
They contain text from various sources with linguistic features labeled
manually by human annotators (following a set of specific guidelines). The
corpora are then distributed with evaluation data, so other researchers can
benchmark their algorithms and everyone can report numbers on the same data.
However, most applications need to learn information that isn’t contained in
any available corpus.</p>
</div></div></aside><p>For example, the corpus spaCy’s <a class="link_root__1Me7D link_with-icon__NAVDA" href="/models/en"><span class="link_source-text__VDP74">English pipelines</span></a> were trained on
defines a <code class="code_inline-code__Bq7ot">PERSON</code> entity as just the <strong>person name</strong>, without titles like “Mr.”
or “Dr.”. This makes sense, because it makes it easier to resolve the entity
type back to a knowledge base. But what if your application needs the full
names, <em>including</em> the titles?</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>While you could try and teach the model a new definition of the <code class="code_inline-code__Bq7ot">PERSON</code> entity
by <a class="link_root__1Me7D" href="/usage/training#example-train-ner">updating it</a> with more examples of spans
that include the title, this might not be the most efficient approach. The
existing model was trained on over 2 million words, so in order to completely
change the definition of an entity type, you might need a lot of training
examples. However, if you already have the predicted <code class="code_inline-code__Bq7ot">PERSON</code> entities, you can
use a rule-based approach that checks whether they come with a title and if so,
expands the entity span by one token. After all, what all titles in this example
have in common is that <em>if</em> they occur, they occur in the <strong>previous token</strong>
right before the person entity.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python code_wrap__b41os"></code></pre><p>The above function takes a <code class="code_inline-code__Bq7ot">Doc</code> object, modifies its <code class="code_inline-code__Bq7ot">doc.ents</code> and returns it.
Using the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#component"><code class="code_inline-code__Bq7ot">@Language.component</code></a> decorator, we can
register it as a <a class="link_root__1Me7D" href="/usage/processing-pipelines">pipeline component</a> so it can run
automatically when processing a text. We can use
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#add_pipe"><code class="code_inline-code__Bq7ot">nlp.add_pipe</code></a> to add it to the current pipeline.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>An alternative approach would be to use an
<a class="link_root__1Me7D" href="/usage/processing-pipelines#custom-components-attributes">extension attribute</a>
like <code class="code_inline-code__Bq7ot">._.person_title</code> and add it to <code class="code_inline-code__Bq7ot">Span</code> objects (which includes entity spans
in <code class="code_inline-code__Bq7ot">doc.ents</code>). The advantage here is that the entity text stays intact and can
still be used to look up the name in a knowledge base. The following function
takes a <code class="code_inline-code__Bq7ot">Span</code> object, checks the previous token if it’s a <code class="code_inline-code__Bq7ot">PERSON</code> entity and
returns the title if one is found. The <code class="code_inline-code__Bq7ot">Span.doc</code> attribute gives us easy access
to the span’s parent document.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>We can now use the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/span#set_extension"><code class="code_inline-code__Bq7ot">Span.set_extension</code></a> method to add
the custom extension attribute <code class="code_inline-code__Bq7ot">&quot;person_title&quot;</code>, using <code class="code_inline-code__Bq7ot">get_person_title</code> as the
getter function.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h3 id="models-rules-pos-dep" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#models-rules-pos-dep" class="heading-text typography_permalink__UiIRy">Example: Using entities, part-of-speech tags and the dependency parse <!-- --> </a></h3><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Linguistic features<!-- --> </span></h4>
<p>This example makes extensive use of part-of-speech tag and dependency
attributes and related <code class="code_inline-code__Bq7ot">Doc</code>, <code class="code_inline-code__Bq7ot">Token</code> and <code class="code_inline-code__Bq7ot">Span</code> methods. For an introduction
on this, see the guide on <a class="link_root__1Me7D" href="/usage/linguistic-features">linguistic features</a>.
Also see the label schemes in the <a class="link_root__1Me7D" href="/models">models directory</a> for details on
the labels.</p>
</div></div></aside><p>Let’s say you want to parse professional biographies and extract the person
names and company names, and whether it’s a company they’re <em>currently</em> working
at, or a <em>previous</em> company. One approach could be to try and train a named
entity recognizer to predict <code class="code_inline-code__Bq7ot">CURRENT_ORG</code> and <code class="code_inline-code__Bq7ot">PREVIOUS_ORG</code> – but this
distinction is very subtle and something the entity recognizer may struggle to
learn. Nothing about “Acme Corp Inc.” is inherently “current” or “previous”.</p><p>However, the syntax of the sentence holds some very important clues: we can
check for trigger words like “work”, whether they’re <strong>past tense</strong> or <strong>present
tense</strong>, whether company names are attached to it and whether the person is the
subject. All of this information is available in the part-of-speech tags and the
dependency parse.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">nsubj</code>: Nominal subject.</li>
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">prep</code>: Preposition.</li>
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">pobj</code>: Object of preposition.</li>
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">NNP</code>: Proper noun, singular.</li>
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">VBD</code>: Verb, past tense.</li>
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">IN</code>: Conjunction, subordinating or preposition.</li>
</ul>
</div></div></aside><figure class="gatsby-resp-image-figure"><img class="embed_image__mSQUH" src="/images/displacy-model-rules.svg" alt="Visualization of dependency parse" width="650" height="auto"/><figcaption class="gatsby-resp-image-figcaption"></figcaption></figure><p>In this example, “worked” is the root of the sentence and is a past tense verb.
Its subject is “Alex Smith”, the person who worked. “at Acme Corp Inc.” is a
prepositional phrase attached to the verb “worked”. To extract this
relationship, we can start by looking at the predicted <code class="code_inline-code__Bq7ot">PERSON</code> entities, find
their heads and check whether they’re attached to a trigger word like “work”.
Next, we can check for prepositional phrases attached to the head and whether
they contain an <code class="code_inline-code__Bq7ot">ORG</code> entity. Finally, to determine whether the company
affiliation is current, we can check the head’s part-of-speech tag.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>To apply this logic automatically when we process a text, we can add it to the
<code class="code_inline-code__Bq7ot">nlp</code> object as a
<a class="link_root__1Me7D" href="/usage/processing-pipelines#custom-components">custom pipeline component</a>. The
above logic also expects that entities are merged into single tokens. spaCy
ships with a handy built-in <code class="code_inline-code__Bq7ot">merge_entities</code> that takes care of that. Instead of
just printing the result, you could also write it to
<a class="link_root__1Me7D" href="/usage/processing-pipelines#custom-components-attributes">custom attributes</a> on
the entity <code class="code_inline-code__Bq7ot">Span</code> – for example <code class="code_inline-code__Bq7ot">._.orgs</code> or <code class="code_inline-code__Bq7ot">._.prev_orgs</code> and
<code class="code_inline-code__Bq7ot">._.current_orgs</code>.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Merging entities<!-- --> </span></h4>
<p>Under the hood, entities are merged using the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc#retokenize"><code class="code_inline-code__Bq7ot">Doc.retokenize</code></a> context manager:</p>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>If you change the sentence structure above, for example to “was working”, you’ll
notice that our current logic fails and doesn’t correctly detect the company as
a past organization. That’s because the root is a participle and the tense
information is in the attached auxiliary “was”:</p><figure class="gatsby-resp-image-figure"><img class="embed_image__mSQUH" src="/images/displacy-model-rules2.svg" alt="Visualization of dependency parse" width="650" height="auto"/></figure><p>To solve this, we can adjust the rules to also check for the above construction:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python code_wrap__b41os"></code></pre><p>In your final rule-based system, you may end up with <strong>several different code
paths</strong> to cover the types of constructions that occur in your data.</p></section><div class="grid_root__EfDZl grid_spacing__fhBCv grid_half__xoJZs"><div style="margin-top:var(--spacing-lg)"><a class="link_root__1Me7D button_root__jwipc button_secondary__ukZAk" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/website/docs/usage/rule-based-matching.mdx">Suggest edits</a></div><a class="link_root__1Me7D readnext_root__JNzwZ link_no-link-layout__RPvod" href="/usage/processing-pipelines"><span><span class="typography_label__l_oVJ">Read next</span>Processing Pipelines</span><span class="readnext_icon__jfRnJ"></span></a></div></article><div class="main_asides__RITE5" style="background-image:url(/_next/static/media/pattern_blue.d167bed5.png"></div><footer class="footer_root__zlkjP"><div class="grid_root__EfDZl footer_content__LaE1F grid_narrow__x_6xS grid_spacing__fhBCv grid_third__edHuB"><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">spaCy</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/usage">Usage</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/models">Models</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/api">API Reference</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://course.spacy.io">Online Course</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/custom-solutions">Custom Solutions</a></li></ul></section><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Community</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/universe">Universe</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/discussions">GitHub Discussions</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/issues">Issue Tracker</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="http://stackoverflow.com/questions/tagged/spacy">Stack Overflow</a></li></ul></section><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Connect</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://twitter.com/spacy_io">Twitter</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy">GitHub</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://youtube.com/c/ExplosionAI">YouTube</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/blog">Blog</a></li></ul></section><section class="footer_full___icln"><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Stay in the loop!</li><li>Receive updates about new releases, tutorials and more.</li><li><form id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" action="//spacy.us12.list-manage.com/subscribe/post?u=83b0498b1e7fa3c91ce68c3f1&amp;amp;id=ecc82e0493" method="post" target="_blank" novalidate=""><div style="position:absolute;left:-5000px" aria-hidden="true"><input type="text" name="b_83b0498b1e7fa3c91ce68c3f1_ecc82e0493" tabindex="-1" value=""/></div><div class="newsletter_root__uh6MU"><input class="newsletter_input___SMSB" id="mce-EMAIL" type="email" name="EMAIL" placeholder="Your email" aria-label="Your email"/><button class="newsletter_button__gKW8E" id="mc-embedded-subscribe" type="submit" name="subscribe">Sign up</button></div></form></li></ul></section></div><div class="footer_content__LaE1F footer_copy__rbjvc"><span>© 2016-<!-- -->2023<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai">Explosion</a></span><a class="link_root__1Me7D footer_logo__BthsJ link_no-link-layout__RPvod" aria-label="Explosion" href="https://explosion.ai"></a><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/legal">Legal / Imprint</a></div></footer></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"title":"Rule-based matching","teaser":"Find phrases and tokens, and match entities","menu":[["Token Matcher","matcher"],["Phrase Matcher","phrasematcher"],["Dependency Matcher","dependencymatcher"],["Entity Ruler","entityruler"],["Span Ruler","spanruler"],["Models \u0026 Rules","models-rules"]],"slug":"/usage/rule-based-matching","mdx":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\n/*TODO: double-check that this still works if the ruler is added to the pipeline on creation, and include suggestion if needed*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    section: \"section\",\n    p: \"p\",\n    strong: \"strong\",\n    em: \"em\",\n    a: \"a\",\n    h2: \"h2\",\n    h3: \"h3\",\n    ol: \"ol\",\n    li: \"li\",\n    pre: \"pre\",\n    code: \"code\",\n    h4: \"h4\",\n    table: \"table\",\n    thead: \"thead\",\n    tr: \"tr\",\n    th: \"th\",\n    tbody: \"tbody\",\n    td: \"td\",\n    del: \"del\",\n    hr: \"hr\",\n    blockquote: \"blockquote\",\n    h5: \"h5\",\n    ul: \"ul\",\n    img: \"img\"\n  }, _provideComponents(), props.components), {InlineCode, Accordion, Infobox, Image, Iframe} = _components;\n  if (!Accordion) _missingMdxReference(\"Accordion\", true);\n  if (!Iframe) _missingMdxReference(\"Iframe\", true);\n  if (!Image) _missingMdxReference(\"Image\", true);\n  if (!Infobox) _missingMdxReference(\"Infobox\", true);\n  if (!InlineCode) _missingMdxReference(\"InlineCode\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.section, {\n      children: [_jsxs(_components.p, {\n        children: [\"Compared to using regular expressions on raw text, spaCy’s rule-based matcher\\nengines and components not only let you find the words and phrases you’re\\nlooking for – they also give you access to the tokens within the document and\\ntheir relationships. This means you can easily access and analyze the\\nsurrounding tokens, merge spans into single tokens or add entries to the named\\nentities in \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \".\"]\n      }), _jsxs(Accordion, {\n        title: \"Should I use rules or train a model?\",\n        id: \"rules-vs-model\",\n        children: [_jsx(_components.p, {\n          children: \"For complex tasks, it’s usually better to train a statistical entity recognition\\nmodel. However, statistical models require training data, so for many\\nsituations, rule-based approaches are more practical. This is especially true at\\nthe start of a project: you can use a rule-based approach as part of a data\\ncollection process, to help you “bootstrap” a statistical model.\"\n        }), _jsxs(_components.p, {\n          children: [\"Training a model is useful if you have some examples and you want your system to\\nbe able to \", _jsx(_components.strong, {\n            children: \"generalize\"\n          }), \" based on those examples. It works especially well if\\nthere are clues in the \", _jsx(_components.em, {\n            children: \"local context\"\n          }), \". For instance, if you’re trying to detect\\nperson or company names, your application may benefit from a statistical named\\nentity recognition model.\"]\n        }), _jsxs(_components.p, {\n          children: [\"Rule-based systems are a good choice if there’s a more or less \", _jsx(_components.strong, {\n            children: \"finite number\"\n          }), \"\\nof examples that you want to find in the data, or if there’s a very \", _jsx(_components.strong, {\n            children: \"clear,\\nstructured pattern\"\n          }), \" you can express with token rules or regular expressions.\\nFor instance, country names, IP addresses or URLs are things you might be able\\nto handle well with a purely rule-based approach.\"]\n        }), _jsxs(_components.p, {\n          children: [\"You can also combine both approaches and improve a statistical model with rules\\nto handle very specific cases and boost accuracy. For details, see the section\\non \", _jsx(_components.a, {\n            href: \"#entityruler\",\n            children: \"rule-based entity recognition\"\n          }), \".\"]\n        })]\n      }), _jsxs(Accordion, {\n        title: \"When should I use the token matcher vs. the phrase matcher?\",\n        id: \"matcher-vs-phrase-matcher\",\n        children: [_jsxs(_components.p, {\n          children: [\"The \", _jsx(InlineCode, {\n            children: \"PhraseMatcher\"\n          }), \" is useful if you already have a large terminology list or\\ngazetteer consisting of single or multi-token phrases that you want to find\\nexact instances of in your data. As of spaCy v2.1.0, you can also match on the\\n\", _jsx(InlineCode, {\n            children: \"LOWER\"\n          }), \" attribute for fast and case-insensitive matching.\"]\n        }), _jsxs(_components.p, {\n          children: [\"The \", _jsx(InlineCode, {\n            children: \"Matcher\"\n          }), \" isn’t as blazing fast as the \", _jsx(InlineCode, {\n            children: \"PhraseMatcher\"\n          }), \", since it compares\\nacross individual token attributes. However, it allows you to write very\\nabstract representations of the tokens you’re looking for, using lexical\\nattributes, linguistic features predicted by the model, operators, set\\nmembership and rich comparison. For example, you can find a noun, followed by a\\nverb with the lemma “love” or “like”, followed by an optional determiner and\\nanother token that’s at least 10 characters long.\"]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-matcher\",\n      children: [_jsx(_components.h2, {\n        id: \"matcher\",\n        children: \"Token-based matching \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy features a rule-matching engine, the \", _jsx(_components.a, {\n          href: \"/api/matcher\",\n          children: _jsx(InlineCode, {\n            children: \"Matcher\"\n          })\n        }), \", that\\noperates over tokens, similar to regular expressions. The rules can refer to\\ntoken annotations (e.g. the token \", _jsx(InlineCode, {\n          children: \"text\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"tag_\"\n        }), \", and flags like \", _jsx(InlineCode, {\n          children: \"IS_PUNCT\"\n        }), \").\\nThe rule matcher also lets you pass in a custom callback to act on matches – for\\nexample, to merge entities and apply custom labels. You can also associate\\npatterns with entity IDs, to allow some basic entity linking or disambiguation.\\nTo match large terminology lists, you can use the\\n\", _jsx(_components.a, {\n          href: \"/api/phrasematcher\",\n          children: _jsx(InlineCode, {\n            children: \"PhraseMatcher\"\n          })\n        }), \", which accepts \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" objects as match\\npatterns.\"]\n      }), _jsx(_components.h3, {\n        id: \"adding-patterns\",\n        children: \"Adding patterns \"\n      }), _jsx(_components.p, {\n        children: \"Let’s say we want to enable spaCy to find a combination of three tokens:\"\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"A token whose \", _jsx(_components.strong, {\n            children: \"lowercase form matches “hello”\"\n          }), \", e.g. “Hello” or “HELLO”.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"A token whose \", _jsxs(_components.strong, {\n            children: [_jsx(InlineCode, {\n              children: \"is_punct\"\n            }), \" flag is set to \", _jsx(InlineCode, {\n              children: \"True\"\n            })]\n          }), \", i.e. any punctuation.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"A token whose \", _jsx(_components.strong, {\n            children: \"lowercase form matches “world”\"\n          }), \", e.g. “World” or “WORLD”.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"[{\\\"LOWER\\\": \\\"hello\\\"}, {\\\"IS_PUNCT\\\": True}, {\\\"LOWER\\\": \\\"world\\\"}]\\n\"\n        })\n      }), _jsxs(Infobox, {\n        title: \"Important note\",\n        variant: \"danger\",\n        children: [_jsxs(_components.p, {\n          children: [\"When writing patterns, keep in mind that \", _jsx(_components.strong, {\n            children: \"each dictionary\"\n          }), \" represents \", _jsx(_components.strong, {\n            children: \"one\\ntoken\"\n          }), \". If spaCy’s tokenization doesn’t match the tokens defined in a pattern,\\nthe pattern is not going to produce any results. When developing complex\\npatterns, make sure to check examples against spaCy’s tokenization:\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"doc = nlp(\\\"A complex-example,!\\\")\\nprint([token.text for token in doc])\\n\"\n          })\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"First, we initialize the \", _jsx(InlineCode, {\n          children: \"Matcher\"\n        }), \" with a vocab. The matcher must always share\\nthe same vocab with the documents it will operate on. We can now call\\n\", _jsx(_components.a, {\n          href: \"/api/matcher#add\",\n          children: _jsx(InlineCode, {\n            children: \"matcher.add()\"\n          })\n        }), \" with an ID and a list of patterns.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import Matcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = Matcher(nlp.vocab)\\n# Add match ID \\\"HelloWorld\\\" with no callback and one pattern\\npattern = [{\\\"LOWER\\\": \\\"hello\\\"}, {\\\"IS_PUNCT\\\": True}, {\\\"LOWER\\\": \\\"world\\\"}]\\nmatcher.add(\\\"HelloWorld\\\", [pattern])\\n\\ndoc = nlp(\\\"Hello, world! Hello world!\\\")\\nmatches = matcher(doc)\\nfor match_id, start, end in matches:\\n    string_id = nlp.vocab.strings[match_id]  # Get string representation\\n    span = doc[start:end]  # The matched span\\n    print(match_id, string_id, start, end, span.text)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The matcher returns a list of \", _jsx(InlineCode, {\n          children: \"(match_id, start, end)\"\n        }), \" tuples – in this case,\\n\", _jsx(InlineCode, {\n          children: \"[('15578876784678163569', 0, 3)]\"\n        }), \", which maps to the span \", _jsx(InlineCode, {\n          children: \"doc[0:3]\"\n        }), \" of our\\noriginal document. The \", _jsx(InlineCode, {\n          children: \"match_id\"\n        }), \" is the \", _jsx(_components.a, {\n          href: \"/usage/spacy-101#vocab\",\n          children: \"hash value\"\n        }), \" of\\nthe string ID “HelloWorld”. To get the string value, you can look up the ID in\\nthe \", _jsx(_components.a, {\n          href: \"/api/stringstore\",\n          children: _jsx(InlineCode, {\n            children: \"StringStore\"\n          })\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"for match_id, start, end in matches:\\n    string_id = nlp.vocab.strings[match_id]  # 'HelloWorld'\\n    span = doc[start:end]                    # The matched span\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"Optionally, we could also choose to add more than one pattern, for example to\\nalso match sequences without punctuation between “hello” and “world”:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"patterns = [\\n    [{\\\"LOWER\\\": \\\"hello\\\"}, {\\\"IS_PUNCT\\\": True}, {\\\"LOWER\\\": \\\"world\\\"}],\\n    [{\\\"LOWER\\\": \\\"hello\\\"}, {\\\"LOWER\\\": \\\"world\\\"}]\\n]\\nmatcher.add(\\\"HelloWorld\\\", patterns)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"By default, the matcher will only return the matches and \", _jsx(_components.strong, {\n          children: \"not do anything\\nelse\"\n        }), \", like merge entities or assign labels. This is all up to you and can be\\ndefined individually for each pattern, by passing in a callback function as the\\n\", _jsx(InlineCode, {\n          children: \"on_match\"\n        }), \" argument on \", _jsx(InlineCode, {\n          children: \"add()\"\n        }), \". This is useful, because it lets you write\\nentirely custom and \", _jsx(_components.strong, {\n          children: \"pattern-specific logic\"\n        }), \". For example, you might want to\\nmerge \", _jsx(_components.em, {\n          children: \"some\"\n        }), \" patterns into one token, while adding entity labels for other\\npattern types. You shouldn’t have to create different matchers for each of those\\nprocesses.\"]\n      }), _jsx(_components.h4, {\n        id: \"adding-patterns-attributes\",\n        children: \"Available token attributes \"\n      }), _jsxs(_components.p, {\n        children: [\"The available token pattern keys correspond to a number of\\n\", _jsxs(_components.a, {\n          href: \"/api/token#attributes\",\n          children: [_jsx(InlineCode, {\n            children: \"Token\"\n          }), \" attributes\"]\n        }), \". The supported attributes for\\nrule-based matching are:\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Attribute\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"ORTH\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The exact verbatim text of a token. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"TEXT\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The exact verbatim text of a token. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NORM\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The normalized form of the token text. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"LOWER\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The lowercase form of the token text. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"LENGTH\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The length of the token text. \", _jsx(_components.del, {\n                children: \"int\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"IS_ALPHA\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"IS_ASCII\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"IS_DIGIT\"\n              })]\n            }), _jsxs(_components.td, {\n              children: [\"Token text consists of alphabetic characters, ASCII characters, digits. \", _jsx(_components.del, {\n                children: \"bool\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"IS_LOWER\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"IS_UPPER\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"IS_TITLE\"\n              })]\n            }), _jsxs(_components.td, {\n              children: [\"Token text is in lowercase, uppercase, titlecase. \", _jsx(_components.del, {\n                children: \"bool\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"IS_PUNCT\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"IS_SPACE\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"IS_STOP\"\n              })]\n            }), _jsxs(_components.td, {\n              children: [\"Token is punctuation, whitespace, stop word. \", _jsx(_components.del, {\n                children: \"bool\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"IS_SENT_START\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Token is start of sentence. \", _jsx(_components.del, {\n                children: \"bool\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"LIKE_NUM\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"LIKE_URL\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"LIKE_EMAIL\"\n              })]\n            }), _jsxs(_components.td, {\n              children: [\"Token text resembles a number, URL, email. \", _jsx(_components.del, {\n                children: \"bool\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"SPACY\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Token has a trailing space. \", _jsx(_components.del, {\n                children: \"bool\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"POS\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"TAG\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"MORPH\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"DEP\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"LEMMA\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"SHAPE\"\n              })]\n            }), _jsxs(_components.td, {\n              children: [\"The token’s simple and extended part-of-speech tag, morphological analysis, dependency label, lemma, shape. Note that the values of these attributes are case-sensitive. For a list of available part-of-speech tags and dependency labels, see the \", _jsx(_components.a, {\n                href: \"/api/annotation\",\n                children: \"Annotation Specifications\"\n              }), \". \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"ENT_TYPE\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The token’s entity label. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"_\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Properties in \", _jsx(_components.a, {\n                href: \"/usage/processing-pipelines#custom-components-attributes\",\n                children: \"custom extension attributes\"\n              }), \". \", _jsx(_components.del, {\n                children: \"Dict[str, Any]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"OP\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(_components.a, {\n                href: \"#quantifiers\",\n                children: \"Operator or quantifier\"\n              }), \" to determine how often to match a token pattern. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          })]\n        })]\n      }), _jsx(Accordion, {\n        title: \"Does it matter if the attribute names are uppercase or lowercase?\",\n        children: _jsxs(_components.p, {\n          children: [\"No, it shouldn’t. spaCy will normalize the names internally and\\n\", _jsx(InlineCode, {\n            children: \"{\\\"LOWER\\\": \\\"text\\\"}\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"{\\\"lower\\\": \\\"text\\\"}\"\n          }), \" will both produce the same result.\\nUsing the uppercase version is mostly a convention to make it clear that the\\nattributes are “special” and don’t exactly map to the token attributes like\\n\", _jsx(InlineCode, {\n            children: \"Token.lower\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"Token.lower_\"\n          }), \".\"]\n        })\n      }), _jsxs(Accordion, {\n        title: \"Why are not all token attributes supported?\",\n        children: [_jsxs(_components.p, {\n          children: [\"spaCy can’t provide access to all of the attributes because the \", _jsx(InlineCode, {\n            children: \"Matcher\"\n          }), \" loops\\nover the Cython data, not the Python objects. Inside the matcher, we’re dealing\\nwith a \", _jsxs(_components.a, {\n            href: \"/api/cython-structs#tokenc\",\n            children: [_jsx(InlineCode, {\n              children: \"TokenC\"\n            }), \" struct\"]\n          }), \" – we don’t have an instance\\nof \", _jsx(_components.a, {\n            href: \"/api/token\",\n            children: _jsx(InlineCode, {\n              children: \"Token\"\n            })\n          }), \". This means that all of the attributes that refer to\\ncomputed properties can’t be accessed.\"]\n        }), _jsxs(_components.p, {\n          children: [\"The uppercase attribute names like \", _jsx(InlineCode, {\n            children: \"LOWER\"\n          }), \" or \", _jsx(InlineCode, {\n            children: \"IS_PUNCT\"\n          }), \" refer to symbols from\\nthe \", _jsx(_components.a, {\n            href: \"https://github.com/explosion/spaCy/tree/master/spacy/attrs.pyx\",\n            children: _jsx(InlineCode, {\n              children: \"spacy.attrs\"\n            })\n          }), \" enum table. They’re passed\\ninto a function that essentially is a big case/switch statement, to figure out\\nwhich struct field to return. The same attribute identifiers are used in\\n\", _jsx(_components.a, {\n            href: \"/api/doc#to_array\",\n            children: _jsx(InlineCode, {\n              children: \"Doc.to_array\"\n            })\n          }), \", and a few other places in the code where\\nyou need to describe fields like this.\"]\n        })]\n      }), _jsx(_components.hr, {}), _jsxs(Infobox, {\n        title: \"Tip: Try the interactive matcher explorer\",\n        children: [_jsx(Image, {\n          src: \"/images/matcher-demo.jpg\",\n          href: \"https://explosion.ai/demos/matcher\",\n          alt: \"Matcher demo\"\n        }), _jsxs(_components.p, {\n          children: [\"The \", _jsx(_components.a, {\n            href: \"https://explosion.ai/demos/matcher\",\n            children: \"Matcher Explorer\"\n          }), \" lets you test the\\nrule-based \", _jsx(InlineCode, {\n            children: \"Matcher\"\n          }), \" by creating token patterns interactively and running them\\nover your text. Each token can set multiple attributes like text value,\\npart-of-speech tag or boolean flags. The token-based view lets you explore how\\nspaCy processes your text – and why your pattern matches, or why it doesn’t.\"]\n        })]\n      }), _jsx(_components.h4, {\n        id: \"adding-patterns-attributes-extended\",\n        version: \"2.1\",\n        children: \"Extended pattern syntax and attributes \"\n      }), _jsxs(_components.p, {\n        children: [\"Instead of mapping to a single value, token patterns can also map to a\\n\", _jsx(_components.strong, {\n          children: \"dictionary of properties\"\n        }), \". For example, to specify that the value of a lemma\\nshould be part of a list of values, or to set a minimum character length. The\\nfollowing rich comparison attributes are available:\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"# Matches \\\"love cats\\\" or \\\"likes flowers\\\"\\npattern1 = [{\\\"LEMMA\\\": {\\\"IN\\\": [\\\"like\\\", \\\"love\\\"]}},\\n            {\\\"POS\\\": \\\"NOUN\\\"}]\\n\\n# Matches tokens of length \u003e= 10\\npattern2 = [{\\\"LENGTH\\\": {\\\"\u003e=\\\": 10}}]\\n\\n# Match based on morph attributes\\npattern3 = [{\\\"MORPH\\\": {\\\"IS_SUBSET\\\": [\\\"Number=Sing\\\", \\\"Gender=Neut\\\"]}}]\\n# \\\"\\\", \\\"Number=Sing\\\" and \\\"Number=Sing|Gender=Neut\\\" will match as subsets\\n# \\\"Number=Plur|Gender=Neut\\\" will not match\\n# \\\"Number=Sing|Gender=Neut|Polite=Infm\\\" will not match because it's a superset\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Attribute\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"IN\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attribute value is member of a list. \", _jsx(_components.del, {\n                children: \"Any\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NOT_IN\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attribute value is \", _jsx(_components.em, {\n                children: \"not\"\n              }), \" member of a list. \", _jsx(_components.del, {\n                children: \"Any\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"IS_SUBSET\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attribute value (for \", _jsx(InlineCode, {\n                children: \"MORPH\"\n              }), \" or custom list attributes) is a subset of a list. \", _jsx(_components.del, {\n                children: \"Any\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"IS_SUPERSET\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attribute value (for \", _jsx(InlineCode, {\n                children: \"MORPH\"\n              }), \" or custom list attributes) is a superset of a list. \", _jsx(_components.del, {\n                children: \"Any\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"INTERSECTS\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attribute value (for \", _jsx(InlineCode, {\n                children: \"MORPH\"\n              }), \" or custom list attributes) has a non-empty intersection with a list. \", _jsx(_components.del, {\n                children: \"Any\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"==\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"\u003e=\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"\u003c=\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"\u003e\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"\u003c\"\n              })]\n            }), _jsxs(_components.td, {\n              children: [\"Attribute value is equal, greater or equal, smaller or equal, greater or smaller. \", _jsx(_components.del, {\n                children: \"Union[int, float]\"\n              })]\n            })]\n          })]\n        })]\n      }), _jsx(_components.h4, {\n        id: \"regex\",\n        version: \"2.1\",\n        children: \"Regular expressions \"\n      }), _jsx(_components.p, {\n        children: \"In some cases, only matching tokens and token attributes isn’t enough – for\\nexample, you might want to match different spellings of a word, without having\\nto add a new pattern for each spelling.\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"pattern = [{\\\"TEXT\\\": {\\\"REGEX\\\": \\\"^[Uu](\\\\\\\\.?|nited)$\\\"}},\\n           {\\\"TEXT\\\": {\\\"REGEX\\\": \\\"^[Ss](\\\\\\\\.?|tates)$\\\"}},\\n           {\\\"LOWER\\\": \\\"president\\\"}]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"REGEX\"\n        }), \" operator allows defining rules for any attribute string value,\\nincluding custom attributes. It always needs to be applied to an attribute like\\n\", _jsx(InlineCode, {\n          children: \"TEXT\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"LOWER\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"TAG\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# Match different spellings of token texts\\npattern = [{\\\"TEXT\\\": {\\\"REGEX\\\": \\\"deff?in[ia]tely\\\"}}]\\n\\n# Match tokens with fine-grained POS tags starting with 'V'\\npattern = [{\\\"TAG\\\": {\\\"REGEX\\\": \\\"^V\\\"}}]\\n\\n# Match custom attribute values with regular expressions\\npattern = [{\\\"_\\\": {\\\"country\\\": {\\\"REGEX\\\": \\\"^[Uu](nited|\\\\\\\\.?) ?[Ss](tates|\\\\\\\\.?)$\\\"}}}]\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"When using the \", _jsx(InlineCode, {\n            children: \"REGEX\"\n          }), \" operator, keep in mind that it operates on \", _jsx(_components.strong, {\n            children: \"single\\ntokens\"\n          }), \", not the whole text. Each expression you provide will be matched on a\\ntoken. If you need to match on the whole text instead, see the details on\\n\", _jsx(_components.a, {\n            href: \"#regex-text\",\n            children: \"regex matching on the whole text\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h5, {\n        id: \"regex-text\",\n        children: \"Matching regular expressions on the full text \"\n      }), _jsxs(_components.p, {\n        children: [\"If your expressions apply to multiple tokens, a simple solution is to match on\\nthe \", _jsx(InlineCode, {\n          children: \"doc.text\"\n        }), \" with \", _jsx(InlineCode, {\n          children: \"re.finditer\"\n        }), \" and use the\\n\", _jsx(_components.a, {\n          href: \"/api/doc#char_span\",\n          children: _jsx(InlineCode, {\n            children: \"Doc.char_span\"\n          })\n        }), \" method to create a \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" from the\\ncharacter indices of the match. If the matched characters don’t map to one or\\nmore valid tokens, \", _jsx(InlineCode, {\n          children: \"Doc.char_span\"\n        }), \" returns \", _jsx(InlineCode, {\n          children: \"None\"\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"What’s a valid token sequence?\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"In the example, the expression will also match \", _jsx(InlineCode, {\n            children: \"\\\"US\\\"\"\n          }), \" in \", _jsx(InlineCode, {\n            children: \"\\\"USA\\\"\"\n          }), \". However,\\n\", _jsx(InlineCode, {\n            children: \"\\\"USA\\\"\"\n          }), \" is a single token and \", _jsx(InlineCode, {\n            children: \"Span\"\n          }), \" objects are \", _jsx(_components.strong, {\n            children: \"sequences of tokens\"\n          }), \". So\\n\", _jsx(InlineCode, {\n            children: \"\\\"US\\\"\"\n          }), \" cannot be its own span, because it does not end on a token boundary.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nimport re\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"The United States of America (USA) are commonly known as the United States (U.S. or US) or America.\\\")\\n\\nexpression = r\\\"[Uu](nited|\\\\\\\\.?) ?[Ss](tates|\\\\\\\\.?)\\\"\\nfor match in re.finditer(expression, doc.text):\\n    start, end = match.span()\\n    span = doc.char_span(start, end)\\n    # This is a Span object or None if match doesn't map to valid token sequence\\n    if span is not None:\\n        print(\\\"Found match:\\\", span.text)\\n\"\n        })\n      }), _jsxs(Accordion, {\n        title: \"How can I expand the match to a valid token sequence?\",\n        children: [_jsxs(_components.p, {\n          children: [\"In some cases, you might want to expand the match to the closest token\\nboundaries, so you can create a \", _jsx(InlineCode, {\n            children: \"Span\"\n          }), \" for \", _jsx(InlineCode, {\n            children: \"\\\"USA\\\"\"\n          }), \", even though only the\\nsubstring \", _jsx(InlineCode, {\n            children: \"\\\"US\\\"\"\n          }), \" is matched. You can calculate this using the character offsets\\nof the tokens in the document, available as\\n\", _jsx(_components.a, {\n            href: \"/api/token#attributes\",\n            children: _jsx(InlineCode, {\n              children: \"Token.idx\"\n            })\n          }), \". This lets you create a list of valid token\\nstart and end boundaries and leaves you with a rather basic algorithmic problem:\\nGiven a number, find the next lowest (start token) or the next highest (end\\ntoken) number that’s part of a given list of numbers. This will be the closest\\nvalid token boundary.\"]\n        }), _jsxs(_components.p, {\n          children: [\"There are many ways to do this and the most straightforward one is to create a\\ndict keyed by characters in the \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \", mapped to the token they’re part of. It’s\\neasy to write and less error-prone, and gives you a constant lookup time: you\\nonly ever need to create the dict once per \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \".\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"chars_to_tokens = {}\\nfor token in doc:\\n    for i in range(token.idx, token.idx + len(token.text)):\\n        chars_to_tokens[i] = token.i\\n\"\n          })\n        }), _jsxs(_components.p, {\n          children: [\"You can then look up character at a given position, and get the index of the\\ncorresponding token that the character is part of. Your span would then be\\n\", _jsx(InlineCode, {\n            children: \"doc[token_start:token_end]\"\n          }), \". If a character isn’t in the dict, it means it’s\\nthe (white)space tokens are split on. That hopefully shouldn’t happen, though,\\nbecause it’d mean your regex is producing matches with leading or trailing\\nwhitespace.\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            highlight: \"5-8\",\n            children: \"span = doc.char_span(start, end)\\nif span is not None:\\n    print(\\\"Found match:\\\", span.text)\\nelse:\\n    start_token = chars_to_tokens.get(start)\\n    end_token = chars_to_tokens.get(end)\\n    if start_token is not None and end_token is not None:\\n        span = doc[start_token:end_token + 1]\\n        print(\\\"Found closest match:\\\", span.text)\\n\"\n          })\n        })]\n      }), _jsx(_components.h4, {\n        id: \"fuzzy\",\n        version: \"3.5\",\n        children: \"Fuzzy matching \"\n      }), _jsx(_components.p, {\n        children: \"Fuzzy matching allows you to match tokens with alternate spellings, typos, etc.\\nwithout specifying every possible variant.\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# Matches \\\"favourite\\\", \\\"favorites\\\", \\\"gavorite\\\", \\\"theatre\\\", \\\"theatr\\\", ...\\npattern = [{\\\"TEXT\\\": {\\\"FUZZY\\\": \\\"favorite\\\"}},\\n           {\\\"TEXT\\\": {\\\"FUZZY\\\": \\\"theater\\\"}}]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"FUZZY\"\n        }), \" attribute allows fuzzy matches for any attribute string value,\\nincluding custom attributes. Just like \", _jsx(InlineCode, {\n          children: \"REGEX\"\n        }), \", it always needs to be applied to\\nan attribute like \", _jsx(InlineCode, {\n          children: \"TEXT\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"LOWER\"\n        }), \". By default \", _jsx(InlineCode, {\n          children: \"FUZZY\"\n        }), \" allows a Levenshtein\\nedit distance of at least 2 and up to 30% of the pattern string length. Using\\nthe more specific attributes \", _jsx(InlineCode, {\n          children: \"FUZZY1\"\n        }), \"..\", _jsx(InlineCode, {\n          children: \"FUZZY9\"\n        }), \" you can specify the maximum\\nallowed edit distance directly.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# Match lowercase with fuzzy matching (allows 2 edits by default)\\npattern = [{\\\"LOWER\\\": {\\\"FUZZY\\\": \\\"definitely\\\"}}]\\n\\n# Match custom attribute values with fuzzy matching (allows 2 edits by default)\\npattern = [{\\\"_\\\": {\\\"country\\\": {\\\"FUZZY\\\": \\\"Kyrgyzstan\\\"}}}]\\n\\n# Match with exact Levenshtein edit distance limits (allows 4 edits)\\npattern = [{\\\"_\\\": {\\\"country\\\": {\\\"FUZZY4\\\": \\\"Kyrgyzstan\\\"}}}]\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"regex-fuzzy-lists\",\n        version: \"3.5\",\n        children: \"Regex and fuzzy matching with lists \"\n      }), _jsxs(_components.p, {\n        children: [\"Starting in spaCy v3.5, both \", _jsx(InlineCode, {\n          children: \"REGEX\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"FUZZY\"\n        }), \" can be combined with the\\nattributes \", _jsx(InlineCode, {\n          children: \"IN\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"NOT_IN\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"pattern = [{\\\"TEXT\\\": {\\\"FUZZY\\\": {\\\"IN\\\": [\\\"awesome\\\", \\\"cool\\\", \\\"wonderful\\\"]}}}]\\n\\npattern = [{\\\"TEXT\\\": {\\\"REGEX\\\": {\\\"NOT_IN\\\": [\\\"^awe(some)?$\\\", \\\"^wonder(ful)?\\\"]}}}]\\n\"\n        })\n      }), _jsx(_components.hr, {}), _jsx(_components.h4, {\n        id: \"quantifiers\",\n        children: \"Operators and quantifiers \"\n      }), _jsxs(_components.p, {\n        children: [\"The matcher also lets you use quantifiers, specified as the \", _jsx(InlineCode, {\n          children: \"'OP'\"\n        }), \" key.\\nQuantifiers let you define sequences of tokens to be matched, e.g. one or more\\npunctuation marks, or specify optional tokens. Note that there are no nested or\\nscoped quantifiers – instead, you can build those behaviors with \", _jsx(InlineCode, {\n          children: \"on_match\"\n        }), \"\\ncallbacks.\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"OP\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"!\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Negate the pattern, by requiring it to match exactly 0 times.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"?\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Make the pattern optional, by allowing it to match 0 or 1 times.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"+\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Require the pattern to match 1 or more times.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"*\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Allow the pattern to match zero or more times.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"{n}\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Require the pattern to match exactly \", _jsx(_components.em, {\n                children: \"n\"\n              }), \" times.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"{n,m}\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Require the pattern to match at least \", _jsx(_components.em, {\n                children: \"n\"\n              }), \" but not more than \", _jsx(_components.em, {\n                children: \"m\"\n              }), \" times.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"{n,}\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Require the pattern to match at least \", _jsx(_components.em, {\n                children: \"n\"\n              }), \" times.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"{,m}\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Require the pattern to match at most \", _jsx(_components.em, {\n                children: \"m\"\n              }), \" times.\"]\n            })]\n          })]\n        })]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"pattern = [{\\\"LOWER\\\": \\\"hello\\\"},\\n           {\\\"IS_PUNCT\\\": True, \\\"OP\\\": \\\"?\\\"}]\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(Infobox, {\n        title: \"Note on operator behaviour\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"In versions before v2.1.0, the semantics of the \", _jsx(InlineCode, {\n            children: \"+\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"*\"\n          }), \" operators behave\\ninconsistently. They were usually interpreted “greedily”, i.e. longer matches\\nare returned where possible. However, if you specify two \", _jsx(InlineCode, {\n            children: \"+\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"*\"\n          }), \" patterns in\\na row and their matches overlap, the first operator will behave non-greedily.\\nThis quirk in the semantics is corrected in spaCy v2.1.0.\"]\n        })\n      }), _jsx(_components.h4, {\n        id: \"adding-patterns-wildcard\",\n        version: \"2\",\n        children: \"Using wildcard token patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"While the token attributes offer many options to write highly specific patterns,\\nyou can also use an empty dictionary, \", _jsx(InlineCode, {\n          children: \"{}\"\n        }), \" as a wildcard representing \", _jsx(_components.strong, {\n          children: \"any\\ntoken\"\n        }), \". This is useful if you know the context of what you’re trying to match,\\nbut very little about the specific token and its characters. For example, let’s\\nsay you’re trying to extract people’s user names from your data. All you know is\\nthat they are listed as “User name: {username}“. The name itself may contain\\nany character, but no whitespace – so you’ll know it will be handled as one\\ntoken.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"[{\\\"ORTH\\\": \\\"User\\\"}, {\\\"ORTH\\\": \\\"name\\\"}, {\\\"ORTH\\\": \\\":\\\"}, {}]\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"pattern-validation\",\n        version: \"2.1\",\n        children: \"Validating and debugging patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"Matcher\"\n        }), \" can validate patterns against a JSON schema with the option\\n\", _jsx(InlineCode, {\n          children: \"validate=True\"\n        }), \". This is useful for debugging patterns during development, in\\nparticular for catching unsupported attributes.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import Matcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = Matcher(nlp.vocab, validate=True)\\n# Add match ID \\\"HelloWorld\\\" with unsupported attribute CASEINSENSITIVE\\npattern = [{\\\"LOWER\\\": \\\"hello\\\"}, {\\\"IS_PUNCT\\\": True}, {\\\"CASEINSENSITIVE\\\": \\\"world\\\"}]\\nmatcher.add(\\\"HelloWorld\\\", [pattern])\\n# 🚨 Raises an error:\\n# MatchPatternError: Invalid token patterns for matcher rule 'HelloWorld'\\n# Pattern 0:\\n# - [pattern -\u003e 2 -\u003e CASEINSENSITIVE] extra fields not permitted\\n\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"on_match\",\n        children: \"Adding on_match rules \"\n      }), _jsxs(_components.p, {\n        children: [\"To move on to a more realistic example, let’s say you’re working with a large\\ncorpus of blog articles, and you want to match all mentions of “Google I/O”\\n(which spaCy tokenizes as \", _jsx(InlineCode, {\n          children: \"['Google', 'I', '/', 'O'\"\n        }), \"]). To be safe, you only\\nmatch on the uppercase versions, avoiding matches with phrases such as “Google\\ni/o”.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\nfrom spacy.matcher import Matcher\\nfrom spacy.tokens import Span\\n\\nnlp = English()\\nmatcher = Matcher(nlp.vocab)\\n\\ndef add_event_ent(matcher, doc, i, matches):\\n    # Get the current match and create tuple of entity label, start and end.\\n    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\\n    match_id, start, end = matches[i]\\n    entity = Span(doc, start, end, label=\\\"EVENT\\\")\\n    doc.ents += (entity,)\\n    print(entity.text)\\n\\npattern = [{\\\"ORTH\\\": \\\"Google\\\"}, {\\\"ORTH\\\": \\\"I\\\"}, {\\\"ORTH\\\": \\\"/\\\"}, {\\\"ORTH\\\": \\\"O\\\"}]\\nmatcher.add(\\\"GoogleIO\\\", [pattern], on_match=add_event_ent)\\ndoc = nlp(\\\"This is a text about Google I/O\\\")\\nmatches = matcher(doc)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"A very similar logic has been implemented in the built-in\\n\", _jsx(_components.a, {\n          href: \"/api/entityruler\",\n          children: _jsx(InlineCode, {\n            children: \"EntityRuler\"\n          })\n        }), \" by the way. It also takes care of handling\\noverlapping matches, which you would otherwise have to take care of yourself.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Tip: Visualizing matches\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"When working with entities, you can use \", _jsx(_components.a, {\n            href: \"/api/top-level#displacy\",\n            children: \"displaCy\"\n          }), \" to\\nquickly generate a NER visualization from your updated \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \", which can be\\nexported as an HTML file:\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"from spacy import displacy\\nhtml = displacy.render(doc, style=\\\"ent\\\", page=True,\\n                       options={\\\"ents\\\": [\\\"EVENT\\\"]})\\n\"\n          })\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"For more info and examples, see the usage guide on\\n\", _jsx(_components.a, {\n            href: \"/usage/visualizers\",\n            children: \"visualizing spaCy\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"We can now call the matcher on our documents. The patterns will be matched in\\nthe order they occur in the text. The matcher will then iterate over the\\nmatches, look up the callback for the match ID that was matched, and invoke it.\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"doc = nlp(YOUR_TEXT_HERE)\\nmatcher(doc)\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"When the callback is invoked, it is passed four arguments: the matcher itself,\\nthe document, the position of the current match, and the total list of matches.\\nThis allows you to write callbacks that consider the entire set of matched\\nphrases, so that you can resolve overlaps and other conflicts in whatever way\\nyou prefer.\"\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Argument\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"matcher\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The matcher instance. \", _jsx(_components.del, {\n                children: \"Matcher\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"doc\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The document the matcher was used on. \", _jsx(_components.del, {\n                children: \"Doc\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"i\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Index of the current match (\", _jsx(InlineCode, {\n                children: \"matches[i\"\n              }), \"]). \", _jsx(_components.del, {\n                children: \"int\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"matches\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A list of \", _jsx(InlineCode, {\n                children: \"(match_id, start, end)\"\n              }), \" tuples, describing the matches. A match tuple describes a span \", _jsx(InlineCode, {\n                children: \"doc[start:end\"\n              }), \"]. \", _jsx(_components.del, {\n                children: \"List[Tuple[int, int int]]\"\n              })]\n            })]\n          })]\n        })]\n      }), _jsx(_components.h3, {\n        id: \"matcher-spans\",\n        children: \"Creating spans from matches \"\n      }), _jsxs(_components.p, {\n        children: [\"Creating \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \" objects from the returned matches is a very common\\nuse case. spaCy makes this easy by giving you access to the \", _jsx(InlineCode, {\n          children: \"start\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"end\"\n        }), \"\\ntoken of each match, which you can use to construct a new span with an optional\\nlabel. As of spaCy v3.0, you can also set \", _jsx(InlineCode, {\n          children: \"as_spans=True\"\n        }), \" when calling the\\nmatcher on a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \", which will return a list of \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \" objects\\nusing the \", _jsx(InlineCode, {\n          children: \"match_id\"\n        }), \" as the span label.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import Matcher\\nfrom spacy.tokens import Span\\n\\nnlp = spacy.blank(\\\"en\\\")\\nmatcher = Matcher(nlp.vocab)\\nmatcher.add(\\\"PERSON\\\", [[{\\\"lower\\\": \\\"barack\\\"}, {\\\"lower\\\": \\\"obama\\\"}]])\\ndoc = nlp(\\\"Barack Obama was the 44th president of the United States\\\")\\n\\n# 1. Return (match_id, start, end) tuples\\nmatches = matcher(doc)\\nfor match_id, start, end in matches:\\n    # Create the matched span and assign the match_id as a label\\n    span = Span(doc, start, end, label=match_id)\\n    print(span.text, span.label_)\\n\\n# 2. Return Span objects directly\\nmatches = matcher(doc, as_spans=True)\\nfor span in matches:\\n    print(span.text, span.label_)\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"matcher-pipeline\",\n        children: \"Using custom pipeline components \"\n      }), _jsxs(_components.p, {\n        children: [\"Let’s say your data also contains some annoying pre-processing artifacts, like\\nleftover HTML line breaks (e.g. \", _jsx(InlineCode, {\n          children: \"\u003cbr\u003e\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"\u003cBR/\u003e\"\n        }), \"). To make your text easier to\\nanalyze, you want to merge those into one token and flag them, to make sure you\\ncan ignore them later. Ideally, this should all be done automatically as you\\nprocess the text. You can achieve this by adding a\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components\",\n          children: \"custom pipeline component\"\n        }), \"\\nthat’s called on each \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object, merges the leftover HTML spans and sets an\\nattribute \", _jsx(InlineCode, {\n          children: \"bad_html\"\n        }), \" on the token.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.language import Language\\nfrom spacy.matcher import Matcher\\nfrom spacy.tokens import Token\\n\\n# We're using a component factory because the component needs to be\\n# initialized with the shared vocab via the nlp object\\n@Language.factory(\\\"html_merger\\\")\\ndef create_bad_html_merger(nlp, name):\\n    return BadHTMLMerger(nlp.vocab)\\n\\nclass BadHTMLMerger:\\n    def __init__(self, vocab):\\n        patterns = [\\n            [{\\\"ORTH\\\": \\\"\u003c\\\"}, {\\\"LOWER\\\": \\\"br\\\"}, {\\\"ORTH\\\": \\\"\u003e\\\"}],\\n            [{\\\"ORTH\\\": \\\"\u003c\\\"}, {\\\"LOWER\\\": \\\"br/\\\"}, {\\\"ORTH\\\": \\\"\u003e\\\"}],\\n        ]\\n        # Register a new token extension to flag bad HTML\\n        Token.set_extension(\\\"bad_html\\\", default=False)\\n        self.matcher = Matcher(vocab)\\n        self.matcher.add(\\\"BAD_HTML\\\", patterns)\\n\\n    def __call__(self, doc):\\n        # This method is invoked when the component is called on a Doc\\n        matches = self.matcher(doc)\\n        spans = []  # Collect the matched spans here\\n        for match_id, start, end in matches:\\n            spans.append(doc[start:end])\\n        with doc.retokenize() as retokenizer:\\n            for span in spans:\\n                retokenizer.merge(span)\\n                for token in span:\\n                    token._.bad_html = True  # Mark token as bad HTML\\n        return doc\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nnlp.add_pipe(\\\"html_merger\\\", last=True)  # Add component to the pipeline\\ndoc = nlp(\\\"Hello\u003cbr\u003eworld! \u003cbr/\u003e This is a test.\\\")\\nfor token in doc:\\n    print(token.text, token._.bad_html)\\n\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Instead of hard-coding the patterns into the component, you could also make it\\ntake a path to a JSON file containing the patterns. This lets you reuse the\\ncomponent with different patterns, depending on your application. When adding\\nthe component to the pipeline with \", _jsx(_components.a, {\n          href: \"/api/language#add_pipe\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.add_pipe\"\n          })\n        }), \", you\\ncan pass in the argument via the \", _jsx(InlineCode, {\n          children: \"config\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"@Language.factory(\\\"html_merger\\\", default_config={\\\"path\\\": None})\\ndef create_bad_html_merger(nlp, name, path):\\n    return BadHTMLMerger(nlp, path=path)\\n\\nnlp.add_pipe(\\\"html_merger\\\", config={\\\"path\\\": \\\"/path/to/patterns.json\\\"})\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Processing pipelines\",\n        emoji: \"📖\",\n        children: _jsxs(_components.p, {\n          children: [\"For more details and examples of how to \", _jsx(_components.strong, {\n            children: \"create custom pipeline components\"\n          }), \"\\nand \", _jsx(_components.strong, {\n            children: \"extension attributes\"\n          }), \", see the\\n\", _jsx(_components.a, {\n            href: \"/usage/processing-pipelines\",\n            children: \"usage guide\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"example1\",\n        children: \"Example: Using linguistic annotations \"\n      }), _jsx(_components.p, {\n        children: \"Let’s say you’re analyzing user comments and you want to find out what people\\nare saying about Facebook. You want to start off by finding adjectives following\\n“Facebook is” or “Facebook was”. This is obviously a very rudimentary solution,\\nbut it’ll be fast, and a great way to get an idea for what’s in your data. Your\\npattern could look like this:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"[{\\\"LOWER\\\": \\\"facebook\\\"}, {\\\"LEMMA\\\": \\\"be\\\"}, {\\\"POS\\\": \\\"ADV\\\", \\\"OP\\\": \\\"*\\\"}, {\\\"POS\\\": \\\"ADJ\\\"}]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"This translates to a token whose lowercase form matches “facebook” (like\\nFacebook, facebook or FACEBOOK), followed by a token with the lemma “be” (for\\nexample, is, was, or ‘s), followed by an \", _jsx(_components.strong, {\n          children: \"optional\"\n        }), \" adverb, followed by an\\nadjective. Using the linguistic annotations here is especially useful, because\\nyou can tell spaCy to match “Facebook’s annoying”, but \", _jsx(_components.strong, {\n          children: \"not\"\n        }), \" “Facebook’s\\nannoying ads”. The optional adverb makes sure you won’t miss adjectives with\\nintensifiers, like “pretty awful” or “very nice”.\"]\n      }), _jsxs(_components.p, {\n        children: [\"To get a quick overview of the results, you could collect all sentences\\ncontaining a match and render them with the\\n\", _jsx(_components.a, {\n          href: \"/usage/visualizers\",\n          children: \"displaCy visualizer\"\n        }), \". In the callback function, you’ll have\\naccess to the \", _jsx(InlineCode, {\n          children: \"start\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"end\"\n        }), \" of each match, as well as the parent \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \". This\\nlets you determine the sentence containing the match, \", _jsx(InlineCode, {\n          children: \"doc[start:end].sent\"\n        }), \", and\\ncalculate the start and end of the matched span within the sentence. Using\\ndisplaCy in \", _jsx(_components.a, {\n          href: \"/usage/visualizers#manual-usage\",\n          children: \"“manual” mode\"\n        }), \" lets you pass in a\\nlist of dictionaries containing the text and entities to render.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy import displacy\\nfrom spacy.matcher import Matcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = Matcher(nlp.vocab)\\nmatched_sents = []  # Collect data of matched sentences to be visualized\\n\\ndef collect_sents(matcher, doc, i, matches):\\n    match_id, start, end = matches[i]\\n    span = doc[start:end]  # Matched span\\n    sent = span.sent  # Sentence containing matched span\\n    # Append mock entity for match in displaCy style to matched_sents\\n    # get the match span by ofsetting the start and end of the span with the\\n    # start and end of the sentence in the doc\\n    match_ents = [{\\n        \\\"start\\\": span.start_char - sent.start_char,\\n        \\\"end\\\": span.end_char - sent.start_char,\\n        \\\"label\\\": \\\"MATCH\\\",\\n    }]\\n    matched_sents.append({\\\"text\\\": sent.text, \\\"ents\\\": match_ents})\\n\\npattern = [{\\\"LOWER\\\": \\\"facebook\\\"}, {\\\"LEMMA\\\": \\\"be\\\"}, {\\\"POS\\\": \\\"ADV\\\", \\\"OP\\\": \\\"*\\\"},\\n           {\\\"POS\\\": \\\"ADJ\\\"}]\\nmatcher.add(\\\"FacebookIs\\\", [pattern], on_match=collect_sents)  # add pattern\\ndoc = nlp(\\\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\\\")\\nmatches = matcher(doc)\\n\\n# Serve visualization of sentences containing match with displaCy\\n# set manual=True to make displaCy render straight from a dictionary\\n# (if you're not running the code within a Jupyer environment, you can\\n# use displacy.serve instead)\\ndisplacy.render(matched_sents, style=\\\"ent\\\", manual=True)\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"example2\",\n        children: \"Example: Phone numbers \"\n      }), _jsxs(_components.p, {\n        children: [\"Phone numbers can have many different formats and matching them is often tricky.\\nDuring tokenization, spaCy will leave sequences of numbers intact and only split\\non whitespace and punctuation. This means that your match pattern will have to\\nlook out for number sequences of a certain length, surrounded by specific\\npunctuation – depending on the\\n\", _jsx(_components.a, {\n          href: \"https://en.wikipedia.org/wiki/National_conventions_for_writing_telephone_numbers\",\n          children: \"national conventions\"\n        }), \".\"]\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"IS_DIGIT\"\n        }), \" flag is not very helpful here, because it doesn’t tell us\\nanything about the length. However, you can use the \", _jsx(InlineCode, {\n          children: \"SHAPE\"\n        }), \" flag, with each \", _jsx(InlineCode, {\n          children: \"d\"\n        }), \"\\nrepresenting a digit (up to 4 digits / characters):\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"[{\\\"ORTH\\\": \\\"(\\\"}, {\\\"SHAPE\\\": \\\"ddd\\\"}, {\\\"ORTH\\\": \\\")\\\"}, {\\\"SHAPE\\\": \\\"dddd\\\"},\\n {\\\"ORTH\\\": \\\"-\\\", \\\"OP\\\": \\\"?\\\"}, {\\\"SHAPE\\\": \\\"dddd\\\"}]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"This will match phone numbers of the format \", _jsx(_components.strong, {\n          children: \"(123) 4567 8901\"\n        }), \" or \", _jsx(_components.strong, {\n          children: \"(123)\\n4567-8901\"\n        }), \". To also match formats like \", _jsx(_components.strong, {\n          children: \"(123) 456 789\"\n        }), \", you can add a second\\npattern using \", _jsx(InlineCode, {\n          children: \"'ddd'\"\n        }), \" in place of \", _jsx(InlineCode, {\n          children: \"'dddd'\"\n        }), \". By hard-coding some values, you can\\nmatch only certain, country-specific numbers. For example, here’s a pattern to\\nmatch the most common formats of\\n\", _jsx(_components.a, {\n          href: \"https://en.wikipedia.org/wiki/National_conventions_for_writing_telephone_numbers#Germany\",\n          children: \"international German numbers\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"[{\\\"ORTH\\\": \\\"+\\\"}, {\\\"ORTH\\\": \\\"49\\\"}, {\\\"ORTH\\\": \\\"(\\\", \\\"OP\\\": \\\"?\\\"}, {\\\"SHAPE\\\": \\\"dddd\\\"},\\n {\\\"ORTH\\\": \\\")\\\", \\\"OP\\\": \\\"?\\\"}, {\\\"SHAPE\\\": \\\"dddd\\\", \\\"LENGTH\\\": 6}]\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"Depending on the formats your application needs to match, creating an extensive\\nset of rules like this is often better than training a model. It’ll produce more\\npredictable results, is much easier to modify and extend, and doesn’t require\\nany training data – only a set of test cases.\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import Matcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = Matcher(nlp.vocab)\\npattern = [{\\\"ORTH\\\": \\\"(\\\"}, {\\\"SHAPE\\\": \\\"ddd\\\"}, {\\\"ORTH\\\": \\\")\\\"}, {\\\"SHAPE\\\": \\\"ddd\\\"},\\n           {\\\"ORTH\\\": \\\"-\\\", \\\"OP\\\": \\\"?\\\"}, {\\\"SHAPE\\\": \\\"ddd\\\"}]\\nmatcher.add(\\\"PHONE_NUMBER\\\", [pattern])\\n\\ndoc = nlp(\\\"Call me at (123) 456 789 or (123) 456 789!\\\")\\nprint([t.text for t in doc])\\nmatches = matcher(doc)\\nfor match_id, start, end in matches:\\n    span = doc[start:end]\\n    print(span.text)\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"example3\",\n        children: \"Example: Hashtags and emoji on social media \"\n      }), _jsx(_components.p, {\n        children: \"Social media posts, especially tweets, can be difficult to work with. They’re\\nvery short and often contain various emoji and hashtags. By only looking at the\\nplain text, you’ll lose a lot of valuable semantic information.\"\n      }), _jsxs(_components.p, {\n        children: [\"Let’s say you’ve extracted a large sample of social media posts on a specific\\ntopic, for example posts mentioning a brand name or product. As the first step\\nof your data exploration, you want to filter out posts containing certain emoji\\nand use them to assign a general sentiment score, based on whether the expressed\\nemotion is positive or negative, e.g. 😀 or 😞. You also want to find, merge and\\nlabel hashtags like \", _jsx(InlineCode, {\n          children: \"#MondayMotivation\"\n        }), \", to be able to ignore or analyze them\\nlater.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Note on sentiment analysis\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"Ultimately, sentiment analysis is not always \", _jsx(_components.em, {\n            children: \"that\"\n          }), \" easy. In addition to the\\nemoji, you’ll also want to take specific words into account and check the\\n\", _jsx(InlineCode, {\n            children: \"subtree\"\n          }), \" for intensifiers like “very”, to increase the sentiment score. At\\nsome point, you might also want to train a sentiment model. However, the\\napproach described in this example is very useful for \", _jsx(_components.strong, {\n            children: \"bootstrapping rules to\\ncollect training data\"\n          }), \". It’s also an incredibly fast way to gather first\\ninsights into your data – with about 1 million tweets, you’d be looking at a\\nprocessing time of \", _jsx(_components.strong, {\n            children: \"under 1 minute\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"By default, spaCy’s tokenizer will split emoji into separate tokens. This means\\nthat you can create a pattern for one or more emoji tokens. Valid hashtags\\nusually consist of a \", _jsx(InlineCode, {\n          children: \"#\"\n        }), \", plus a sequence of ASCII characters with no\\nwhitespace, making them easy to match as well.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\nfrom spacy.matcher import Matcher\\n\\nnlp = English()  # We only want the tokenizer, so no need to load a pipeline\\nmatcher = Matcher(nlp.vocab)\\n\\npos_emoji = [\\\"😀\\\", \\\"😃\\\", \\\"😂\\\", \\\"🤣\\\", \\\"😊\\\", \\\"😍\\\"]  # Positive emoji\\nneg_emoji = [\\\"😞\\\", \\\"😠\\\", \\\"😩\\\", \\\"😢\\\", \\\"😭\\\", \\\"😒\\\"]  # Negative emoji\\n\\n# Add patterns to match one or more emoji tokens\\npos_patterns = [[{\\\"ORTH\\\": emoji}] for emoji in pos_emoji]\\nneg_patterns = [[{\\\"ORTH\\\": emoji}] for emoji in neg_emoji]\\n\\n# Function to label the sentiment\\ndef label_sentiment(matcher, doc, i, matches):\\n    match_id, start, end = matches[i]\\n    if doc.vocab.strings[match_id] == \\\"HAPPY\\\":  # Don't forget to get string!\\n        doc.sentiment += 0.1  # Add 0.1 for positive sentiment\\n    elif doc.vocab.strings[match_id] == \\\"SAD\\\":\\n        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment\\n\\nmatcher.add(\\\"HAPPY\\\", pos_patterns, on_match=label_sentiment)  # Add positive pattern\\nmatcher.add(\\\"SAD\\\", neg_patterns, on_match=label_sentiment)  # Add negative pattern\\n\\n# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\\nmatcher.add(\\\"HASHTAG\\\", [[{\\\"ORTH\\\": \\\"#\\\"}, {\\\"IS_ASCII\\\": True}]])\\n\\ndoc = nlp(\\\"Hello world 😀 #MondayMotivation\\\")\\nmatches = matcher(doc)\\nfor match_id, start, end in matches:\\n    string_id = doc.vocab.strings[match_id]  # Look up string ID\\n    span = doc[start:end]\\n    print(string_id, span.text)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Because the \", _jsx(InlineCode, {\n          children: \"on_match\"\n        }), \" callback receives the ID of each match, you can use the\\nsame function to handle the sentiment assignment for both the positive and\\nnegative pattern. To keep it simple, we’ll either add or subtract \", _jsx(InlineCode, {\n          children: \"0.1\"\n        }), \" points –\\nthis way, the score will also reflect combinations of emoji, even positive \", _jsx(_components.em, {\n          children: \"and\"\n        }), \"\\nnegative ones.\"]\n      }), _jsxs(_components.p, {\n        children: [\"With a library like \", _jsx(_components.a, {\n          href: \"https://github.com/bcongdon/python-emojipedia\",\n          children: \"Emojipedia\"\n        }), \",\\nwe can also retrieve a short description for each emoji – for example, 😍‘s\\nofficial title is “Smiling Face With Heart-Eyes”. Assigning it to a\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components-attributes\",\n          children: \"custom attribute\"\n        }), \" on\\nthe emoji span will make it available as \", _jsx(InlineCode, {\n          children: \"span._.emoji_desc\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"from emojipedia import Emojipedia  # Installation: pip install emojipedia\\nfrom spacy.tokens import Span  # Get the global Span object\\n\\nSpan.set_extension(\\\"emoji_desc\\\", default=None)  # Register the custom attribute\\n\\ndef label_sentiment(matcher, doc, i, matches):\\n    match_id, start, end = matches[i]\\n    if doc.vocab.strings[match_id] == \\\"HAPPY\\\":  # Don't forget to get string!\\n        doc.sentiment += 0.1  # Add 0.1 for positive sentiment\\n    elif doc.vocab.strings[match_id] == \\\"SAD\\\":\\n        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment\\n    span = doc[start:end]\\n    emoji = Emojipedia.search(span[0].text)  # Get data for emoji\\n    span._.emoji_desc = emoji.title  # Assign emoji description\\n\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"To label the hashtags, we can use a\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components-attributes\",\n          children: \"custom attribute\"\n        }), \" set\\non the respective token:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import Matcher\\nfrom spacy.tokens import Token\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = Matcher(nlp.vocab)\\n\\n# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\\nmatcher.add(\\\"HASHTAG\\\", [[{\\\"ORTH\\\": \\\"#\\\"}, {\\\"IS_ASCII\\\": True}]])\\n\\n# Register token extension\\nToken.set_extension(\\\"is_hashtag\\\", default=False)\\n\\ndoc = nlp(\\\"Hello world 😀 #MondayMotivation\\\")\\nmatches = matcher(doc)\\nhashtags = []\\nfor match_id, start, end in matches:\\n    if doc.vocab.strings[match_id] == \\\"HASHTAG\\\":\\n        hashtags.append(doc[start:end])\\nwith doc.retokenize() as retokenizer:\\n    for span in hashtags:\\n        retokenizer.merge(span)\\n        for token in span:\\n            token._.is_hashtag = True\\n\\nfor token in doc:\\n    print(token.text, token._.is_hashtag)\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-phrasematcher\",\n      children: [_jsx(_components.h2, {\n        id: \"phrasematcher\",\n        children: \"Efficient phrase matching \"\n      }), _jsxs(_components.p, {\n        children: [\"If you need to match large terminology lists, you can also use the\\n\", _jsx(_components.a, {\n          href: \"/api/phrasematcher\",\n          children: _jsx(InlineCode, {\n            children: \"PhraseMatcher\"\n          })\n        }), \" and create \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" objects\\ninstead of token patterns, which is much more efficient overall. The \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \"\\npatterns can contain single or multiple tokens.\"]\n      }), _jsx(_components.h3, {\n        id: \"adding-phrase-patterns\",\n        children: \"Adding phrase patterns \"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import PhraseMatcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = PhraseMatcher(nlp.vocab)\\nterms = [\\\"Barack Obama\\\", \\\"Angela Merkel\\\", \\\"Washington, D.C.\\\"]\\n# Only run nlp.make_doc to speed things up\\npatterns = [nlp.make_doc(text) for text in terms]\\nmatcher.add(\\\"TerminologyList\\\", patterns)\\n\\ndoc = nlp(\\\"German Chancellor Angela Merkel and US President Barack Obama \\\"\\n          \\\"converse in the Oval Office inside the White House in Washington, D.C.\\\")\\nmatches = matcher(doc)\\nfor match_id, start, end in matches:\\n    span = doc[start:end]\\n    print(span.text)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Since spaCy is used for processing both the patterns and the text to be matched,\\nyou won’t have to worry about specific tokenization – for example, you can\\nsimply pass in \", _jsx(InlineCode, {\n          children: \"nlp(\\\"Washington, D.C.\\\")\"\n        }), \" and won’t have to write a complex token\\npattern covering the exact tokenization of the term.\"]\n      }), _jsxs(Infobox, {\n        title: \"Important note on creating patterns\",\n        variant: \"warning\",\n        children: [_jsxs(_components.p, {\n          children: [\"To create the patterns, each phrase has to be processed with the \", _jsx(InlineCode, {\n            children: \"nlp\"\n          }), \" object.\\nIf you have a trained pipeline loaded, doing this in a loop or list\\ncomprehension can easily become inefficient and slow. If you \", _jsx(_components.strong, {\n            children: \"only need the\\ntokenization and lexical attributes\"\n          }), \", you can run\\n\", _jsx(_components.a, {\n            href: \"/api/language#make_doc\",\n            children: _jsx(InlineCode, {\n              children: \"nlp.make_doc\"\n            })\n          }), \" instead, which will only run the\\ntokenizer. For an additional speed boost, you can also use the\\n\", _jsx(_components.a, {\n            href: \"/api/tokenizer#pipe\",\n            children: _jsx(InlineCode, {\n              children: \"nlp.tokenizer.pipe\"\n            })\n          }), \" method, which will process the texts\\nas a stream.\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-diff\",\n            lang: \"diff\",\n            children: \"- patterns = [nlp(term) for term in LOTS_OF_TERMS]\\n+ patterns = [nlp.make_doc(term) for term in LOTS_OF_TERMS]\\n+ patterns = list(nlp.tokenizer.pipe(LOTS_OF_TERMS))\\n\"\n          })\n        })]\n      }), _jsx(_components.h3, {\n        id: \"phrasematcher-attrs\",\n        version: \"2.1\",\n        children: \"Matching on other token attributes \"\n      }), _jsxs(_components.p, {\n        children: [\"By default, the \", _jsx(InlineCode, {\n          children: \"PhraseMatcher\"\n        }), \" will match on the verbatim token text, e.g.\\n\", _jsx(InlineCode, {\n          children: \"Token.text\"\n        }), \". By setting the \", _jsx(InlineCode, {\n          children: \"attr\"\n        }), \" argument on initialization, you can change\\n\", _jsx(_components.strong, {\n          children: \"which token attribute the matcher should use\"\n        }), \" when comparing the phrase\\npattern to the matched \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \". For example, using the attribute \", _jsx(InlineCode, {\n          children: \"LOWER\"\n        }), \" lets you\\nmatch on \", _jsx(InlineCode, {\n          children: \"Token.lower\"\n        }), \" and create case-insensitive match patterns:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\nfrom spacy.matcher import PhraseMatcher\\n\\nnlp = English()\\nmatcher = PhraseMatcher(nlp.vocab, attr=\\\"LOWER\\\")\\npatterns = [nlp.make_doc(name) for name in [\\\"Angela Merkel\\\", \\\"Barack Obama\\\"]]\\nmatcher.add(\\\"Names\\\", patterns)\\n\\ndoc = nlp(\\\"angela merkel and us president barack Obama\\\")\\nfor match_id, start, end in matcher(doc):\\n    print(\\\"Matched based on lowercase token text:\\\", doc[start:end])\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Important note on creating patterns\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"The examples here use \", _jsx(_components.a, {\n            href: \"/api/language#make_doc\",\n            children: _jsx(InlineCode, {\n              children: \"nlp.make_doc\"\n            })\n          }), \" to create \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \"\\nobject patterns as efficiently as possible and without running any of the other\\npipeline components. If the token attribute you want to match on is set by a\\npipeline component, \", _jsx(_components.strong, {\n            children: \"make sure that the pipeline component runs\"\n          }), \" when you\\ncreate the pattern. For example, to match on \", _jsx(InlineCode, {\n            children: \"POS\"\n          }), \" or \", _jsx(InlineCode, {\n            children: \"LEMMA\"\n          }), \", the pattern \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \"\\nobjects need to have part-of-speech tags set by the \", _jsx(InlineCode, {\n            children: \"tagger\"\n          }), \" or \", _jsx(InlineCode, {\n            children: \"morphologizer\"\n          }), \".\\nYou can either call the \", _jsx(InlineCode, {\n            children: \"nlp\"\n          }), \" object on your pattern texts instead of\\n\", _jsx(InlineCode, {\n            children: \"nlp.make_doc\"\n          }), \", or use \", _jsx(_components.a, {\n            href: \"/api/language#select_pipes\",\n            children: _jsx(InlineCode, {\n              children: \"nlp.select_pipes\"\n            })\n          }), \" to\\ndisable components selectively.\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Another possible use case is matching number tokens like IP addresses based on\\ntheir shape. This means that you won’t have to worry about how those strings\\nwill be tokenized and you’ll be able to find tokens and combinations of tokens\\nbased on a few examples. Here, we’re matching on the shapes \", _jsx(InlineCode, {\n          children: \"ddd.d.d.d\"\n        }), \" and\\n\", _jsx(InlineCode, {\n          children: \"ddd.ddd.d.d\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\nfrom spacy.matcher import PhraseMatcher\\n\\nnlp = English()\\nmatcher = PhraseMatcher(nlp.vocab, attr=\\\"SHAPE\\\")\\nmatcher.add(\\\"IP\\\", [nlp(\\\"127.0.0.1\\\"), nlp(\\\"127.127.0.0\\\")])\\n\\ndoc = nlp(\\\"Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.\\\")\\nfor match_id, start, end in matcher(doc):\\n    print(\\\"Matched based on token shape:\\\", doc[start:end])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"In theory, the same also works for attributes like \", _jsx(InlineCode, {\n          children: \"POS\"\n        }), \". For example, a pattern\\n\", _jsx(InlineCode, {\n          children: \"nlp(\\\"I like cats\\\")\"\n        }), \" matched based on its part-of-speech tag would return a\\nmatch for “I love dogs”. You could also match on boolean flags like \", _jsx(InlineCode, {\n          children: \"IS_PUNCT\"\n        }), \"\\nto match phrases with the same sequence of punctuation and non-punctuation\\ntokens as the pattern. But this can easily get confusing and doesn’t have much\\nof an advantage over writing one or two token patterns.\"]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-dependencymatcher\",\n      children: [_jsx(_components.h2, {\n        id: \"dependencymatcher\",\n        version: \"3\",\n        model: \"parser\",\n        children: \"Dependency Matcher \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/dependencymatcher\",\n          children: _jsx(InlineCode, {\n            children: \"DependencyMatcher\"\n          })\n        }), \" lets you match patterns within\\nthe dependency parse using\\n\", _jsx(_components.a, {\n          href: \"https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.html\",\n          children: \"Semgrex\"\n        }), \"\\noperators. It requires a model containing a parser such as the\\n\", _jsx(_components.a, {\n          href: \"/api/dependencyparser\",\n          children: _jsx(InlineCode, {\n            children: \"DependencyParser\"\n          })\n        }), \". Instead of defining a list of\\nadjacent tokens as in \", _jsx(InlineCode, {\n          children: \"Matcher\"\n        }), \" patterns, the \", _jsx(InlineCode, {\n          children: \"DependencyMatcher\"\n        }), \" patterns match\\ntokens in the dependency parse and specify the relations between them.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            title: \"Example\",\n            children: \"from spacy.matcher import DependencyMatcher\\n\\n# \\\"[subject] ... initially founded\\\"\\npattern = [\\n  # anchor token: founded\\n  {\\n    \\\"RIGHT_ID\\\": \\\"founded\\\",\\n    \\\"RIGHT_ATTRS\\\": {\\\"ORTH\\\": \\\"founded\\\"}\\n  },\\n  # founded -\u003e subject\\n  {\\n    \\\"LEFT_ID\\\": \\\"founded\\\",\\n    \\\"REL_OP\\\": \\\"\u003e\\\",\\n    \\\"RIGHT_ID\\\": \\\"subject\\\",\\n    \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": \\\"nsubj\\\"}\\n  },\\n  # \\\"founded\\\" follows \\\"initially\\\"\\n  {\\n    \\\"LEFT_ID\\\": \\\"founded\\\",\\n    \\\"REL_OP\\\": \\\";\\\",\\n    \\\"RIGHT_ID\\\": \\\"initially\\\",\\n    \\\"RIGHT_ATTRS\\\": {\\\"ORTH\\\": \\\"initially\\\"}\\n  }\\n]\\n\\nmatcher = DependencyMatcher(nlp.vocab)\\nmatcher.add(\\\"FOUNDED\\\", [pattern])\\nmatches = matcher(doc)\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"A pattern added to the dependency matcher consists of a \", _jsx(_components.strong, {\n          children: \"list of\\ndictionaries\"\n        }), \", with each dictionary describing a \", _jsx(_components.strong, {\n          children: \"token to match\"\n        }), \" and its\\n\", _jsx(_components.strong, {\n          children: \"relation to an existing token\"\n        }), \" in the pattern. Except for the first\\ndictionary, which defines an anchor token using only \", _jsx(InlineCode, {\n          children: \"RIGHT_ID\"\n        }), \" and\\n\", _jsx(InlineCode, {\n          children: \"RIGHT_ATTRS\"\n        }), \", each pattern should have the following keys:\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"LEFT_ID\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The name of the left-hand node in the relation, which has been defined in an earlier node. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"REL_OP\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"An operator that describes how the two nodes are related. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"RIGHT_ID\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A unique name for the right-hand node in the relation. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"RIGHT_ATTRS\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The token attributes to match for the right-hand node in the same format as patterns provided to the regular token-based \", _jsx(_components.a, {\n                href: \"/api/matcher\",\n                children: _jsx(InlineCode, {\n                  children: \"Matcher\"\n                })\n              }), \". \", _jsx(_components.del, {\n                children: \"Dict[str, Any]\"\n              })]\n            })]\n          })]\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"Each additional token added to the pattern is linked to an existing token\\n\", _jsx(InlineCode, {\n          children: \"LEFT_ID\"\n        }), \" by the relation \", _jsx(InlineCode, {\n          children: \"REL_OP\"\n        }), \". The new token is given the name \", _jsx(InlineCode, {\n          children: \"RIGHT_ID\"\n        }), \"\\nand described by the attributes \", _jsx(InlineCode, {\n          children: \"RIGHT_ATTRS\"\n        }), \".\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Because the unique token \", _jsx(_components.strong, {\n            children: \"names\"\n          }), \" in \", _jsx(InlineCode, {\n            children: \"LEFT_ID\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"RIGHT_ID\"\n          }), \" are used to\\nidentify tokens, the order of the dicts in the patterns is important: a token\\nname needs to be defined as \", _jsx(InlineCode, {\n            children: \"RIGHT_ID\"\n          }), \" in one dict in the pattern \", _jsx(_components.strong, {\n            children: \"before\"\n          }), \" it\\ncan be used as \", _jsx(InlineCode, {\n            children: \"LEFT_ID\"\n          }), \" in another dict.\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"dependencymatcher-operators\",\n        children: \"Dependency matcher operators \"\n      }), _jsxs(_components.p, {\n        children: [\"The following operators are supported by the \", _jsx(InlineCode, {\n          children: \"DependencyMatcher\"\n        }), \", most of which\\ncome directly from\\n\", _jsx(_components.a, {\n          href: \"https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.html\",\n          children: \"Semgrex\"\n        }), \":\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Symbol\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A \u003c B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" is the immediate dependent of \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A \u003e B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" is the immediate head of \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A \u003c\u003c B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" is the dependent in a chain to \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \" following dep → head paths.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A \u003e\u003e B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" is the head in a chain to \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \" following head → dep paths.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A . B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" immediately precedes \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A.i == B.i - 1\"\n              }), \", and both are within the same dependency tree.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A .* B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" precedes \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A.i \u003c B.i\"\n              }), \", and both are within the same dependency tree \", _jsx(_components.em, {\n                children: \"(not in Semgrex)\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A ; B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" immediately follows \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A.i == B.i + 1\"\n              }), \", and both are within the same dependency tree \", _jsx(_components.em, {\n                children: \"(not in Semgrex)\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A ;* B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" follows \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A.i \u003e B.i\"\n              }), \", and both are within the same dependency tree \", _jsx(_components.em, {\n                children: \"(not in Semgrex)\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A $+ B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"B\"\n              }), \" is a right immediate sibling of \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \" and \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \" have the same parent and \", _jsx(InlineCode, {\n                children: \"A.i == B.i - 1\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A $- B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"B\"\n              }), \" is a left immediate sibling of \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \" and \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \" have the same parent and \", _jsx(InlineCode, {\n                children: \"A.i == B.i + 1\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A $++ B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"B\"\n              }), \" is a right sibling of \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \" and \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \" have the same parent and \", _jsx(InlineCode, {\n                children: \"A.i \u003c B.i\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A $-- B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"B\"\n              }), \" is a left sibling of \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \" and \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \" have the same parent and \", _jsx(InlineCode, {\n                children: \"A.i \u003e B.i\"\n              }), \".\"]\n            })]\n          })]\n        })]\n      }), _jsx(_components.h3, {\n        id: \"dependencymatcher-patterns\",\n        children: \"Designing dependency matcher patterns \"\n      }), _jsx(_components.p, {\n        children: \"Let’s say we want to find sentences describing who founded what kind of company:\"\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: _jsx(_components.em, {\n            children: \"Smith founded a healthcare company in 2005.\"\n          })\n        }), \"\\n\", _jsx(_components.li, {\n          children: _jsx(_components.em, {\n            children: \"Williams initially founded an insurance company in 1987.\"\n          })\n        }), \"\\n\", _jsx(_components.li, {\n          children: _jsx(_components.em, {\n            children: \"Lee, an experienced CEO, has founded two AI startups.\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"The dependency parse for “Smith founded a healthcare company” shows types of\\nrelations and tokens we want to match:\"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Visualizing the parse\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"The \", _jsxs(_components.a, {\n            href: \"/usage/visualizers\",\n            children: [_jsx(InlineCode, {\n              children: \"displacy\"\n            }), \" visualizer\"]\n          }), \" lets you render \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \" objects\\nand their dependency parse and part-of-speech tags:\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"import spacy\\nfrom spacy import displacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Smith founded a healthcare company\\\")\\ndisplacy.serve(doc)\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(Iframe, {\n        title: \"displaCy visualization of dependencies\",\n        src: \"/images/displacy-dep-founded.html\",\n        height: 450\n      }), _jsx(_components.p, {\n        children: \"The relations we’re interested in are:\"\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"the founder is the \", _jsx(_components.strong, {\n            children: \"subject\"\n          }), \" (\", _jsx(InlineCode, {\n            children: \"nsubj\"\n          }), \") of the token with the text \", _jsx(InlineCode, {\n            children: \"founded\"\n          })]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"the company is the \", _jsx(_components.strong, {\n            children: \"object\"\n          }), \" (\", _jsx(InlineCode, {\n            children: \"dobj\"\n          }), \") of \", _jsx(InlineCode, {\n            children: \"founded\"\n          })]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"the kind of company may be an \", _jsx(_components.strong, {\n            children: \"adjective\"\n          }), \" (\", _jsx(InlineCode, {\n            children: \"amod\"\n          }), \", not shown above) or a\\n\", _jsx(_components.strong, {\n            children: \"compound\"\n          }), \" (\", _jsx(InlineCode, {\n            children: \"compound\"\n          }), \")\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"The first step is to pick an \", _jsx(_components.strong, {\n          children: \"anchor token\"\n        }), \" for the pattern. Since it’s the\\nroot of the dependency parse, \", _jsx(InlineCode, {\n          children: \"founded\"\n        }), \" is a good choice here. It is often\\neasier to construct patterns when all dependency relation operators point from\\nthe head to the children. In this example, we’ll only use \", _jsx(InlineCode, {\n          children: \"\u003e\"\n        }), \", which connects a\\nhead to an immediate dependent as \", _jsx(InlineCode, {\n          children: \"head \u003e child\"\n        }), \".\"]\n      }), _jsx(_components.p, {\n        children: \"The simplest dependency matcher pattern will identify and name a single token in\\nthe tree:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import DependencyMatcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = DependencyMatcher(nlp.vocab)\\npattern = [\\n  {\\n    \\\"RIGHT_ID\\\": \\\"anchor_founded\\\",       # unique name\\n    \\\"RIGHT_ATTRS\\\": {\\\"ORTH\\\": \\\"founded\\\"}  # token pattern for \\\"founded\\\"\\n  }\\n]\\nmatcher.add(\\\"FOUNDED\\\", [pattern])\\ndoc = nlp(\\\"Smith founded two companies.\\\")\\nmatches = matcher(doc)\\nprint(matches) # [(4851363122962674176, [1])]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Now that we have a named anchor token (\", _jsx(InlineCode, {\n          children: \"anchor_founded\"\n        }), \"), we can add the founder\\nas the immediate dependent (\", _jsx(InlineCode, {\n          children: \"\u003e\"\n        }), \") of \", _jsx(InlineCode, {\n          children: \"founded\"\n        }), \" with the dependency label \", _jsx(InlineCode, {\n          children: \"nsubj\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Step 1\",\n          highlight: \"8,10\",\n          children: \"pattern = [\\n    {\\n        \\\"RIGHT_ID\\\": \\\"anchor_founded\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"ORTH\\\": \\\"founded\\\"}\\n    },\\n    {\\n        \\\"LEFT_ID\\\": \\\"anchor_founded\\\",\\n        \\\"REL_OP\\\": \\\"\u003e\\\",\\n        \\\"RIGHT_ID\\\": \\\"founded_subject\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": \\\"nsubj\\\"},\\n    }\\n    # ...\\n]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The direct object (\", _jsx(InlineCode, {\n          children: \"dobj\"\n        }), \") is added in the same way:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Step 2\",\n          children: \"pattern = [\\n    #...\\n    {\\n        \\\"LEFT_ID\\\": \\\"anchor_founded\\\",\\n        \\\"REL_OP\\\": \\\"\u003e\\\",\\n        \\\"RIGHT_ID\\\": \\\"founded_object\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": \\\"dobj\\\"},\\n    }\\n    # ...\\n]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"When the subject and object tokens are added, they are required to have names\\nunder the key \", _jsx(InlineCode, {\n          children: \"RIGHT_ID\"\n        }), \", which are allowed to be any unique string, e.g.\\n\", _jsx(InlineCode, {\n          children: \"founded_subject\"\n        }), \". These names can then be used as \", _jsx(InlineCode, {\n          children: \"LEFT_ID\"\n        }), \" to \", _jsx(_components.strong, {\n          children: \"link new\\ntokens into the pattern\"\n        }), \". For the final part of our pattern, we’ll specify that\\nthe token \", _jsx(InlineCode, {\n          children: \"founded_object\"\n        }), \" should have a modifier with the dependency relation\\n\", _jsx(InlineCode, {\n          children: \"amod\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"compound\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Step 3\",\n          highlight: \"7\",\n          children: \"pattern = [\\n    # ...\\n    {\\n        \\\"LEFT_ID\\\": \\\"founded_object\\\",\\n        \\\"REL_OP\\\": \\\"\u003e\\\",\\n        \\\"RIGHT_ID\\\": \\\"founded_object_modifier\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": {\\\"IN\\\": [\\\"amod\\\", \\\"compound\\\"]}},\\n    }\\n]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"You can picture the process of creating a dependency matcher pattern as defining\\nan anchor token on the left and building up the pattern by linking tokens\\none-by-one on the right using relation operators. To create a valid pattern,\\neach new token needs to be linked to an existing token on its left. As for\\n\", _jsx(InlineCode, {\n          children: \"founded\"\n        }), \" in this example, a token may be linked to more than one token on its\\nright:\"]\n      }), _jsx(_components.img, {\n        src: \"/images/dep-match-diagram.svg\",\n        alt: \"Dependency matcher pattern\"\n      }), _jsx(_components.p, {\n        children: \"The full pattern comes together as shown in the example below:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import DependencyMatcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = DependencyMatcher(nlp.vocab)\\n\\npattern = [\\n    {\\n        \\\"RIGHT_ID\\\": \\\"anchor_founded\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"ORTH\\\": \\\"founded\\\"}\\n    },\\n    {\\n        \\\"LEFT_ID\\\": \\\"anchor_founded\\\",\\n        \\\"REL_OP\\\": \\\"\u003e\\\",\\n        \\\"RIGHT_ID\\\": \\\"founded_subject\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": \\\"nsubj\\\"},\\n    },\\n    {\\n        \\\"LEFT_ID\\\": \\\"anchor_founded\\\",\\n        \\\"REL_OP\\\": \\\"\u003e\\\",\\n        \\\"RIGHT_ID\\\": \\\"founded_object\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": \\\"dobj\\\"},\\n    },\\n    {\\n        \\\"LEFT_ID\\\": \\\"founded_object\\\",\\n        \\\"REL_OP\\\": \\\"\u003e\\\",\\n        \\\"RIGHT_ID\\\": \\\"founded_object_modifier\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": {\\\"IN\\\": [\\\"amod\\\", \\\"compound\\\"]}},\\n    }\\n]\\n\\nmatcher.add(\\\"FOUNDED\\\", [pattern])\\ndoc = nlp(\\\"Lee, an experienced CEO, has founded two AI startups.\\\")\\nmatches = matcher(doc)\\n\\nprint(matches) # [(4851363122962674176, [6, 0, 10, 9])]\\n# Each token_id corresponds to one pattern dict\\nmatch_id, token_ids = matches[0]\\nfor i in range(len(token_ids)):\\n    print(pattern[i][\\\"RIGHT_ID\\\"] + \\\":\\\", doc[token_ids[i]].text)\\n\"\n        })\n      }), _jsxs(Infobox, {\n        title: \"Important note on speed\",\n        variant: \"warning\",\n        children: [_jsxs(_components.p, {\n          children: [\"The dependency matcher may be slow when token patterns can potentially match\\nmany tokens in the sentence or when relation operators allow longer paths in the\\ndependency parse, e.g. \", _jsx(InlineCode, {\n            children: \"\u003c\u003c\"\n          }), \", \", _jsx(InlineCode, {\n            children: \"\u003e\u003e\"\n          }), \", \", _jsx(InlineCode, {\n            children: \".*\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \";*\"\n          }), \".\"]\n        }), _jsxs(_components.p, {\n          children: [\"To improve the matcher speed, try to make your token patterns and operators as\\nspecific as possible. For example, use \", _jsx(InlineCode, {\n            children: \"\u003e\"\n          }), \" instead of \", _jsx(InlineCode, {\n            children: \"\u003e\u003e\"\n          }), \" if possible and use\\ntoken patterns that include dependency labels and other token attributes instead\\nof patterns such as \", _jsx(InlineCode, {\n            children: \"{}\"\n          }), \" that match any token in the sentence.\"]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-entityruler\",\n      children: [_jsx(_components.h2, {\n        id: \"entityruler\",\n        version: \"2.1\",\n        children: \"Rule-based entity recognition \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/entityruler\",\n          children: _jsx(InlineCode, {\n            children: \"EntityRuler\"\n          })\n        }), \" is a component that lets you add named\\nentities based on pattern dictionaries, which makes it easy to combine\\nrule-based and statistical named entity recognition for even more powerful\\npipelines.\"]\n      }), _jsx(_components.h3, {\n        id: \"entityruler-patterns\",\n        children: \"Entity Patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"Entity patterns are dictionaries with two keys: \", _jsx(InlineCode, {\n          children: \"\\\"label\\\"\"\n        }), \", specifying the label\\nto assign to the entity if the pattern is matched, and \", _jsx(InlineCode, {\n          children: \"\\\"pattern\\\"\"\n        }), \", the match\\npattern. The entity ruler accepts two types of patterns:\"]\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.strong, {\n              children: \"Phrase patterns\"\n            }), \" for exact string matches (string).\"]\n          }), \"\\n\", _jsx(_components.pre, {\n            children: _jsx(_components.code, {\n              className: \"language-python\",\n              lang: \"python\",\n              children: \"{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"}\\n\"\n            })\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.strong, {\n              children: \"Token patterns\"\n            }), \" with one dictionary describing one token (list).\"]\n          }), \"\\n\", _jsx(_components.pre, {\n            children: _jsx(_components.code, {\n              className: \"language-python\",\n              lang: \"python\",\n              children: \"{\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}]}\\n\"\n            })\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.h3, {\n        id: \"entityruler-usage\",\n        children: \"Using the entity ruler \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/entityruler\",\n          children: _jsx(InlineCode, {\n            children: \"EntityRuler\"\n          })\n        }), \" is a pipeline component that’s typically\\nadded via \", _jsx(_components.a, {\n          href: \"/api/language#add_pipe\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.add_pipe\"\n          })\n        }), \". When the \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object is\\ncalled on a text, it will find matches in the \", _jsx(InlineCode, {\n          children: \"doc\"\n        }), \" and add them as entities to\\nthe \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \", using the specified pattern label as the entity label. If any\\nmatches were to overlap, the pattern matching most tokens takes priority. If\\nthey also happen to be equally long, then the match occurring first in the \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \"\\nis chosen.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\n\\nnlp = English()\\nruler = nlp.add_pipe(\\\"entity_ruler\\\")\\npatterns = [{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"},\\n            {\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}]}]\\nruler.add_patterns(patterns)\\n\\ndoc = nlp(\\\"Apple is opening its first big office in San Francisco.\\\")\\nprint([(ent.text, ent.label_) for ent in doc.ents])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The entity ruler is designed to integrate with spaCy’s existing pipeline\\ncomponents and enhance the named entity recognizer. If it’s added \", _jsxs(_components.strong, {\n          children: [\"before the\\n\", _jsx(InlineCode, {\n            children: \"\\\"ner\\\"\"\n          }), \" component\"]\n        }), \", the entity recognizer will respect the existing entity\\nspans and adjust its predictions around it. This can significantly improve\\naccuracy in some cases. If it’s added \", _jsxs(_components.strong, {\n          children: [\"after the \", _jsx(InlineCode, {\n            children: \"\\\"ner\\\"\"\n          }), \" component\"]\n        }), \", the\\nentity ruler will only add spans to the \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \" if they don’t overlap with\\nexisting entities predicted by the model. To overwrite overlapping entities, you\\ncan set \", _jsx(InlineCode, {\n          children: \"overwrite_ents=True\"\n        }), \" on initialization.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nruler = nlp.add_pipe(\\\"entity_ruler\\\")\\npatterns = [{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"MyCorp Inc.\\\"}]\\nruler.add_patterns(patterns)\\n\\ndoc = nlp(\\\"MyCorp Inc. is a company in the U.S.\\\")\\nprint([(ent.text, ent.label_) for ent in doc.ents])\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"entityruler-pattern-validation\",\n        version: \"2.1.8\",\n        children: \"Validating and debugging EntityRuler patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"The entity ruler can validate patterns against a JSON schema with the config\\nsetting \", _jsx(InlineCode, {\n          children: \"\\\"validate\\\"\"\n        }), \". See details under\\n\", _jsx(_components.a, {\n          href: \"#pattern-validation\",\n          children: \"Validating and debugging patterns\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"ruler = nlp.add_pipe(\\\"entity_ruler\\\", config={\\\"validate\\\": True})\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"entityruler-ent-ids\",\n        version: \"2.2.2\",\n        children: \"Adding IDs to patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/entityruler\",\n          children: _jsx(InlineCode, {\n            children: \"EntityRuler\"\n          })\n        }), \" can also accept an \", _jsx(InlineCode, {\n          children: \"id\"\n        }), \" attribute for each\\npattern. Using the \", _jsx(InlineCode, {\n          children: \"id\"\n        }), \" attribute allows multiple patterns to be associated with\\nthe same entity.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\n\\nnlp = English()\\nruler = nlp.add_pipe(\\\"entity_ruler\\\")\\npatterns = [{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\", \\\"id\\\": \\\"apple\\\"},\\n            {\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}], \\\"id\\\": \\\"san-francisco\\\"},\\n            {\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"fran\\\"}], \\\"id\\\": \\\"san-francisco\\\"}]\\nruler.add_patterns(patterns)\\n\\ndoc1 = nlp(\\\"Apple is opening its first big office in San Francisco.\\\")\\nprint([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])\\n\\ndoc2 = nlp(\\\"Apple is opening its first big office in San Fran.\\\")\\nprint([(ent.text, ent.label_, ent.ent_id_) for ent in doc2.ents])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"If the \", _jsx(InlineCode, {\n          children: \"id\"\n        }), \" attribute is included in the \", _jsx(_components.a, {\n          href: \"/api/entityruler\",\n          children: _jsx(InlineCode, {\n            children: \"EntityRuler\"\n          })\n        }), \"\\npatterns, the \", _jsx(InlineCode, {\n          children: \"ent_id_\"\n        }), \" property of the matched entity is set to the \", _jsx(InlineCode, {\n          children: \"id\"\n        }), \" given\\nin the patterns. So in the example above it’s easy to identify that “San\\nFrancisco” and “San Fran” are both the same entity.\"]\n      }), _jsx(_components.h3, {\n        id: \"entityruler-files\",\n        children: \"Using pattern files \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/entityruler#to_disk\",\n          children: _jsx(InlineCode, {\n            children: \"to_disk\"\n          })\n        }), \" and\\n\", _jsx(_components.a, {\n          href: \"/api/entityruler#from_disk\",\n          children: _jsx(InlineCode, {\n            children: \"from_disk\"\n          })\n        }), \" let you save and load patterns to and\\nfrom JSONL (newline-delimited JSON) files, containing one pattern object per\\nline.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-json\",\n          lang: \"json\",\n          title: \"patterns.jsonl\",\n          children: \"{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"}\\n{\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}]}\\n\"\n        })\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"ruler.to_disk(\\\"./patterns.jsonl\\\")\\nnew_ruler = nlp.add_pipe(\\\"entity_ruler\\\").from_disk(\\\"./patterns.jsonl\\\")\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Integration with Prodigy\",\n        children: _jsxs(_components.p, {\n          children: [\"If you’re using the \", _jsx(_components.a, {\n            href: \"https://prodi.gy\",\n            children: \"Prodigy\"\n          }), \" annotation tool, you might\\nrecognize these pattern files from bootstrapping your named entity and text\\nclassification labelling. The patterns for the \", _jsx(InlineCode, {\n            children: \"EntityRuler\"\n          }), \" follow the same\\nsyntax, so you can use your existing Prodigy pattern files in spaCy, and vice\\nversa.\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"When you save out an \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object that has an \", _jsx(InlineCode, {\n          children: \"EntityRuler\"\n        }), \" added to its\\npipeline, its patterns are automatically exported to the pipeline directory:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"nlp = spacy.load(\\\"en_core_web_sm\\\")\\nruler = nlp.add_pipe(\\\"entity_ruler\\\")\\nruler.add_patterns([{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"}])\\nnlp.to_disk(\\\"/path/to/pipeline\\\")\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The saved pipeline now includes the \", _jsx(InlineCode, {\n          children: \"\\\"entity_ruler\\\"\"\n        }), \" in its\\n\", _jsx(_components.a, {\n          href: \"/api/data-formats#config\",\n          children: _jsx(InlineCode, {\n            children: \"config.cfg\"\n          })\n        }), \" and the pipeline directory contains a\\nfile \", _jsx(InlineCode, {\n          children: \"patterns.jsonl\"\n        }), \" with the patterns. When you load the pipeline back in, all\\npipeline components will be restored and deserialized – including the entity\\nruler. This lets you ship powerful pipeline packages with binary weights \", _jsx(_components.em, {\n          children: \"and\"\n        }), \"\\nrules included!\"]\n      }), _jsx(_components.h3, {\n        id: \"entityruler-large-phrase-patterns\",\n        version: \"2.2.4\",\n        children: \"Using a large number of phrase patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"When using a large amount of \", _jsx(_components.strong, {\n          children: \"phrase patterns\"\n        }), \" (roughly \u003e 10000) it’s useful\\nto understand how the \", _jsx(InlineCode, {\n          children: \"add_patterns\"\n        }), \" function of the entity ruler works. For\\neach \", _jsx(_components.strong, {\n          children: \"phrase pattern\"\n        }), \", the EntityRuler calls the nlp object to construct a doc\\nobject. This happens in case you try to add the EntityRuler at the end of an\\nexisting pipeline with, for example, a POS tagger and want to extract matches\\nbased on the pattern’s POS signature. In this case you would pass a config value\\nof \", _jsx(InlineCode, {\n          children: \"\\\"phrase_matcher_attr\\\": \\\"POS\\\"\"\n        }), \" for the entity ruler.\"]\n      }), _jsxs(_components.p, {\n        children: [\"Running the full language pipeline across every pattern in a large list scales\\nlinearly and can therefore take a long time on large amounts of phrase patterns.\\nAs of spaCy v2.2.4 the \", _jsx(InlineCode, {\n          children: \"add_patterns\"\n        }), \" function has been refactored to use\\n\", _jsx(InlineCode, {\n          children: \"nlp.pipe\"\n        }), \" on all phrase patterns resulting in about a 10x-20x speed up with\\n5,000-100,000 phrase patterns respectively. Even with this speedup (but\\nespecially if you’re using an older version) the \", _jsx(InlineCode, {\n          children: \"add_patterns\"\n        }), \" function can\\nstill take a long time. An easy workaround to make this function run faster is\\ndisabling the other language pipes while adding the phrase patterns.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"ruler = nlp.add_pipe(\\\"entity_ruler\\\")\\npatterns = [{\\\"label\\\": \\\"TEST\\\", \\\"pattern\\\": str(i)} for i in range(100000)]\\nwith nlp.select_pipes(enable=\\\"tagger\\\"):\\n    ruler.add_patterns(patterns)\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-spanruler\",\n      children: [_jsx(_components.h2, {\n        id: \"spanruler\",\n        version: \"3.3.1\",\n        children: \"Rule-based span matching \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/spanruler\",\n          children: _jsx(InlineCode, {\n            children: \"SpanRuler\"\n          })\n        }), \" is a generalized version of the entity ruler\\nthat lets you add spans to \", _jsx(InlineCode, {\n          children: \"doc.spans\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \" based on pattern\\ndictionaries, which makes it easy to combine rule-based and statistical pipeline\\ncomponents.\"]\n      }), _jsx(_components.h3, {\n        id: \"spanruler-patterns\",\n        children: \"Span patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"#entityruler-patterns\",\n          children: \"pattern format\"\n        }), \" is the same as for the entity ruler:\"]\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.strong, {\n              children: \"Phrase patterns\"\n            }), \" for exact string matches (string).\"]\n          }), \"\\n\", _jsx(_components.pre, {\n            children: _jsx(_components.code, {\n              className: \"language-python\",\n              lang: \"python\",\n              children: \"{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"}\\n\"\n            })\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.strong, {\n              children: \"Token patterns\"\n            }), \" with one dictionary describing one token (list).\"]\n          }), \"\\n\", _jsx(_components.pre, {\n            children: _jsx(_components.code, {\n              className: \"language-python\",\n              lang: \"python\",\n              children: \"{\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}]}\\n\"\n            })\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.h3, {\n        id: \"spanruler-usage\",\n        children: \"Using the span ruler \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/spanruler\",\n          children: _jsx(InlineCode, {\n            children: \"SpanRuler\"\n          })\n        }), \" is a pipeline component that’s typically added\\nvia \", _jsx(_components.a, {\n          href: \"/api/language#add_pipe\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.add_pipe\"\n          })\n        }), \". When the \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object is called on\\na text, it will find matches in the \", _jsx(InlineCode, {\n          children: \"doc\"\n        }), \" and add them as spans to\\n\", _jsx(InlineCode, {\n          children: \"doc.spans[\\\"ruler\\\"]\"\n        }), \", using the specified pattern label as the entity label.\\nUnlike in \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \", overlapping matches are allowed in \", _jsx(InlineCode, {\n          children: \"doc.spans\"\n        }), \", so no\\nfiltering is required, but optional filtering and sorting can be applied to the\\nspans before they’re saved.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.blank(\\\"en\\\")\\nruler = nlp.add_pipe(\\\"span_ruler\\\")\\npatterns = [{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"},\\n            {\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}]}]\\nruler.add_patterns(patterns)\\n\\ndoc = nlp(\\\"Apple is opening its first big office in San Francisco.\\\")\\nprint([(span.text, span.label_) for span in doc.spans[\\\"ruler\\\"]])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The span ruler is designed to integrate with spaCy’s existing pipeline\\ncomponents and enhance the \", _jsx(_components.a, {\n          href: \"/api/spancat\",\n          children: \"SpanCategorizer\"\n        }), \" and\\n\", _jsx(_components.a, {\n          href: \"/api/entityrecognizer\",\n          children: \"EntityRecognizer\"\n        }), \". The \", _jsx(InlineCode, {\n          children: \"overwrite\"\n        }), \" setting determines\\nwhether the existing annotation in \", _jsx(InlineCode, {\n          children: \"doc.spans\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \" is preserved.\\nBecause overlapping entities are not allowed for \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \", the entities are\\nalways filtered, using \", _jsx(_components.a, {\n          href: \"/api/top-level#util.filter_spans\",\n          children: _jsx(InlineCode, {\n            children: \"util.filter_spans\"\n          })\n        }), \"\\nby default. See the \", _jsxs(_components.a, {\n          href: \"/api/spanruler\",\n          children: [_jsx(InlineCode, {\n            children: \"SpanRuler\"\n          }), \" API docs\"]\n        }), \" for more information\\nabout how to customize the sorting and filtering of matched spans.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\n# only annotate doc.ents, not doc.spans\\nconfig = {\\\"spans_key\\\": None, \\\"annotate_ents\\\": True, \\\"overwrite\\\": False}\\nruler = nlp.add_pipe(\\\"span_ruler\\\", config=config)\\npatterns = [{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"MyCorp Inc.\\\"}]\\nruler.add_patterns(patterns)\\n\\ndoc = nlp(\\\"MyCorp Inc. is a company in the U.S.\\\")\\nprint([(ent.text, ent.label_) for ent in doc.ents])\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"spanruler-files\",\n        children: \"Using pattern files \"\n      }), _jsxs(_components.p, {\n        children: [\"You can save patterns in a JSONL file (newline-delimited JSON) to load with\\n\", _jsx(_components.a, {\n          href: \"/api/spanruler#initialize\",\n          children: _jsx(InlineCode, {\n            children: \"SpanRuler.initialize\"\n          })\n        }), \" or\\n\", _jsx(_components.a, {\n          href: \"/api/spanruler#add_patterns\",\n          children: _jsx(InlineCode, {\n            children: \"SpanRuler.add_patterns\"\n          })\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-json\",\n          lang: \"json\",\n          title: \"patterns.jsonl\",\n          children: \"{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"}\\n{\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}]}\\n\"\n        })\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"import srsly\\n\\npatterns = srsly.read_jsonl(\\\"patterns.jsonl\\\")\\nruler = nlp.add_pipe(\\\"span_ruler\\\")\\nruler.add_patterns(patterns)\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Unlike the entity ruler, the span ruler cannot load patterns on initialization\\nwith \", _jsx(InlineCode, {\n            children: \"SpanRuler(patterns=patterns)\"\n          }), \" or directly from a JSONL file path with\\n\", _jsx(InlineCode, {\n            children: \"SpanRuler.from_disk(jsonl_path)\"\n          }), \". Patterns should be loaded from the JSONL file\\nseparately and then added through\\n\", _jsx(_components.a, {\n            href: \"/api/spanruler#initialize%5D\",\n            children: _jsx(InlineCode, {\n              children: \"SpanRuler.initialize\"\n            })\n          }), \" or\\n\", _jsx(_components.a, {\n            href: \"/api/spanruler#add_patterns\",\n            children: _jsx(InlineCode, {\n              children: \"SpanRuler.add_patterns\"\n            })\n          }), \" as shown above.\"]\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-models-rules\",\n      children: [_jsx(_components.h2, {\n        id: \"models-rules\",\n        children: \"Combining models and rules \"\n      }), _jsx(_components.p, {\n        children: \"You can combine statistical and rule-based components in a variety of ways.\\nRule-based components can be used to improve the accuracy of statistical models,\\nby presetting tags, entities or sentence boundaries for specific tokens. The\\nstatistical models will usually respect these preset annotations, which\\nsometimes improves the accuracy of other decisions. You can also use rule-based\\ncomponents after a statistical model to correct common errors. Finally,\\nrule-based components can reference the attributes set by statistical models, in\\norder to implement more abstract logic.\"\n      }), _jsx(_components.h3, {\n        id: \"models-rules-ner\",\n        children: \"Example: Expanding named entities \"\n      }), _jsxs(_components.p, {\n        children: [\"When using a trained\\n\", _jsx(_components.a, {\n          href: \"/usage/linguistic-features/#named-entities\",\n          children: \"named entity recognition\"\n        }), \" model to\\nextract information from your texts, you may find that the predicted span only\\nincludes parts of the entity you’re looking for. Sometimes, this happens if\\nstatistical model predicts entities incorrectly. Other times, it happens if the\\nway the entity type was defined in the original training corpus doesn’t match\\nwhat you need for your application.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Where corpora come from\"\n        }), \"\\n\", _jsx(_components.p, {\n          children: \"Corpora used to train pipelines from scratch are often produced in academia.\\nThey contain text from various sources with linguistic features labeled\\nmanually by human annotators (following a set of specific guidelines). The\\ncorpora are then distributed with evaluation data, so other researchers can\\nbenchmark their algorithms and everyone can report numbers on the same data.\\nHowever, most applications need to learn information that isn’t contained in\\nany available corpus.\"\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"For example, the corpus spaCy’s \", _jsx(_components.a, {\n          href: \"/models/en\",\n          children: \"English pipelines\"\n        }), \" were trained on\\ndefines a \", _jsx(InlineCode, {\n          children: \"PERSON\"\n        }), \" entity as just the \", _jsx(_components.strong, {\n          children: \"person name\"\n        }), \", without titles like “Mr.”\\nor “Dr.”. This makes sense, because it makes it easier to resolve the entity\\ntype back to a knowledge base. But what if your application needs the full\\nnames, \", _jsx(_components.em, {\n          children: \"including\"\n        }), \" the titles?\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Dr. Alex Smith chaired first board meeting of Acme Corp Inc.\\\")\\nprint([(ent.text, ent.label_) for ent in doc.ents])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"While you could try and teach the model a new definition of the \", _jsx(InlineCode, {\n          children: \"PERSON\"\n        }), \" entity\\nby \", _jsx(_components.a, {\n          href: \"/usage/training/#example-train-ner\",\n          children: \"updating it\"\n        }), \" with more examples of spans\\nthat include the title, this might not be the most efficient approach. The\\nexisting model was trained on over 2 million words, so in order to completely\\nchange the definition of an entity type, you might need a lot of training\\nexamples. However, if you already have the predicted \", _jsx(InlineCode, {\n          children: \"PERSON\"\n        }), \" entities, you can\\nuse a rule-based approach that checks whether they come with a title and if so,\\nexpands the entity span by one token. After all, what all titles in this example\\nhave in common is that \", _jsx(_components.em, {\n          children: \"if\"\n        }), \" they occur, they occur in the \", _jsx(_components.strong, {\n          children: \"previous token\"\n        }), \"\\nright before the person entity.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          highlight: \"9-13\",\n          children: \"from spacy.language import Language\\nfrom spacy.tokens import Span\\n\\n@Language.component(\\\"expand_person_entities\\\")\\ndef expand_person_entities(doc):\\n    new_ents = []\\n    for ent in doc.ents:\\n        # Only check for title if it's a person and not the first token\\n        if ent.label_ == \\\"PERSON\\\" and ent.start != 0:\\n            prev_token = doc[ent.start - 1]\\n            if prev_token.text in (\\\"Dr\\\", \\\"Dr.\\\", \\\"Mr\\\", \\\"Mr.\\\", \\\"Ms\\\", \\\"Ms.\\\"):\\n                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\\n                new_ents.append(new_ent)\\n            else:\\n                new_ents.append(ent)\\n        else:\\n            new_ents.append(ent)\\n    doc.ents = new_ents\\n    return doc\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The above function takes a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object, modifies its \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \" and returns it.\\nUsing the \", _jsx(_components.a, {\n          href: \"/api/language#component\",\n          children: _jsx(InlineCode, {\n            children: \"@Language.component\"\n          })\n        }), \" decorator, we can\\nregister it as a \", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines\",\n          children: \"pipeline component\"\n        }), \" so it can run\\nautomatically when processing a text. We can use\\n\", _jsx(_components.a, {\n          href: \"/api/language#add_pipe\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.add_pipe\"\n          })\n        }), \" to add it to the current pipeline.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.language import Language\\nfrom spacy.tokens import Span\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\n\\n@Language.component(\\\"expand_person_entities\\\")\\ndef expand_person_entities(doc):\\n    new_ents = []\\n    for ent in doc.ents:\\n        if ent.label_ == \\\"PERSON\\\" and ent.start != 0:\\n            prev_token = doc[ent.start - 1]\\n            if prev_token.text in (\\\"Dr\\\", \\\"Dr.\\\", \\\"Mr\\\", \\\"Mr.\\\", \\\"Ms\\\", \\\"Ms.\\\"):\\n                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\\n                new_ents.append(new_ent)\\n        else:\\n            new_ents.append(ent)\\n    doc.ents = new_ents\\n    return doc\\n\\n# Add the component after the named entity recognizer\\nnlp.add_pipe(\\\"expand_person_entities\\\", after=\\\"ner\\\")\\n\\ndoc = nlp(\\\"Dr. Alex Smith chaired first board meeting of Acme Corp Inc.\\\")\\nprint([(ent.text, ent.label_) for ent in doc.ents])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"An alternative approach would be to use an\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines/#custom-components-attributes\",\n          children: \"extension attribute\"\n        }), \"\\nlike \", _jsx(InlineCode, {\n          children: \"._.person_title\"\n        }), \" and add it to \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" objects (which includes entity spans\\nin \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \"). The advantage here is that the entity text stays intact and can\\nstill be used to look up the name in a knowledge base. The following function\\ntakes a \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" object, checks the previous token if it’s a \", _jsx(InlineCode, {\n          children: \"PERSON\"\n        }), \" entity and\\nreturns the title if one is found. The \", _jsx(InlineCode, {\n          children: \"Span.doc\"\n        }), \" attribute gives us easy access\\nto the span’s parent document.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"def get_person_title(span):\\n    if span.label_ == \\\"PERSON\\\" and span.start != 0:\\n        prev_token = span.doc[span.start - 1]\\n        if prev_token.text in (\\\"Dr\\\", \\\"Dr.\\\", \\\"Mr\\\", \\\"Mr.\\\", \\\"Ms\\\", \\\"Ms.\\\"):\\n            return prev_token.text\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"We can now use the \", _jsx(_components.a, {\n          href: \"/api/span#set_extension\",\n          children: _jsx(InlineCode, {\n            children: \"Span.set_extension\"\n          })\n        }), \" method to add\\nthe custom extension attribute \", _jsx(InlineCode, {\n          children: \"\\\"person_title\\\"\"\n        }), \", using \", _jsx(InlineCode, {\n          children: \"get_person_title\"\n        }), \" as the\\ngetter function.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.tokens import Span\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\n\\ndef get_person_title(span):\\n    if span.label_ == \\\"PERSON\\\" and span.start != 0:\\n        prev_token = span.doc[span.start - 1]\\n        if prev_token.text in (\\\"Dr\\\", \\\"Dr.\\\", \\\"Mr\\\", \\\"Mr.\\\", \\\"Ms\\\", \\\"Ms.\\\"):\\n            return prev_token.text\\n\\n# Register the Span extension as 'person_title'\\nSpan.set_extension(\\\"person_title\\\", getter=get_person_title)\\n\\ndoc = nlp(\\\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\\\")\\nprint([(ent.text, ent.label_, ent._.person_title) for ent in doc.ents])\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"models-rules-pos-dep\",\n        children: \"Example: Using entities, part-of-speech tags and the dependency parse \"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Linguistic features\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"This example makes extensive use of part-of-speech tag and dependency\\nattributes and related \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \", \", _jsx(InlineCode, {\n            children: \"Token\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"Span\"\n          }), \" methods. For an introduction\\non this, see the guide on \", _jsx(_components.a, {\n            href: \"/usage/linguistic-features/\",\n            children: \"linguistic features\"\n          }), \".\\nAlso see the label schemes in the \", _jsx(_components.a, {\n            href: \"/models\",\n            children: \"models directory\"\n          }), \" for details on\\nthe labels.\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"Let’s say you want to parse professional biographies and extract the person\\nnames and company names, and whether it’s a company they’re \", _jsx(_components.em, {\n          children: \"currently\"\n        }), \" working\\nat, or a \", _jsx(_components.em, {\n          children: \"previous\"\n        }), \" company. One approach could be to try and train a named\\nentity recognizer to predict \", _jsx(InlineCode, {\n          children: \"CURRENT_ORG\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"PREVIOUS_ORG\"\n        }), \" – but this\\ndistinction is very subtle and something the entity recognizer may struggle to\\nlearn. Nothing about “Acme Corp Inc.” is inherently “current” or “previous”.\"]\n      }), _jsxs(_components.p, {\n        children: [\"However, the syntax of the sentence holds some very important clues: we can\\ncheck for trigger words like “work”, whether they’re \", _jsx(_components.strong, {\n          children: \"past tense\"\n        }), \" or \", _jsx(_components.strong, {\n          children: \"present\\ntense\"\n        }), \", whether company names are attached to it and whether the person is the\\nsubject. All of this information is available in the part-of-speech tags and the\\ndependency parse.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Alex Smith worked at Acme Corp Inc.\\\")\\nprint([(ent.text, ent.label_) for ent in doc.ents])\\n\"\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"nsubj\"\n            }), \": Nominal subject.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"prep\"\n            }), \": Preposition.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"pobj\"\n            }), \": Object of preposition.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"NNP\"\n            }), \": Proper noun, singular.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"VBD\"\n            }), \": Verb, past tense.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"IN\"\n            }), \": Conjunction, subordinating or preposition.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.img, {\n        src: \"/images/displacy-model-rules.svg\",\n        alt: \"Visualization of dependency parse\",\n        title: \"[`spacy.displacy`](/api/top-level#displacy) visualization with `options={'fine_grained': True}` to output the fine-grained part-of-speech tags, i.e. `Token.tag_`\"\n      }), _jsxs(_components.p, {\n        children: [\"In this example, “worked” is the root of the sentence and is a past tense verb.\\nIts subject is “Alex Smith”, the person who worked. “at Acme Corp Inc.” is a\\nprepositional phrase attached to the verb “worked”. To extract this\\nrelationship, we can start by looking at the predicted \", _jsx(InlineCode, {\n          children: \"PERSON\"\n        }), \" entities, find\\ntheir heads and check whether they’re attached to a trigger word like “work”.\\nNext, we can check for prepositional phrases attached to the head and whether\\nthey contain an \", _jsx(InlineCode, {\n          children: \"ORG\"\n        }), \" entity. Finally, to determine whether the company\\naffiliation is current, we can check the head’s part-of-speech tag.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"person_entities = [ent for ent in doc.ents if ent.label_ == \\\"PERSON\\\"]\\nfor ent in person_entities:\\n    # Because the entity is a span, we need to use its root token. The head\\n    # is the syntactic governor of the person, e.g. the verb\\n    head = ent.root.head\\n    if head.lemma_ == \\\"work\\\":\\n        # Check if the children contain a preposition\\n        preps = [token for token in head.children if token.dep_ == \\\"prep\\\"]\\n        for prep in preps:\\n            # Check if tokens part of ORG entities are in the preposition's\\n            # children, e.g. at -\u003e Acme Corp Inc.\\n            orgs = [token for token in prep.children if token.ent_type_ == \\\"ORG\\\"]\\n            # If the verb is in past tense, the company was a previous company\\n            print({\\\"person\\\": ent, \\\"orgs\\\": orgs, \\\"past\\\": head.tag_ == \\\"VBD\\\"})\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"To apply this logic automatically when we process a text, we can add it to the\\n\", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object as a\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines/#custom-components\",\n          children: \"custom pipeline component\"\n        }), \". The\\nabove logic also expects that entities are merged into single tokens. spaCy\\nships with a handy built-in \", _jsx(InlineCode, {\n          children: \"merge_entities\"\n        }), \" that takes care of that. Instead of\\njust printing the result, you could also write it to\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components-attributes\",\n          children: \"custom attributes\"\n        }), \" on\\nthe entity \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" – for example \", _jsx(InlineCode, {\n          children: \"._.orgs\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"._.prev_orgs\"\n        }), \" and\\n\", _jsx(InlineCode, {\n          children: \"._.current_orgs\"\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Merging entities\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"Under the hood, entities are merged using the\\n\", _jsx(_components.a, {\n            href: \"/api/doc#retokenize\",\n            children: _jsx(InlineCode, {\n              children: \"Doc.retokenize\"\n            })\n          }), \" context manager:\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"with doc.retokenize() as retokenizer:\\n  for ent in doc.ents:\\n      retokenizer.merge(ent)\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.language import Language\\nfrom spacy import displacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\n\\n@Language.component(\\\"extract_person_orgs\\\")\\ndef extract_person_orgs(doc):\\n    person_entities = [ent for ent in doc.ents if ent.label_ == \\\"PERSON\\\"]\\n    for ent in person_entities:\\n        head = ent.root.head\\n        if head.lemma_ == \\\"work\\\":\\n            preps = [token for token in head.children if token.dep_ == \\\"prep\\\"]\\n            for prep in preps:\\n                orgs = [token for token in prep.children if token.ent_type_ == \\\"ORG\\\"]\\n                print({'person': ent, 'orgs': orgs, 'past': head.tag_ == \\\"VBD\\\"})\\n    return doc\\n\\n# To make the entities easier to work with, we'll merge them into single tokens\\nnlp.add_pipe(\\\"merge_entities\\\")\\nnlp.add_pipe(\\\"extract_person_orgs\\\")\\n\\ndoc = nlp(\\\"Alex Smith worked at Acme Corp Inc.\\\")\\n# If you're not in a Jupyter / IPython environment, use displacy.serve\\ndisplacy.render(doc, options={\\\"fine_grained\\\": True})\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"If you change the sentence structure above, for example to “was working”, you’ll\\nnotice that our current logic fails and doesn’t correctly detect the company as\\na past organization. That’s because the root is a participle and the tense\\ninformation is in the attached auxiliary “was”:\"\n      }), _jsx(_components.img, {\n        src: \"/images/displacy-model-rules2.svg\",\n        alt: \"Visualization of dependency parse\"\n      }), _jsx(_components.p, {\n        children: \"To solve this, we can adjust the rules to also check for the above construction:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          highlight: \"10-12\",\n          children: \"@Language.component(\\\"extract_person_orgs\\\")\\ndef extract_person_orgs(doc):\\n    person_entities = [ent for ent in doc.ents if ent.label_ == \\\"PERSON\\\"]\\n    for ent in person_entities:\\n        head = ent.root.head\\n        if head.lemma_ == \\\"work\\\":\\n            preps = [token for token in head.children if token.dep_ == \\\"prep\\\"]\\n            for prep in preps:\\n                orgs = [t for t in prep.children if t.ent_type_ == \\\"ORG\\\"]\\n                aux = [token for token in head.children if token.dep_ == \\\"aux\\\"]\\n                past_aux = any(t.tag_ == \\\"VBD\\\" for t in aux)\\n                past = head.tag_ == \\\"VBD\\\" or head.tag_ == \\\"VBG\\\" and past_aux\\n                print({'person': ent, 'orgs': orgs, 'past': past})\\n    return doc\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"In your final rule-based system, you may end up with \", _jsx(_components.strong, {\n          children: \"several different code\\npaths\"\n        }), \" to cover the types of constructions that occur in your data.\"]\n      })]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{"title":"Rule-based matching","teaser":"Find phrases and tokens, and match entities","menu":[["Token Matcher","matcher"],["Phrase Matcher","phrasematcher"],["Dependency Matcher","dependencymatcher"],["Entity Ruler","entityruler"],["Span Ruler","spanruler"],["Models \u0026 Rules","models-rules"]]},"scope":{}},"sectionTitle":"Usage Documentation","theme":"blue","section":"usage","apiDetails":{"stringName":null,"baseClass":null,"trainable":null},"isIndex":false,"next":{"slug":"/usage/processing-pipelines","title":"Processing Pipelines"}},"__N_SSG":true},"page":"/[...listPathPage]","query":{"listPathPage":["usage","rule-based-matching"]},"buildId":"2lY2cUyEfZosk4VLgkHb2","isFallback":false,"dynamicIds":[728,5492,5561],"gsp":true,"scriptLoader":[]}</script></body></html>