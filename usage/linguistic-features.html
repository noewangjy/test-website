<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="sitemap" type="application/xml" href="/sitemap.xml"/><link rel="shortcut icon" href="/icons/icon-192x192.png"/><link rel="manifest" href="/manifest.webmanifest"/><meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=5.0, shrink-to-fit=no, viewport-fit=cover"/><meta name="theme-color" content="#09a3d5"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png"/><title>Linguistic Features · spaCy Usage Documentation</title><meta name="description" content="spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more."/><meta property="og:title" content="Linguistic Features · spaCy Usage Documentation"/><meta property="og:description" content="spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more."/><meta property="og:type" content="website"/><meta property="og:site_name" content="Linguistic Features"/><meta property="og:image" content="https://spacy.io/_next/static/media/social_default.96b04585.jpg"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image" content="https://spacy.io/_next/static/media/social_default.96b04585.jpg"/><meta name="twitter:creator" content="@spacy_io"/><meta name="twitter:site" content="@spacy_io"/><meta name="twitter:title" content="Linguistic Features · spaCy Usage Documentation"/><meta name="twitter:description" content="spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more."/><meta name="docsearch:language" content="en"/><meta name="next-head-count" content="24"/><link rel="preload" href="/_next/static/css/8f0b94edbc18d62d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8f0b94edbc18d62d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e6995e0e8addcf99.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e6995e0e8addcf99.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script defer="" src="/_next/static/chunks/262.c647d33d06232ef6.js"></script><script defer="" src="/_next/static/chunks/728.cf6ba0da2700fa1b.js"></script><script defer="" src="/_next/static/chunks/4ad82c5e.9f71e347f3d5ee0a.js"></script><script defer="" src="/_next/static/chunks/fec483df.e5c4c2e1905c02db.js"></script><script defer="" src="/_next/static/chunks/d9c63220.b8e7d78d95edcd53.js"></script><script defer="" src="/_next/static/chunks/876.15142163f03ee62c.js"></script><script defer="" src="/_next/static/chunks/661.267b5ccf1a1d90d0.js"></script><script defer="" src="/_next/static/chunks/492.0a716e87ad804aae.js"></script><script src="/_next/static/chunks/webpack-8161fc2bb14cec39.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-a0f603ce323043fd.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eb3ea1261af64e73.js" defer=""></script><script src="/_next/static/chunks/94-57434c8b7a6c3878.js" defer=""></script><script src="/_next/static/chunks/128-76b45627a109219b.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...listPathPage%5D-45eea57fe8c2902c.js" defer=""></script><script src="/_next/static/Ugre-usgT1EZhnSeYcBR9/_buildManifest.js" defer=""></script><script src="/_next/static/Ugre-usgT1EZhnSeYcBR9/_ssgManifest.js" defer=""></script></head><body class="theme-blue"><div id="__next"><div class="theme-blue"><nav class="navigation_root__yPL8O"><span class="navigation_has-alert__s0Drf"><a class="link_root__1Me7D link_no-link-layout__RPvod" aria-label="spaCy" href="/"><h1 class="navigation_title__pm49s">spaCy</h1></a> <span class="navigation_alert__ZOXon"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/usage/v3-5"><strong>💥 Out now:</strong> spaCy v3.5</a></span></span><div class="navigation_menu__ZMJxN"><select class="dropdown_root__3uiQq navigation_dropdown__4j4pI"><option value="title" disabled="">Menu</option><option value="/usage" selected="">Usage</option><option value="/models">Models</option><option value="/api">API</option><option value="/universe">Universe</option></select><ul class="navigation_list__DCzqi"><li class="navigation_item__ln1O1 navigation_is-active__RjVJG"><a class="link_root__1Me7D link_no-link-layout__RPvod" tabindex="-1" href="/usage">Usage</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/models">Models</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/api">API</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/universe">Universe</a></li><li class="navigation_item__ln1O1 navigation_github__MpFNv"><span><a href="https://github.com/explosion/spaCy" data-size="large" data-show-count="true" aria-label="Star spaCy on GitHub"></a></span></li></ul><div class="navigation_search__BKZCn"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div><progress class="progress_root__9huWN" value="0" max="100"></progress></nav><menu class="sidebar sidebar_root__s2No7"><h1 hidden="" aria-hidden="true" class="h0 sidebar_active-heading___dkf9">Guides</h1><div class="sidebar_dropdown__vyqjz"><select class="dropdown_root__3uiQq sidebar_dropdown-select__Nwbq9"><option disabled="">Select page...</option><option value="/usage">Get started<!-- --> › <!-- -->Installation</option><option value="/usage/models">Get started<!-- --> › <!-- -->Models &amp; Languages</option><option value="/usage/facts-figures">Get started<!-- --> › <!-- -->Facts &amp; Figures</option><option value="/usage/spacy-101">Get started<!-- --> › <!-- -->spaCy 101</option><option value="/usage/v3">Get started<!-- --> › <!-- -->New in v3.0</option><option value="/usage/v3-1">Get started<!-- --> › <!-- -->New in v3.1</option><option value="/usage/v3-2">Get started<!-- --> › <!-- -->New in v3.2</option><option value="/usage/v3-3">Get started<!-- --> › <!-- -->New in v3.3</option><option value="/usage/v3-4">Get started<!-- --> › <!-- -->New in v3.4</option><option value="/usage/v3-5">Get started<!-- --> › <!-- -->New in v3.5</option><option value="/usage/linguistic-features" selected="">Guides<!-- --> › <!-- -->Linguistic Features</option><option value="/usage/rule-based-matching">Guides<!-- --> › <!-- -->Rule-based Matching</option><option value="/usage/processing-pipelines">Guides<!-- --> › <!-- -->Processing Pipelines</option><option value="/usage/embeddings-transformers">Guides<!-- --> › <!-- -->Embeddings &amp; Transformers</option><option value="/usage/training">Guides<!-- --> › <!-- -->Training Models</option><option value="/usage/layers-architectures">Guides<!-- --> › <!-- -->Layers &amp; Model Architectures</option><option value="/usage/projects">Guides<!-- --> › <!-- -->spaCy Projects</option><option value="/usage/saving-loading">Guides<!-- --> › <!-- -->Saving &amp; Loading</option><option value="/usage/visualizers">Guides<!-- --> › <!-- -->Visualizers</option><option value="https://github.com/explosion/projects">Resources<!-- --> › <!-- -->Project Templates</option><option value="https://v2.spacy.io">Resources<!-- --> › <!-- -->v2.x Documentation</option><option value="https://explosion.ai/custom-solutions">Resources<!-- --> › <!-- -->Custom Solutions</option></select></div><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Get started</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage">Installation</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/models">Models &amp; Languages</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/facts-figures">Facts &amp; Figures</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/spacy-101">spaCy 101</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3">New in v3.0</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-1">New in v3.1</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-2">New in v3.2</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-3">New in v3.3</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-4">New in v3.4</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-5">New in v3.5</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Guides</li><li><a class="link_root__1Me7D sidebar_link__sKXFP sidebar_is-active__yVTtL is-active" href="/usage/linguistic-features">Linguistic Features</a><ul class="sidebar_crumbs__NhM2y"><li class="sidebar_crumb__tiiDl sidebar_crumb-active__zq8BI"><a href="#pos-tagging">POS Tagging</a></li><li class="sidebar_crumb__tiiDl"><a href="#morphology">Morphology</a></li><li class="sidebar_crumb__tiiDl"><a href="#lemmatization">Lemmatization</a></li><li class="sidebar_crumb__tiiDl"><a href="#dependency-parse">Dependency Parse</a></li><li class="sidebar_crumb__tiiDl"><a href="#named-entities">Named Entities</a></li><li class="sidebar_crumb__tiiDl"><a href="#entity-linking">Entity Linking</a></li><li class="sidebar_crumb__tiiDl"><a href="#tokenization">Tokenization</a></li><li class="sidebar_crumb__tiiDl"><a href="#retokenization">Merging &amp; Splitting</a></li><li class="sidebar_crumb__tiiDl"><a href="#sbd">Sentence Segmentation</a></li><li class="sidebar_crumb__tiiDl"><a href="#mappings-exceptions">Mappings &amp; Exceptions</a></li><li class="sidebar_crumb__tiiDl"><a href="#vectors-similarity">Vectors &amp; Similarity</a></li><li class="sidebar_crumb__tiiDl"><a href="#language-data">Language Data</a></li></ul></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/rule-based-matching">Rule-based Matching</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/processing-pipelines">Processing Pipelines</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/embeddings-transformers">Embeddings &amp; Transformers<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/training">Training Models<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/layers-architectures">Layers &amp; Model Architectures<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/projects">spaCy Projects<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/saving-loading">Saving &amp; Loading</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/visualizers">Visualizers</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Resources</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/projects">Project Templates</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="https://v2.spacy.io">v2.x Documentation</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="https://explosion.ai/custom-solutions">Custom Solutions</a></li></ul></menu><main class="main_root__7f6Tj main_with-sidebar__uH1df main_with-asides__ikQT6"><article class="main_content__8zFCH"><header class="title_root__pS2WQ"><h1 id="_title" class="typography_heading__D82WZ typography_h1__b7dt9 title_h1__l3CW1"><span class="heading-text">Linguistic Features<!-- --> </span></h1></header><section class="section_root__k1hUl"><p>Processing raw text intelligently is difficult: most words are rare, and it’s
common for words that look completely different to mean almost the same thing.
The same words in a different order can mean something completely different.
Even splitting text into useful word-like units can be difficult in many
languages. While it’s possible to solve some problems starting from only the raw
characters, it’s usually better to use linguistic knowledge to add useful
information. That’s exactly what spaCy is designed to do: you put in raw text,
and get back a <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> object, that comes with a variety of
annotations.</p></section>
<section id="section-pos-tagging" class="section_root__k1hUl"><h2 id="pos-tagging" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#pos-tagging" class="heading-text typography_permalink__UiIRy">Part-of-speech tagging <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="To use this functionality, spaCy needs a trained pipeline that supports the following capabilities: tagger, parser">Needs model</span></h2><p>After tokenization, spaCy can <strong>parse</strong> and <strong>tag</strong> a given <code class="code_inline-code__Bq7ot">Doc</code>. This is where
the trained pipeline and its statistical models come in, which enable spaCy to
<strong>make predictions</strong> of which tag or label most likely applies in this context.
A trained component includes binary data that is produced by showing a system
enough examples for it to make predictions that generalize across the language –
for example, a word following “the” in English is most likely a noun.</p>
<p>Linguistic annotations are available as
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#attributes"><code class="code_inline-code__Bq7ot">Token</code> attributes</a>. Like many NLP libraries, spaCy
<strong>encodes all strings to hash values</strong> to reduce memory usage and improve
efficiency. So to get the readable string representation of an attribute, we
need to add an underscore <code class="code_inline-code__Bq7ot">_</code> to its name:</p>
<pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre>
<aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><strong>Text:</strong> The original word text.</li>
<li class="list_li__sfx_z"><strong>Lemma:</strong> The base form of the word.</li>
<li class="list_li__sfx_z"><strong>POS:</strong> The simple <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://universaldependencies.org/u/pos/">UPOS</a>
part-of-speech tag.</li>
<li class="list_li__sfx_z"><strong>Tag:</strong> The detailed part-of-speech tag.</li>
<li class="list_li__sfx_z"><strong>Dep:</strong> Syntactic dependency, i.e. the relation between tokens.</li>
<li class="list_li__sfx_z"><strong>Shape:</strong> The word shape – capitalization, punctuation, digits.</li>
<li class="list_li__sfx_z"><strong>is alpha:</strong> Is the token an alpha character?</li>
<li class="list_li__sfx_z"><strong>is stop:</strong> Is the token part of a stop list, i.e. the most common words of
the language?</li>
</ul>
</div></div></aside>
<table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Text</th><th class="table_th__QJ9F8">Lemma</th><th class="table_th__QJ9F8">POS</th><th class="table_th__QJ9F8">Tag</th><th class="table_th__QJ9F8">Dep</th><th class="table_th__QJ9F8">Shape</th><th class="table_th__QJ9F8">alpha</th><th class="table_th__QJ9F8">stop</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">Apple</td><td class="table_td__rmpJx">apple</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">PROPN</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">NNP</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">nsubj</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">Xxxxx</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">True</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">False</code></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">is</td><td class="table_td__rmpJx">be</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">AUX</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VBZ</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">aux</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">xx</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">True</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">True</code></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">looking</td><td class="table_td__rmpJx">look</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VERB</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VBG</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">ROOT</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">xxxx</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">True</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">False</code></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">at</td><td class="table_td__rmpJx">at</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">ADP</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">IN</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">prep</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">xx</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">True</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">True</code></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">buying</td><td class="table_td__rmpJx">buy</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VERB</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VBG</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">pcomp</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">xxxx</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">True</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">False</code></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">U.K.</td><td class="table_td__rmpJx">u.k.</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">PROPN</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">NNP</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">compound</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">X.X.</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">False</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">False</code></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">startup</td><td class="table_td__rmpJx">startup</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">NOUN</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">NN</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">dobj</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">xxxx</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">True</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">False</code></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">for</td><td class="table_td__rmpJx">for</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">ADP</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">IN</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">prep</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">xxx</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">True</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">True</code></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">$</td><td class="table_td__rmpJx">$</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">SYM</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">$</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">quantmod</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">$</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">False</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">False</code></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">1</td><td class="table_td__rmpJx">1</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">NUM</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">CD</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">compound</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">d</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">False</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">False</code></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">billion</td><td class="table_td__rmpJx">billion</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">NUM</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">CD</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">pobj</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">xxxx</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">True</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">False</code></td></tr></tbody></table>
<aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Tip: Understanding tags and labels<!-- --> </span></h4>
<p>Most of the tags and labels look pretty abstract, and they vary between
languages. <code class="code_inline-code__Bq7ot">spacy.explain</code> will show you a short description – for example,
<code class="code_inline-code__Bq7ot">spacy.explain(&quot;VBZ&quot;)</code> returns “verb, 3rd person singular present”.</p>
</div></div></aside>
<p>Using spaCy’s built-in <a class="link_root__1Me7D" href="/usage/visualizers">displaCy visualizer</a>, here’s what
our example sentence and its dependencies look like:</p>
<iframe class="embed_standalone__RHbIL" title="displaCy visualization of dependencies and entities" src="/images/displacy-long.html" width="800" height="450" allowfullscreen="" frameBorder="0"></iframe><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">📖</span>Part-of-speech tag scheme</span></h4><p>For a list of the fine-grained and coarse-grained part-of-speech tags assigned
by spaCy’s models across different languages, see the label schemes documented
in the <a class="link_root__1Me7D" href="/models">models directory</a>.</p></aside></section>
<section id="section-morphology" class="section_root__k1hUl"><h2 id="morphology" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#morphology" class="heading-text typography_permalink__UiIRy">Morphology <!-- --> </a></h2><p>Inflectional morphology is the process by which a root form of a word is
modified by adding prefixes or suffixes that specify its grammatical function
but do not change its part-of-speech. We say that a <strong>lemma</strong> (root form) is
<strong>inflected</strong> (modified/combined) with one or more <strong>morphological features</strong> to
create a surface form. Here are some examples:</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Context</th><th class="table_th__QJ9F8">Surface</th><th class="table_th__QJ9F8">Lemma</th><th class="table_th__QJ9F8">POS</th><th class="table_th__QJ9F8">Morphological Features</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">I was reading the paper</td><td class="table_td__rmpJx">reading</td><td class="table_td__rmpJx">read</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VERB</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VerbForm=Ger</code></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">I don’t watch the news, I read the paper</td><td class="table_td__rmpJx">read</td><td class="table_td__rmpJx">read</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VERB</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VerbForm=Fin</code>, <code class="code_inline-code__Bq7ot">Mood=Ind</code>, <code class="code_inline-code__Bq7ot">Tense=Pres</code></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">I read the paper yesterday</td><td class="table_td__rmpJx">read</td><td class="table_td__rmpJx">read</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VERB</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VerbForm=Fin</code>, <code class="code_inline-code__Bq7ot">Mood=Ind</code>, <code class="code_inline-code__Bq7ot">Tense=Past</code></td></tr></tbody></table><p>Morphological features are stored in the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/morphology#morphanalysis"><code class="code_inline-code__Bq7ot">MorphAnalysis</code></a> under <code class="code_inline-code__Bq7ot">Token.morph</code>, which
allows you to access individual morphological features.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">📝 Things to try<!-- --> </span></h4>
<ol class="list_ol__aclSa">
<li class="list_li__sfx_z">Change “I” to “She”. You should see that the morphological features change
and express that it’s a pronoun in the third person.</li>
<li class="list_li__sfx_z">Inspect <code class="code_inline-code__Bq7ot">token.morph</code> for the other tokens.</li>
</ol>
</div></div></aside><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h3 id="morphologizer" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#morphologizer" class="heading-text typography_permalink__UiIRy">Statistical morphology <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="This feature is new and was introduced in spaCy v3.0">v<!-- -->3.0</span><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="To use this functionality, spaCy needs a trained pipeline that supports the following capabilities: morphologizer">Needs model</span></h3><p>spaCy’s statistical <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/morphologizer"><code class="code_inline-code__Bq7ot">Morphologizer</code></a> component assigns the
morphological features and coarse-grained part-of-speech tags as <code class="code_inline-code__Bq7ot">Token.morph</code>
and <code class="code_inline-code__Bq7ot">Token.pos</code>.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h3 id="rule-based-morphology" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#rule-based-morphology" class="heading-text typography_permalink__UiIRy">Rule-based morphology <!-- --> </a></h3><p>For languages with relatively simple morphological systems like English, spaCy
can assign morphological features through a rule-based approach, which uses the
<strong>token text</strong> and <strong>fine-grained part-of-speech tags</strong> to produce
coarse-grained part-of-speech tags and morphological features.</p><ol class="list_ol__aclSa">
<li class="list_li__sfx_z">The part-of-speech tagger assigns each token a <strong>fine-grained part-of-speech
tag</strong>. In the API, these tags are known as <code class="code_inline-code__Bq7ot">Token.tag</code>. They express the
part-of-speech (e.g. verb) and some amount of morphological information, e.g.
that the verb is past tense (e.g. <code class="code_inline-code__Bq7ot">VBD</code> for a past tense verb in the Penn
Treebank) .</li>
<li class="list_li__sfx_z">For words whose coarse-grained POS is not set by a prior process, a
<a class="link_root__1Me7D" href="/usage/linguistic-features#mappings-exceptions">mapping table</a> maps the fine-grained tags to a
coarse-grained POS tags and morphological features.</li>
</ol><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre></section>
<section id="section-lemmatization" class="section_root__k1hUl"><h2 id="lemmatization" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#lemmatization" class="heading-text typography_permalink__UiIRy">Lemmatization <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="This feature is new and was introduced in spaCy v3.0">v<!-- -->3.0</span><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="To use this functionality, spaCy needs a trained pipeline that supports the following capabilities: lemmatizer">Needs model</span></h2><p>spaCy provides two pipeline components for lemmatization:</p><ol class="list_ol__aclSa">
<li class="list_li__sfx_z">The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/lemmatizer"><code class="code_inline-code__Bq7ot">Lemmatizer</code></a> component provides lookup and rule-based
lemmatization methods in a configurable component. An individual language can
extend the <code class="code_inline-code__Bq7ot">Lemmatizer</code> as part of its <a class="link_root__1Me7D" href="/usage/linguistic-features#language-data">language data</a>.</li>
<li class="list_li__sfx_z">The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/edittreelemmatizer"><code class="code_inline-code__Bq7ot">EditTreeLemmatizer</code></a>
<span class="tag_root__NTSnK" data-tooltip="This feature is new and was introduced in spaCy v3.3">v<!-- -->3.3</span> component provides a trainable lemmatizer.</li>
</ol><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Changed in v3.0</span></h4><p>Unlike spaCy v2, spaCy v3 models do <em>not</em> provide lemmas by default or switch
automatically between lookup and rule-based lemmas depending on whether a tagger
is in the pipeline. To have lemmas in a <code class="code_inline-code__Bq7ot">Doc</code>, the pipeline needs to include a
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/lemmatizer"><code class="code_inline-code__Bq7ot">Lemmatizer</code></a> component. The lemmatizer component is
configured to use a single mode such as <code class="code_inline-code__Bq7ot">&quot;lookup&quot;</code> or <code class="code_inline-code__Bq7ot">&quot;rule&quot;</code> on
initialization. The <code class="code_inline-code__Bq7ot">&quot;rule&quot;</code> mode requires <code class="code_inline-code__Bq7ot">Token.pos</code> to be set by a previous
component.</p></aside><p>The data for spaCy’s lemmatizers is distributed in the package
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spacy-lookups-data"><code class="code_inline-code__Bq7ot">spacy-lookups-data</code></a>. The
provided trained pipelines already include all the required tables, but if you
are creating new pipelines, you’ll probably want to install <code class="code_inline-code__Bq7ot">spacy-lookups-data</code>
to provide the data when the lemmatizer is initialized.</p><h3 id="lemmatizer-lookup" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#lemmatizer-lookup" class="heading-text typography_permalink__UiIRy">Lookup lemmatizer <!-- --> </a></h3><p>For pipelines without a tagger or morphologizer, a lookup lemmatizer can be
added to the pipeline as long as a lookup table is provided, typically through
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spacy-lookups-data"><code class="code_inline-code__Bq7ot">spacy-lookups-data</code></a>. The
lookup lemmatizer looks up the token surface form in the lookup table without
reference to the token’s part-of-speech or context.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><h3 id="lemmatizer-rule" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#lemmatizer-rule" class="heading-text typography_permalink__UiIRy">Rule-based lemmatizer <!-- --> </a></h3><p>When training pipelines that include a component that assigns part-of-speech
tags (a morphologizer or a tagger with a <a class="link_root__1Me7D" href="/usage/linguistic-features#mappings-exceptions">POS mapping</a>), a
rule-based lemmatizer can be added using rule tables from
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spacy-lookups-data"><code class="code_inline-code__Bq7ot">spacy-lookups-data</code></a>:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>The rule-based deterministic lemmatizer maps the surface form to a lemma in
light of the previously assigned coarse-grained part-of-speech and morphological
information, without consulting the context of the token. The rule-based
lemmatizer also accepts list-based exception files. For English, these are
acquired from <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://wordnet.princeton.edu/">WordNet</a>.</p><h3 class="typography_heading__D82WZ typography_h3__mPKmB"><span class="heading-text">Trainable lemmatizer<!-- --> </span></h3><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/edittreelemmatizer"><code class="code_inline-code__Bq7ot">EditTreeLemmatizer</code></a> can learn form-to-lemma
transformations from a training corpus that includes lemma annotations. This
removes the need to write language-specific rules and can (in many cases)
provide higher accuracies than lookup and rule-based lemmatizers.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre></section>
<section id="section-dependency-parse" class="section_root__k1hUl"><h2 id="dependency-parse" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#dependency-parse" class="heading-text typography_permalink__UiIRy">Dependency Parsing <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="To use this functionality, spaCy needs a trained pipeline that supports the following capabilities: parser">Needs model</span></h2><p>spaCy features a fast and accurate syntactic dependency parser, and has a rich
API for navigating the tree. The parser also powers the sentence boundary
detection, and lets you iterate over base noun phrases, or “chunks”. You can
check whether a <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> object has been parsed by calling
<code class="code_inline-code__Bq7ot">doc.has_annotation(&quot;DEP&quot;)</code>, which checks whether the attribute <code class="code_inline-code__Bq7ot">Token.dep</code> has
been set returns a boolean value. If the result is <code class="code_inline-code__Bq7ot">False</code>, the default sentence
iterator will raise an exception.</p><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">📖</span>Dependency label scheme</span></h4><p>For a list of the syntactic dependency labels assigned by spaCy’s models across
different languages, see the label schemes documented in the
<a class="link_root__1Me7D" href="/models">models directory</a>.</p></aside><h3 id="noun-chunks" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#noun-chunks" class="heading-text typography_permalink__UiIRy">Noun chunks <!-- --> </a></h3><p>Noun chunks are “base noun phrases” – flat phrases that have a noun as their
head. You can think of noun chunks as a noun plus the words describing the noun
– for example, “the lavish green grass” or “the world’s largest tech fund”. To
get the noun chunks in a document, simply iterate over
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc#noun_chunks"><code class="code_inline-code__Bq7ot">Doc.noun_chunks</code></a>.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><strong>Text:</strong> The original noun chunk text.</li>
<li class="list_li__sfx_z"><strong>Root text:</strong> The original text of the word connecting the noun chunk to
the rest of the parse.</li>
<li class="list_li__sfx_z"><strong>Root dep:</strong> Dependency relation connecting the root to its head.</li>
<li class="list_li__sfx_z"><strong>Root head text:</strong> The text of the root token’s head.</li>
</ul>
</div></div></aside><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Text</th><th class="table_th__QJ9F8">root.text</th><th class="table_th__QJ9F8">root.dep_</th><th class="table_th__QJ9F8">root.head.text</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">Autonomous cars</td><td class="table_td__rmpJx">cars</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">nsubj</code></td><td class="table_td__rmpJx">shift</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">insurance liability</td><td class="table_td__rmpJx">liability</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">dobj</code></td><td class="table_td__rmpJx">shift</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">manufacturers</td><td class="table_td__rmpJx">manufacturers</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">pobj</code></td><td class="table_td__rmpJx">toward</td></tr></tbody></table><h3 id="navigating" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#navigating" class="heading-text typography_permalink__UiIRy">Navigating the parse tree <!-- --> </a></h3><p>spaCy uses the terms <strong>head</strong> and <strong>child</strong> to describe the words <strong>connected by
a single arc</strong> in the dependency tree. The term <strong>dep</strong> is used for the arc
label, which describes the type of syntactic relation that connects the child to
the head. As with other attributes, the value of <code class="code_inline-code__Bq7ot">.dep</code> is a hash value. You can
get the string value with <code class="code_inline-code__Bq7ot">.dep_</code>.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><strong>Text:</strong> The original token text.</li>
<li class="list_li__sfx_z"><strong>Dep:</strong> The syntactic relation connecting child to head.</li>
<li class="list_li__sfx_z"><strong>Head text:</strong> The original text of the token head.</li>
<li class="list_li__sfx_z"><strong>Head POS:</strong> The part-of-speech tag of the token head.</li>
<li class="list_li__sfx_z"><strong>Children:</strong> The immediate syntactic dependents of the token.</li>
</ul>
</div></div></aside><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Text</th><th class="table_th__QJ9F8">Dep</th><th class="table_th__QJ9F8">Head text</th><th class="table_th__QJ9F8">Head POS</th><th class="table_th__QJ9F8">Children</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">Autonomous</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">amod</code></td><td class="table_td__rmpJx">cars</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">NOUN</code></td><td class="table_td__rmpJx"></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">cars</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">nsubj</code></td><td class="table_td__rmpJx">shift</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VERB</code></td><td class="table_td__rmpJx">Autonomous</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">shift</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">ROOT</code></td><td class="table_td__rmpJx">shift</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VERB</code></td><td class="table_td__rmpJx">cars, liability, toward</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">insurance</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">compound</code></td><td class="table_td__rmpJx">liability</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">NOUN</code></td><td class="table_td__rmpJx"></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">liability</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">dobj</code></td><td class="table_td__rmpJx">shift</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VERB</code></td><td class="table_td__rmpJx">insurance</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">toward</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">prep</code></td><td class="table_td__rmpJx">shift</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">NOUN</code></td><td class="table_td__rmpJx">manufacturers</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">manufacturers</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">pobj</code></td><td class="table_td__rmpJx">toward</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">ADP</code></td><td class="table_td__rmpJx"></td></tr></tbody></table><iframe class="embed_standalone__RHbIL" title="displaCy visualization of dependencies and entities 2" src="/images/displacy-long2.html" width="800" height="450" allowfullscreen="" frameBorder="0"></iframe><p>Because the syntactic relations form a tree, every word has <strong>exactly one
head</strong>. You can therefore iterate over the arcs in the tree by iterating over
the words in the sentence. This is usually the best way to match an arc of
interest – from below:</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>If you try to match from above, you’ll have to iterate twice. Once for the head,
and then again through the children:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>To iterate through the children, use the <code class="code_inline-code__Bq7ot">token.children</code> attribute, which
provides a sequence of <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token"><code class="code_inline-code__Bq7ot">Token</code></a> objects.</p><h4 id="navigating-around" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#navigating-around" class="heading-text typography_permalink__UiIRy">Iterating around the local tree <!-- --> </a></h4><p>A few more convenience attributes are provided for iterating around the local
tree from the token. <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#lefts"><code class="code_inline-code__Bq7ot">Token.lefts</code></a> and
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#rights"><code class="code_inline-code__Bq7ot">Token.rights</code></a> attributes provide sequences of syntactic
children that occur before and after the token. Both sequences are in sentence
order. There are also two integer-typed attributes,
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#n_lefts"><code class="code_inline-code__Bq7ot">Token.n_lefts</code></a> and
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#n_rights"><code class="code_inline-code__Bq7ot">Token.n_rights</code></a> that give the number of left and right
children.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>You can get a whole phrase by its syntactic head using the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#subtree"><code class="code_inline-code__Bq7ot">Token.subtree</code></a> attribute. This returns an ordered
sequence of tokens. You can walk up the tree with the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#ancestors"><code class="code_inline-code__Bq7ot">Token.ancestors</code></a> attribute, and check dominance with
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#is_ancestor"><code class="code_inline-code__Bq7ot">Token.is_ancestor</code></a></p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Projective vs. non-projective<!-- --> </span></h4>
<p>For the <a class="link_root__1Me7D link_with-icon__NAVDA" href="/models/en"><span class="link_source-text__VDP74">default English pipelines</span></a>, the parse tree is
<strong>projective</strong>, which means that there are no crossing brackets. The tokens
returned by <code class="code_inline-code__Bq7ot">.subtree</code> are therefore guaranteed to be contiguous. This is not
true for the German pipelines, which have many
<a class="link_root__1Me7D" href="https://explosion.ai/blog/german-model#word-order">non-projective dependencies</a>.</p>
</div></div></aside><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Text</th><th class="table_th__QJ9F8">Dep</th><th class="table_th__QJ9F8">n_lefts</th><th class="table_th__QJ9F8">n_rights</th><th class="table_th__QJ9F8">ancestors</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">Credit</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">nmod</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">0</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">2</code></td><td class="table_td__rmpJx">holders, submit</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">and</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">cc</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">0</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">0</code></td><td class="table_td__rmpJx">holders, submit</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">mortgage</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">compound</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">0</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">0</code></td><td class="table_td__rmpJx">account, Credit, holders, submit</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">account</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">conj</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">1</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">0</code></td><td class="table_td__rmpJx">Credit, holders, submit</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">holders</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">nsubj</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">1</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">0</code></td><td class="table_td__rmpJx">submit</td></tr></tbody></table><p>Finally, the <code class="code_inline-code__Bq7ot">.left_edge</code> and <code class="code_inline-code__Bq7ot">.right_edge</code> attributes can be especially useful,
because they give you the first and last token of the subtree. This is the
easiest way to create a <code class="code_inline-code__Bq7ot">Span</code> object for a syntactic phrase. Note that
<code class="code_inline-code__Bq7ot">.right_edge</code> gives a token <strong>within</strong> the subtree – so if you use it as the
end-point of a range, don’t forget to <code class="code_inline-code__Bq7ot">+1</code>!</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Text</th><th class="table_th__QJ9F8">POS</th><th class="table_th__QJ9F8">Dep</th><th class="table_th__QJ9F8">Head text</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">Credit and mortgage account holders</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">NOUN</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">nsubj</code></td><td class="table_td__rmpJx">submit</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">must</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VERB</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">aux</code></td><td class="table_td__rmpJx">submit</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">submit</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">VERB</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">ROOT</code></td><td class="table_td__rmpJx">submit</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">their</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">ADJ</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">poss</code></td><td class="table_td__rmpJx">requests</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">requests</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">NOUN</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">dobj</code></td><td class="table_td__rmpJx">submit</td></tr></tbody></table><p>The dependency parse can be a useful tool for <strong>information extraction</strong>,
especially when combined with other predictions like
<a class="link_root__1Me7D" href="/usage/linguistic-features#named-entities">named entities</a>. The following example extracts money and
currency values, i.e. entities labeled as <code class="code_inline-code__Bq7ot">MONEY</code>, and then uses the dependency
parse to find the noun phrase they are referring to – for example <code class="code_inline-code__Bq7ot">&quot;Net income&quot;</code>
→ <code class="code_inline-code__Bq7ot">&quot;$9.4 million&quot;</code>.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">📖</span>Combining models and rules</span></h4><p>For more examples of how to write rule-based information extraction logic that
takes advantage of the model’s predictions produced by the different components,
see the usage guide on
<a class="link_root__1Me7D" href="/usage/rule-based-matching#models-rules">combining models and rules</a>.</p></aside><h3 id="displacy" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#displacy" class="heading-text typography_permalink__UiIRy">Visualizing dependencies <!-- --> </a></h3><p>The best way to understand spaCy’s dependency parser is interactively. To make
this easier, spaCy comes with a visualization module. You can pass a <code class="code_inline-code__Bq7ot">Doc</code> or a
list of <code class="code_inline-code__Bq7ot">Doc</code> objects to displaCy and run
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#displacy.serve"><code class="code_inline-code__Bq7ot">displacy.serve</code></a> to run the web server, or
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#displacy.render"><code class="code_inline-code__Bq7ot">displacy.render</code></a> to generate the raw markup.
If you want to know how to write rules that hook into some type of syntactic
construction, just plug the sentence into the visualizer and see how spaCy
annotates it.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><aside class="infobox_root__yNIMg"><p>For more details and examples, see the
<a class="link_root__1Me7D" href="/usage/visualizers">usage guide on visualizing spaCy</a>. You can also test
displaCy in our <a class="link_root__1Me7D" href="https://explosion.ai/demos/displacy">online demo</a>..</p></aside><h3 id="disabling" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#disabling" class="heading-text typography_permalink__UiIRy">Disabling the parser <!-- --> </a></h3><p>In the <a class="link_root__1Me7D" href="/models">trained pipelines</a> provided by spaCy, the parser is loaded and
enabled by default as part of the
<a class="link_root__1Me7D" href="/usage/processing-pipelines">standard processing pipeline</a>. If you don’t need
any of the syntactic information, you should disable the parser. Disabling the
parser will make spaCy load and run much faster. If you want to load the parser,
but need to disable it for specific documents, you can also control its use on
the <code class="code_inline-code__Bq7ot">nlp</code> object. For more details, see the usage guide on
<a class="link_root__1Me7D" href="/usage/processing-pipelines#disabling">disabling pipeline components</a>.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre></section>
<section id="section-named-entities" class="section_root__k1hUl"><h2 id="named-entities" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#named-entities" class="heading-text typography_permalink__UiIRy">Named Entity Recognition <!-- --> </a></h2><p>spaCy features an extremely fast statistical entity recognition system, that
assigns labels to contiguous spans of tokens. The default
<a class="link_root__1Me7D" href="/models">trained pipelines</a> can identify a variety of named and numeric
entities, including companies, locations, organizations and products. You can
add arbitrary classes to the entity recognition system, and update the model
with new examples.</p><h3 id="named-entities-101" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#named-entities-101" class="heading-text typography_permalink__UiIRy">Named Entity Recognition 101 <!-- --> </a></h3><p>A named entity is a “real-world object” that’s assigned a name – for example, a
person, a country, a product or a book title. spaCy can <strong>recognize various
types of named entities in a document, by asking the model for a prediction</strong>.
Because models are statistical and strongly depend on the examples they were
trained on, this doesn’t always work <em>perfectly</em> and might need some tuning
later, depending on your use case.</p>
<p>Named entities are available as the <code class="code_inline-code__Bq7ot">ents</code> property of a <code class="code_inline-code__Bq7ot">Doc</code>:</p>
<pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre>
<aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><strong>Text:</strong> The original entity text.</li>
<li class="list_li__sfx_z"><strong>Start:</strong> Index of start of entity in the <code class="code_inline-code__Bq7ot">Doc</code>.</li>
<li class="list_li__sfx_z"><strong>End:</strong> Index of end of entity in the <code class="code_inline-code__Bq7ot">Doc</code>.</li>
<li class="list_li__sfx_z"><strong>Label:</strong> Entity label, i.e. type.</li>
</ul>
</div></div></aside>
<table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Text</th><th class="table_th__QJ9F8" align="center">Start</th><th class="table_th__QJ9F8" align="center">End</th><th class="table_th__QJ9F8">Label</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">Apple</td><td class="table_td__rmpJx" align="center">0</td><td class="table_td__rmpJx" align="center">5</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">ORG</code></td><td class="table_td__rmpJx">Companies, agencies, institutions.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">U.K.</td><td class="table_td__rmpJx table_num__mTxMB" align="center">27</td><td class="table_td__rmpJx table_num__mTxMB" align="center">31</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">GPE</code></td><td class="table_td__rmpJx">Geopolitical entity, i.e. countries, cities, states.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">$1 billion</td><td class="table_td__rmpJx table_num__mTxMB" align="center">44</td><td class="table_td__rmpJx table_num__mTxMB" align="center">54</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">MONEY</code></td><td class="table_td__rmpJx">Monetary values, including unit.</td></tr></tbody></table>
<p>Using spaCy’s built-in <a class="link_root__1Me7D" href="/usage/visualizers">displaCy visualizer</a>, here’s what
our example sentence and its named entities look like:</p>
<iframe class="embed_standalone__RHbIL" title="displaCy visualization of entities" src="/images/displacy-ent1.html" width="800" height="100" allowfullscreen="" frameBorder="0"></iframe><h3 id="accessing-ner" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#accessing-ner" class="heading-text typography_permalink__UiIRy">Accessing entity annotations and labels <!-- --> </a></h3><p>The standard way to access entity annotations is the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc#ents"><code class="code_inline-code__Bq7ot">doc.ents</code></a>
property, which produces a sequence of <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/span"><code class="code_inline-code__Bq7ot">Span</code></a> objects. The entity
type is accessible either as a hash value or as a string, using the attributes
<code class="code_inline-code__Bq7ot">ent.label</code> and <code class="code_inline-code__Bq7ot">ent.label_</code>. The <code class="code_inline-code__Bq7ot">Span</code> object acts as a sequence of tokens, so
you can iterate over the entity or index into it. You can also get the text form
of the whole entity, as though it were a single token.</p><p>You can also access token entity annotations using the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#attributes"><code class="code_inline-code__Bq7ot">token.ent_iob</code></a> and
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#attributes"><code class="code_inline-code__Bq7ot">token.ent_type</code></a> attributes. <code class="code_inline-code__Bq7ot">token.ent_iob</code> indicates
whether an entity starts, continues or ends on the tag. If no entity type is set
on a token, it will return an empty string.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">IOB Scheme<!-- --> </span></h4>
<ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">I</code> – Token is <strong>inside</strong> an entity.</li>
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">O</code> – Token is <strong>outside</strong> an entity.</li>
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">B</code> – Token is the <strong>beginning</strong> of an entity.</li>
</ul>
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">BILUO Scheme<!-- --> </span></h4>
<ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">B</code> – Token is the <strong>beginning</strong> of a multi-token entity.</li>
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">I</code> – Token is <strong>inside</strong> a multi-token entity.</li>
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">L</code> – Token is the <strong>last</strong> token of a multi-token entity.</li>
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">U</code> – Token is a single-token <strong>unit</strong> entity.</li>
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">O</code> – Token is <strong>outside</strong> an entity.</li>
</ul>
</div></div></aside><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Text</th><th class="table_th__QJ9F8">ent_iob</th><th class="table_th__QJ9F8">ent_iob_</th><th class="table_th__QJ9F8">ent_type_</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">San</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">3</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">B</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">&quot;GPE&quot;</code></td><td class="table_td__rmpJx">beginning of an entity</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">Francisco</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">1</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">I</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">&quot;GPE&quot;</code></td><td class="table_td__rmpJx">inside an entity</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">considers</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">2</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">O</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">&quot;&quot;</code></td><td class="table_td__rmpJx">outside an entity</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">banning</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">2</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">O</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">&quot;&quot;</code></td><td class="table_td__rmpJx">outside an entity</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">sidewalk</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">2</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">O</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">&quot;&quot;</code></td><td class="table_td__rmpJx">outside an entity</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">delivery</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">2</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">O</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">&quot;&quot;</code></td><td class="table_td__rmpJx">outside an entity</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx">robots</td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">2</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">O</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">&quot;&quot;</code></td><td class="table_td__rmpJx">outside an entity</td></tr></tbody></table><h3 id="setting-entities" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#setting-entities" class="heading-text typography_permalink__UiIRy">Setting entity annotations <!-- --> </a></h3><p>To ensure that the sequence of token annotations remains consistent, you have to
set entity annotations <strong>at the document level</strong>. However, you can’t write
directly to the <code class="code_inline-code__Bq7ot">token.ent_iob</code> or <code class="code_inline-code__Bq7ot">token.ent_type</code> attributes, so the easiest
way to set entities is to use the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc#set_ents"><code class="code_inline-code__Bq7ot">doc.set_ents</code></a> function
and create the new entity as a <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/span"><code class="code_inline-code__Bq7ot">Span</code></a>.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>Keep in mind that <code class="code_inline-code__Bq7ot">Span</code> is initialized with the start and end <strong>token</strong>
indices, not the character offsets. To create a span from character offsets, use
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc#char_span"><code class="code_inline-code__Bq7ot">Doc.char_span</code></a>:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><h4 id="setting-from-array" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#setting-from-array" class="heading-text typography_permalink__UiIRy">Setting entity annotations from array <!-- --> </a></h4><p>You can also assign entity annotations using the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc#from_array"><code class="code_inline-code__Bq7ot">doc.from_array</code></a> method. To do this, you should include
both the <code class="code_inline-code__Bq7ot">ENT_TYPE</code> and the <code class="code_inline-code__Bq7ot">ENT_IOB</code> attributes in the array you’re importing
from.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h4 id="setting-cython" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#setting-cython" class="heading-text typography_permalink__UiIRy">Setting entity annotations in Cython <!-- --> </a></h4><p>Finally, you can always write to the underlying struct if you compile a
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="http://cython.org/">Cython</a> function. This is easy to do, and allows you to
write efficient native code.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>Obviously, if you write directly to the array of <code class="code_inline-code__Bq7ot">TokenC*</code> structs, you’ll have
responsibility for ensuring that the data is left in a consistent state.</p><h3 id="entity-types" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#entity-types" class="heading-text typography_permalink__UiIRy">Built-in entity types <!-- --> </a></h3><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Tip: Understanding entity types<!-- --> </span></h4>
<p>You can also use <code class="code_inline-code__Bq7ot">spacy.explain()</code> to get the description for the string
representation of an entity label. For example, <code class="code_inline-code__Bq7ot">spacy.explain(&quot;LANGUAGE&quot;)</code>
will return “any named language”.</p>
</div></div></aside><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span>Annotation scheme</span></h4><p>For details on the entity types available in spaCy’s trained pipelines, see the
“label scheme” sections of the individual models in the
<a class="link_root__1Me7D" href="/models">models directory</a>.</p></aside><h3 id="displacy" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#displacy" class="heading-text typography_permalink__UiIRy">Visualizing named entities <!-- --> </a></h3><p>The
<a class="link_root__1Me7D" href="https://explosion.ai/demos/displacy-ent">displaCy <sup>ENT</sup> visualizer</a>
lets you explore an entity recognition model’s behavior interactively. If you’re
training a model, it’s very useful to run the visualization yourself. To help
you do that, spaCy comes with a visualization module. You can pass a <code class="code_inline-code__Bq7ot">Doc</code> or a
list of <code class="code_inline-code__Bq7ot">Doc</code> objects to displaCy and run
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#displacy.serve"><code class="code_inline-code__Bq7ot">displacy.serve</code></a> to run the web server, or
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#displacy.render"><code class="code_inline-code__Bq7ot">displacy.render</code></a> to generate the raw markup.</p><p>For more details and examples, see the
<a class="link_root__1Me7D" href="/usage/visualizers">usage guide on visualizing spaCy</a>.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Named Entity example</h4><code class="code_code__CILJL language-python language-python"></code></pre><iframe class="embed_standalone__RHbIL" title="displaCy visualizer for entities" src="/images/displacy-ent2.html" width="800" height="180" allowfullscreen="" frameBorder="0"></iframe></section>
<section id="section-entity-linking" class="section_root__k1hUl"><h2 id="entity-linking" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#entity-linking" class="heading-text typography_permalink__UiIRy">Entity Linking <!-- --> </a></h2><p>To ground the named entities into the “real world”, spaCy provides functionality
to perform entity linking, which resolves a textual entity to a unique
identifier from a knowledge base (KB). You can create your own
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/kb"><code class="code_inline-code__Bq7ot">KnowledgeBase</code></a> and <a class="link_root__1Me7D" href="/usage/training">train</a> a new
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/entitylinker"><code class="code_inline-code__Bq7ot">EntityLinker</code></a> using that custom knowledge base.</p><h3 id="entity-linking-accessing" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#entity-linking-accessing" class="heading-text typography_permalink__UiIRy">Accessing entity identifiers <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="To use this functionality, spaCy needs a trained pipeline that supports the following capabilities: entity linking">Needs model</span></h3><p>The annotated KB identifier is accessible as either a hash value or as a string,
using the attributes <code class="code_inline-code__Bq7ot">ent.kb_id</code> and <code class="code_inline-code__Bq7ot">ent.kb_id_</code> of a <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/span"><code class="code_inline-code__Bq7ot">Span</code></a>
object, or the <code class="code_inline-code__Bq7ot">ent_kb_id</code> and <code class="code_inline-code__Bq7ot">ent_kb_id_</code> attributes of a
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token"><code class="code_inline-code__Bq7ot">Token</code></a> object.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre></section>
<section id="section-tokenization" class="section_root__k1hUl"><h2 id="tokenization" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#tokenization" class="heading-text typography_permalink__UiIRy">Tokenization <!-- --> </a></h2><p>Tokenization is the task of splitting a text into meaningful segments, called
<em>tokens</em>. The input to the tokenizer is a unicode text, and the output is a
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> object. To construct a <code class="code_inline-code__Bq7ot">Doc</code> object, you need a
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/vocab"><code class="code_inline-code__Bq7ot">Vocab</code></a> instance, a sequence of <code class="code_inline-code__Bq7ot">word</code> strings, and optionally a
sequence of <code class="code_inline-code__Bq7ot">spaces</code> booleans, which allow you to maintain alignment of the
tokens into the original string.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>spaCy’s tokenization is <strong>non-destructive</strong>, which means that you’ll always be
able to reconstruct the original input from the tokenized output. Whitespace
information is preserved in the tokens and no information is added or removed
during tokenization. This is kind of a core principle of spaCy’s <code class="code_inline-code__Bq7ot">Doc</code> object:
<code class="code_inline-code__Bq7ot">doc.text == input_text</code> should always hold true.</p></aside><p>During processing, spaCy first <strong>tokenizes</strong> the text, i.e. segments it into
words, punctuation and so on. This is done by applying rules specific to each
language. For example, punctuation at the end of a sentence should be split off
– whereas “U.K.” should remain one token. Each <code class="code_inline-code__Bq7ot">Doc</code> consists of individual
tokens, and we can iterate over them:</p>
<pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre>
<table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8" align="center">0</th><th class="table_th__QJ9F8" align="center">1</th><th class="table_th__QJ9F8" align="center">2</th><th class="table_th__QJ9F8" align="center">3</th><th class="table_th__QJ9F8" align="center">4</th><th class="table_th__QJ9F8" align="center">5</th><th class="table_th__QJ9F8" align="center">6</th><th class="table_th__QJ9F8" align="center">7</th><th class="table_th__QJ9F8" align="center">8</th><th class="table_th__QJ9F8" align="center">9</th><th class="table_th__QJ9F8" align="center">10</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx" align="center">Apple</td><td class="table_td__rmpJx" align="center">is</td><td class="table_td__rmpJx" align="center">looking</td><td class="table_td__rmpJx" align="center">at</td><td class="table_td__rmpJx" align="center">buying</td><td class="table_td__rmpJx" align="center">U.K.</td><td class="table_td__rmpJx" align="center">startup</td><td class="table_td__rmpJx" align="center">for</td><td class="table_td__rmpJx" align="center">$</td><td class="table_td__rmpJx" align="center">1</td><td class="table_td__rmpJx" align="center">billion</td></tr></tbody></table>
<p>First, the raw text is split on whitespace characters, similar to
<code class="code_inline-code__Bq7ot">text.split(&#x27; &#x27;)</code>. Then, the tokenizer processes the text from left to right. On
each substring, it performs two checks:</p>
<ol class="list_ol__aclSa">
<li class="list_li__sfx_z">
<p><strong>Does the substring match a tokenizer exception rule?</strong> For example, “don’t”
does not contain whitespace, but should be split into two tokens, “do” and
“n’t”, while “U.K.” should always remain one token.</p>
</li>
<li class="list_li__sfx_z">
<p><strong>Can a prefix, suffix or infix be split off?</strong> For example punctuation like
commas, periods, hyphens or quotes.</p>
</li>
</ol>
<p>If there’s a match, the rule is applied and the tokenizer continues its loop,
starting with the newly split substrings. This way, spaCy can split <strong>complex,
nested tokens</strong> like combinations of abbreviations and multiple punctuation
marks.</p>
<aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><strong>Tokenizer exception:</strong> Special-case rule to split a string into several
tokens or prevent a token from being split when punctuation rules are
applied.</li>
<li class="list_li__sfx_z"><strong>Prefix:</strong> Character(s) at the beginning, e.g. <code class="code_inline-code__Bq7ot">$</code>, <code class="code_inline-code__Bq7ot">(</code>, <code class="code_inline-code__Bq7ot">“</code>, <code class="code_inline-code__Bq7ot">¿</code>.</li>
<li class="list_li__sfx_z"><strong>Suffix:</strong> Character(s) at the end, e.g. <code class="code_inline-code__Bq7ot">km</code>, <code class="code_inline-code__Bq7ot">)</code>, <code class="code_inline-code__Bq7ot">”</code>, <code class="code_inline-code__Bq7ot">!</code>.</li>
<li class="list_li__sfx_z"><strong>Infix:</strong> Character(s) in between, e.g. <code class="code_inline-code__Bq7ot">-</code>, <code class="code_inline-code__Bq7ot">--</code>, <code class="code_inline-code__Bq7ot">/</code>, <code class="code_inline-code__Bq7ot">…</code>.</li>
</ul>
</div></div></aside>
<figure class="gatsby-resp-image-figure"><img class="embed_image__mSQUH" src="/images/tokenization.svg" alt="Example of the tokenization process" width="650" height="auto"/></figure>
<p>While punctuation rules are usually pretty general, tokenizer exceptions
strongly depend on the specifics of the individual language. This is why each
<a class="link_root__1Me7D" href="/usage/models#languages">available language</a> has its own subclass, like
<code class="code_inline-code__Bq7ot">English</code> or <code class="code_inline-code__Bq7ot">German</code>, that loads in lists of hard-coded data and exception
rules.</p><section class="accordion" id="how-tokenizer-works"><div class="accordion_root__pPltq accordion_spaced__Ebyjn"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">Algorithm details: How spaCy&#x27;s tokenizer works</span><a class="link_root__1Me7D accordion_anchor__kidBh link_no-link-layout__RPvod" href="/usage/linguistic-features#how-tokenizer-works">¶</a></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><p>spaCy introduces a novel tokenization algorithm that gives a better balance
between performance, ease of definition and ease of alignment into the original
string.</p><p>After consuming a prefix or suffix, we consult the special cases again. We want
the special cases to handle things like “don’t” in English, and we want the same
rule to work for “(don’t)!“. We do this by splitting off the open bracket, then
the exclamation, then the closed bracket, and finally matching the special case.
Here’s an implementation of the algorithm in Python optimized for readability
rather than performance:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>The algorithm can be summarized as follows:</p><ol class="list_ol__aclSa">
<li class="list_li__sfx_z">Iterate over space-separated substrings.</li>
<li class="list_li__sfx_z">Check whether we have an explicitly defined special case for this substring.
If we do, use it.</li>
<li class="list_li__sfx_z">Look for a token match. If there is a match, stop processing and keep this
token.</li>
<li class="list_li__sfx_z">Check whether we have an explicitly defined special case for this substring.
If we do, use it.</li>
<li class="list_li__sfx_z">Otherwise, try to consume one prefix. If we consumed a prefix, go back to #3,
so that the token match and special cases always get priority.</li>
<li class="list_li__sfx_z">If we didn’t consume a prefix, try to consume a suffix and then go back to
#3.</li>
<li class="list_li__sfx_z">If we can’t consume a prefix or a suffix, look for a URL match.</li>
<li class="list_li__sfx_z">If there’s no URL match, then look for a special case.</li>
<li class="list_li__sfx_z">Look for “infixes” – stuff like hyphens etc. and split the substring into
tokens on all infixes.</li>
<li class="list_li__sfx_z">Once we can’t consume any more of the string, handle it as a single token.</li>
<li class="list_li__sfx_z">Make a final pass over the text to check for special cases that include
spaces or that were missed due to the incremental processing of affixes.</li>
</ol></div></div></section><p><strong>Global</strong> and <strong>language-specific</strong> tokenizer data is supplied via the language
data in <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/lang"><code class="code_inline-code__Bq7ot">spacy/lang</code></a>. The tokenizer exceptions
define special cases like “don’t” in English, which needs to be split into two
tokens: <code class="code_inline-code__Bq7ot">{ORTH: &quot;do&quot;}</code> and <code class="code_inline-code__Bq7ot">{ORTH: &quot;n&#x27;t&quot;, NORM: &quot;not&quot;}</code>. The prefixes, suffixes
and infixes mostly define punctuation rules – for example, when to split off
periods (at the end of a sentence), and when to leave tokens containing periods
intact (abbreviations like “U.S.”).</p><section class="accordion" id="lang-data-vs-tokenizer"><div class="accordion_root__pPltq"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">Should I change the language data or add custom tokenizer rules?</span><a class="link_root__1Me7D accordion_anchor__kidBh link_no-link-layout__RPvod" href="/usage/linguistic-features#lang-data-vs-tokenizer">¶</a></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><p>Tokenization rules that are specific to one language, but can be <strong>generalized
across that language</strong>, should ideally live in the language data in
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/lang"><code class="code_inline-code__Bq7ot">spacy/lang</code></a> – we always appreciate pull requests!
Anything that’s specific to a domain or text type – like financial trading
abbreviations or Bavarian youth slang – should be added as a special case rule
to your tokenizer instance. If you’re dealing with a lot of customizations, it
might make sense to create an entirely custom subclass.</p></div></div></section><hr class="section_hr__07Hes"/><h3 id="special-cases" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#special-cases" class="heading-text typography_permalink__UiIRy">Adding special case tokenization rules <!-- --> </a></h3><p>Most domains have at least some idiosyncrasies that require custom tokenization
rules. This could be very certain expressions, or abbreviations only used in
this specific field. Here’s how to add a special case rule to an existing
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/tokenizer"><code class="code_inline-code__Bq7ot">Tokenizer</code></a> instance:</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>The special case doesn’t have to match an entire whitespace-delimited substring.
The tokenizer will incrementally split off punctuation, and keep looking up the
remaining substring. The special case rules also have precedence over the
punctuation splitting.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><h4 id="tokenizer-debug" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#tokenizer-debug" class="heading-text typography_permalink__UiIRy">Debugging the tokenizer <!-- --> </a></h4><p>A working implementation of the pseudo-code above is available for debugging as
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/tokenizer#explain"><code class="code_inline-code__Bq7ot">nlp.tokenizer.explain(text)</code></a>. It returns a list of
tuples showing which tokenizer rule or pattern was matched for each token. The
tokens produced are identical to <code class="code_inline-code__Bq7ot">nlp.tokenizer()</code> except for whitespace tokens:</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Expected output<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-none"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h3 id="native-tokenizers" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#native-tokenizers" class="heading-text typography_permalink__UiIRy">Customizing spaCy’s Tokenizer class <!-- --> </a></h3><p>Let’s imagine you wanted to create a tokenizer for a new language or specific
domain. There are six things you may need to define:</p><ol class="list_ol__aclSa">
<li class="list_li__sfx_z">A dictionary of <strong>special cases</strong>. This handles things like contractions,
units of measurement, emoticons, certain abbreviations, etc.</li>
<li class="list_li__sfx_z">A function <code class="code_inline-code__Bq7ot">prefix_search</code>, to handle <strong>preceding punctuation</strong>, such as open
quotes, open brackets, etc.</li>
<li class="list_li__sfx_z">A function <code class="code_inline-code__Bq7ot">suffix_search</code>, to handle <strong>succeeding punctuation</strong>, such as
commas, periods, close quotes, etc.</li>
<li class="list_li__sfx_z">A function <code class="code_inline-code__Bq7ot">infix_finditer</code>, to handle non-whitespace separators, such as
hyphens etc.</li>
<li class="list_li__sfx_z">An optional boolean function <code class="code_inline-code__Bq7ot">token_match</code> matching strings that should never
be split, overriding the infix rules. Useful for things like numbers.</li>
<li class="list_li__sfx_z">An optional boolean function <code class="code_inline-code__Bq7ot">url_match</code>, which is similar to <code class="code_inline-code__Bq7ot">token_match</code>
except that prefixes and suffixes are removed before applying the match.</li>
</ol><p>You shouldn’t usually need to create a <code class="code_inline-code__Bq7ot">Tokenizer</code> subclass. Standard usage is
to use <code class="code_inline-code__Bq7ot">re.compile()</code> to build a regular expression object, and pass its
<code class="code_inline-code__Bq7ot">.search()</code> and <code class="code_inline-code__Bq7ot">.finditer()</code> methods:</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>If you need to subclass the tokenizer instead, the relevant methods to
specialize are <code class="code_inline-code__Bq7ot">find_prefix</code>, <code class="code_inline-code__Bq7ot">find_suffix</code> and <code class="code_inline-code__Bq7ot">find_infix</code>.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>When customizing the prefix, suffix and infix handling, remember that you’re
passing in <strong>functions</strong> for spaCy to execute, e.g. <code class="code_inline-code__Bq7ot">prefix_re.search</code> – not
just the regular expressions. This means that your functions also need to define
how the rules should be applied. For example, if you’re adding your own prefix
rules, you need to make sure they’re only applied to characters at the
<strong>beginning of a token</strong>, e.g. by adding <code class="code_inline-code__Bq7ot">^</code>. Similarly, suffix rules should
only be applied at the <strong>end of a token</strong>, so your expression should end with a
<code class="code_inline-code__Bq7ot">$</code>.</p></aside><h4 id="native-tokenizer-additions" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#native-tokenizer-additions" class="heading-text typography_permalink__UiIRy">Modifying existing rule sets <!-- --> </a></h4><p>In many situations, you don’t necessarily need entirely custom rules. Sometimes
you just want to add another character to the prefixes, suffixes or infixes. The
default prefix, suffix and infix rules are available via the <code class="code_inline-code__Bq7ot">nlp</code> object’s
<code class="code_inline-code__Bq7ot">Defaults</code> and the <code class="code_inline-code__Bq7ot">Tokenizer</code> attributes such as
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/tokenizer#attributes"><code class="code_inline-code__Bq7ot">Tokenizer.suffix_search</code></a> are writable, so you can
overwrite them with compiled regular expression objects using modified default
rules. spaCy ships with utility functions to help you compile the regular
expressions – for example,
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#util.compile_suffix_regex"><code class="code_inline-code__Bq7ot">compile_suffix_regex</code></a>:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>Similarly, you can remove a character from the default suffixes:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>The <code class="code_inline-code__Bq7ot">Tokenizer.suffix_search</code> attribute should be a function which takes a
unicode string and returns a <strong>regex match object</strong> or <code class="code_inline-code__Bq7ot">None</code>. Usually we use
the <code class="code_inline-code__Bq7ot">.search</code> attribute of a compiled regex object, but you can use some other
function that behaves the same way.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>If you’ve loaded a trained pipeline, writing to the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#defaults"><code class="code_inline-code__Bq7ot">nlp.Defaults</code></a> or <code class="code_inline-code__Bq7ot">English.Defaults</code> directly won’t
work, since the regular expressions are read from the pipeline data and will be
compiled when you load it. If you modify <code class="code_inline-code__Bq7ot">nlp.Defaults</code>, you’ll only see the
effect if you call <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#spacy.blank"><code class="code_inline-code__Bq7ot">spacy.blank</code></a>. If you want to
modify the tokenizer loaded from a trained pipeline, you should modify
<code class="code_inline-code__Bq7ot">nlp.tokenizer</code> directly. If you’re training your own pipeline, you can register
<a class="link_root__1Me7D" href="/usage/training#custom-code-nlp-callbacks">callbacks</a> to modify the <code class="code_inline-code__Bq7ot">nlp</code>
object before training.</p></aside><p>The prefix, infix and suffix rule sets include not only individual characters
but also detailed regular expressions that take the surrounding context into
account. For example, there is a regular expression that treats a hyphen between
letters as an infix. If you do not want the tokenizer to split on hyphens
between letters, you can modify the existing infix definition from
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/lang/punctuation.py"><code class="code_inline-code__Bq7ot">lang/punctuation.py</code></a>:</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>For an overview of the default regular expressions, see
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/lang/punctuation.py"><code class="code_inline-code__Bq7ot">lang/punctuation.py</code></a> and
language-specific definitions such as
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/lang/de/punctuation.py"><code class="code_inline-code__Bq7ot">lang/de/punctuation.py</code></a> for
German.</p><h3 id="custom-tokenizer" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#custom-tokenizer" class="heading-text typography_permalink__UiIRy">Hooking a custom tokenizer into the pipeline <!-- --> </a></h3><p>The tokenizer is the first component of the processing pipeline and the only one
that can’t be replaced by writing to <code class="code_inline-code__Bq7ot">nlp.pipeline</code>. This is because it has a
different signature from all the other components: it takes a text and returns a
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a>, whereas all other components expect to already receive a
tokenized <code class="code_inline-code__Bq7ot">Doc</code>.</p><figure class="gatsby-resp-image-figure"><img class="embed_image__mSQUH" src="/images/pipeline.svg" alt="The processing pipeline" width="650" height="auto"/></figure><p>To overwrite the existing tokenizer, you need to replace <code class="code_inline-code__Bq7ot">nlp.tokenizer</code> with a
custom function that takes a text and returns a <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a>.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Creating a Doc<!-- --> </span></h4>
<p>Constructing a <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> object manually requires at least two
arguments: the shared <code class="code_inline-code__Bq7ot">Vocab</code> and a list of words. Optionally, you can pass in
a list of <code class="code_inline-code__Bq7ot">spaces</code> values indicating whether the token at this position is
followed by a space (default <code class="code_inline-code__Bq7ot">True</code>). See the section on
<a class="link_root__1Me7D" href="/usage/linguistic-features#own-annotations">pre-tokenized text</a> for more info.</p>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Argument</th><th class="table_th__QJ9F8">Type</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">text</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">str</code></td><td class="table_td__rmpJx">The raw text to tokenize.</td></tr><tr class="table_tr__K_tkF table_footer__gJRIy table-footer"><td class="table_td__rmpJx"><strong>RETURNS</strong></td><td class="table_td__rmpJx"><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a></td><td class="table_td__rmpJx">The tokenized document.</td></tr></tbody></table><h4 id="custom-tokenizer-example" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#custom-tokenizer-example" class="heading-text typography_permalink__UiIRy">Example 1: Basic whitespace tokenizer <!-- --> </a></h4><p>Here’s an example of the most basic whitespace tokenizer. It takes the shared
vocab, so it can construct <code class="code_inline-code__Bq7ot">Doc</code> objects. When it’s called on a text, it returns
a <code class="code_inline-code__Bq7ot">Doc</code> object consisting of the text split on single space characters. We can
then overwrite the <code class="code_inline-code__Bq7ot">nlp.tokenizer</code> attribute with an instance of our custom
tokenizer.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h4 id="custom-tokenizer-example2" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#custom-tokenizer-example2" class="heading-text typography_permalink__UiIRy">Example 2: Third-party tokenizers (BERT word pieces) <!-- --> </a></h4><p>You can use the same approach to plug in any other third-party tokenizers. Your
custom callable just needs to return a <code class="code_inline-code__Bq7ot">Doc</code> object with the tokens produced by
your tokenizer. In this example, the wrapper uses the <strong>BERT word piece
tokenizer</strong>, provided by the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/huggingface/tokenizers"><code class="code_inline-code__Bq7ot">tokenizers</code></a> library. The tokens
available in the <code class="code_inline-code__Bq7ot">Doc</code> object returned by spaCy now match the exact word pieces
produced by the tokenizer.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">💡 Tip: spacy-transformers<!-- --> </span></h4>
<p>If you’re working with transformer models like BERT, check out the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spacy-transformers"><code class="code_inline-code__Bq7ot">spacy-transformers</code></a>
extension package and <a class="link_root__1Me7D" href="/usage/embeddings-transformers">documentation</a>. It
includes a pipeline component for using pretrained transformer weights and
<strong>training transformer models</strong> in spaCy, as well as helpful utilities for
aligning word pieces to linguistic tokenization.</p>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Custom BERT word piece tokenizer</h4><code class="code_code__CILJL language-python language-python"></code></pre><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note on tokenization and models</span></h4><p>Keep in mind that your models’ results may be less accurate if the tokenization
during training differs from the tokenization at runtime. So if you modify a
trained pipeline’s tokenization afterwards, it may produce very different
predictions. You should therefore train your pipeline with the <strong>same
tokenizer</strong> it will be using at runtime. See the docs on
<a class="link_root__1Me7D" href="/usage/linguistic-features#custom-tokenizer-training">training with custom tokenization</a> for details.</p></aside><h4 id="custom-tokenizer-training" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#custom-tokenizer-training" class="heading-text typography_permalink__UiIRy">Training with custom tokenization <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="This feature is new and was introduced in spaCy v3.0">v<!-- -->3.0</span></h4><p>spaCy’s <a class="link_root__1Me7D" href="/usage/training#config">training config</a> describes the settings,
hyperparameters, pipeline and tokenizer used for constructing and training the
pipeline. The <code class="code_inline-code__Bq7ot">[nlp.tokenizer]</code> block refers to a <strong>registered function</strong> that
takes the <code class="code_inline-code__Bq7ot">nlp</code> object and returns a tokenizer. Here, we’re registering a
function called <code class="code_inline-code__Bq7ot">whitespace_tokenizer</code> in the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#registry"><code class="code_inline-code__Bq7ot">@tokenizers</code> registry</a>. To make sure spaCy knows how
to construct your tokenizer during training, you can pass in your Python file by
setting <code class="code_inline-code__Bq7ot">--code functions.py</code> when you run <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a>.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">config.cfg<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">functions.py</h4><code class="code_code__CILJL language-python language-python code_wrap__b41os"></code></pre><p>Registered functions can also take arguments that are then passed in from the
config. This allows you to quickly change and keep track of different settings.
Here, the registered function called <code class="code_inline-code__Bq7ot">bert_word_piece_tokenizer</code> takes two
arguments: the path to a vocabulary file and whether to lowercase the text. The
Python type hints <code class="code_inline-code__Bq7ot">str</code> and <code class="code_inline-code__Bq7ot">bool</code> ensure that the received values have the
correct type.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">config.cfg<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">functions.py</h4><code class="code_code__CILJL language-python language-python code_wrap__b41os"></code></pre><p>To avoid hard-coding local paths into your config file, you can also set the
vocab path on the CLI by using the <code class="code_inline-code__Bq7ot">--nlp.tokenizer.vocab_file</code>
<a class="link_root__1Me7D" href="/usage/training#config-overrides">override</a> when you run
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a>. For more details on using registered functions,
see the docs in <a class="link_root__1Me7D" href="/usage/training#custom-code">training with custom code</a>.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><p>Remember that a registered function should always be a function that spaCy
<strong>calls to create something</strong>, not the “something” itself. In this case, it
<strong>creates a function</strong> that takes the <code class="code_inline-code__Bq7ot">nlp</code> object and returns a callable that
takes a text and returns a <code class="code_inline-code__Bq7ot">Doc</code>.</p></aside><h4 id="own-annotations" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#own-annotations" class="heading-text typography_permalink__UiIRy">Using pre-tokenized text <!-- --> </a></h4><p>spaCy generally assumes by default that your data is <strong>raw text</strong>. However,
sometimes your data is partially annotated, e.g. with pre-existing tokenization,
part-of-speech tags, etc. The most common situation is that you have
<strong>pre-defined tokenization</strong>. If you have a list of strings, you can create a
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> object directly. Optionally, you can also specify a list of
boolean values, indicating whether each word is followed by a space.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">✏️ Things to try<!-- --> </span></h4>
<ol class="list_ol__aclSa">
<li class="list_li__sfx_z">Change a boolean value in the list of <code class="code_inline-code__Bq7ot">spaces</code>. You should see it reflected
in the <code class="code_inline-code__Bq7ot">doc.text</code> and whether the token is followed by a space.</li>
<li class="list_li__sfx_z">Remove <code class="code_inline-code__Bq7ot">spaces=spaces</code> from the <code class="code_inline-code__Bq7ot">Doc</code>. You should see that every token is
now followed by a space.</li>
<li class="list_li__sfx_z">Copy-paste a random sentence from the internet and manually construct a
<code class="code_inline-code__Bq7ot">Doc</code> with <code class="code_inline-code__Bq7ot">words</code> and <code class="code_inline-code__Bq7ot">spaces</code> so that the <code class="code_inline-code__Bq7ot">doc.text</code> matches the original
input text.</li>
</ol>
</div></div></aside><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>If provided, the spaces list must be the <strong>same length</strong> as the words list. The
spaces list affects the <code class="code_inline-code__Bq7ot">doc.text</code>, <code class="code_inline-code__Bq7ot">span.text</code>, <code class="code_inline-code__Bq7ot">token.idx</code>, <code class="code_inline-code__Bq7ot">span.start_char</code>
and <code class="code_inline-code__Bq7ot">span.end_char</code> attributes. If you don’t provide a <code class="code_inline-code__Bq7ot">spaces</code> sequence, spaCy
will assume that all words are followed by a space. Once you have a
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> object, you can write to its attributes to set the
part-of-speech tags, syntactic dependencies, named entities and other
attributes.</p><h4 id="aligning-tokenization" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#aligning-tokenization" class="heading-text typography_permalink__UiIRy">Aligning tokenization <!-- --> </a></h4><p>spaCy’s tokenization is non-destructive and uses language-specific rules
optimized for compatibility with treebank annotations. Other tools and resources
can sometimes tokenize things differently – for example, <code class="code_inline-code__Bq7ot">&quot;I&#x27;m&quot;</code> →
<code class="code_inline-code__Bq7ot">[&quot;I&quot;, &quot;&#x27;&quot;, &quot;m&quot;]</code> instead of <code class="code_inline-code__Bq7ot">[&quot;I&quot;, &quot;&#x27;m&quot;]</code>.</p><p>In situations like that, you often want to align the tokenization so that you
can merge annotations from different sources together, or take vectors predicted
by a
<a class="link_root__1Me7D link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/huggingface/pytorch-transformers"><span class="link_source-text__VDP74">pretrained BERT model</span></a> and
apply them to spaCy tokens. spaCy’s <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example#alignment-object"><code class="code_inline-code__Bq7ot">Alignment</code></a>
object allows the one-to-one mappings of token indices in both directions as
well as taking into account indices where multiple tokens align to one single
token.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">✏️ Things to try<!-- --> </span></h4>
<ol class="list_ol__aclSa">
<li class="list_li__sfx_z">Change the capitalization in one of the token lists – for example,
<code class="code_inline-code__Bq7ot">&quot;obama&quot;</code> to <code class="code_inline-code__Bq7ot">&quot;Obama&quot;</code>. You’ll see that the alignment is case-insensitive.</li>
<li class="list_li__sfx_z">Change <code class="code_inline-code__Bq7ot">&quot;podcasts&quot;</code> in <code class="code_inline-code__Bq7ot">other_tokens</code> to <code class="code_inline-code__Bq7ot">&quot;pod&quot;, &quot;casts&quot;</code>. You should see
that there are now two tokens of length 2 in <code class="code_inline-code__Bq7ot">y2x</code>, one corresponding to
“‘s”, and one to “podcasts”.</li>
<li class="list_li__sfx_z">Make <code class="code_inline-code__Bq7ot">other_tokens</code> and <code class="code_inline-code__Bq7ot">spacy_tokens</code> identical. You’ll see that all
tokens now correspond 1-to-1.</li>
</ol>
</div></div></aside><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>Here are some insights from the alignment information generated in the example
above:</p><ul class="list_ul__fe_HF">
<li class="list_li__sfx_z">The one-to-one mappings for the first four tokens are identical, which means
they map to each other. This makes sense because they’re also identical in the
input: <code class="code_inline-code__Bq7ot">&quot;i&quot;</code>, <code class="code_inline-code__Bq7ot">&quot;listened&quot;</code>, <code class="code_inline-code__Bq7ot">&quot;to&quot;</code> and <code class="code_inline-code__Bq7ot">&quot;obama&quot;</code>.</li>
<li class="list_li__sfx_z">The value of <code class="code_inline-code__Bq7ot">x2y.data[6]</code> is <code class="code_inline-code__Bq7ot">5</code>, which means that <code class="code_inline-code__Bq7ot">other_tokens[6]</code>
(<code class="code_inline-code__Bq7ot">&quot;podcasts&quot;</code>) aligns to <code class="code_inline-code__Bq7ot">spacy_tokens[5]</code> (also <code class="code_inline-code__Bq7ot">&quot;podcasts&quot;</code>).</li>
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">x2y.data[4]</code> and <code class="code_inline-code__Bq7ot">x2y.data[5]</code> are both <code class="code_inline-code__Bq7ot">4</code>, which means that both tokens 4
and 5 of <code class="code_inline-code__Bq7ot">other_tokens</code> (<code class="code_inline-code__Bq7ot">&quot;&#x27;&quot;</code> and <code class="code_inline-code__Bq7ot">&quot;s&quot;</code>) align to token 4 of <code class="code_inline-code__Bq7ot">spacy_tokens</code>
(<code class="code_inline-code__Bq7ot">&quot;&#x27;s&quot;</code>).</li>
</ul><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>The current implementation of the alignment algorithm assumes that both
tokenizations add up to the same string. For example, you’ll be able to align
<code class="code_inline-code__Bq7ot">[&quot;I&quot;, &quot;&#x27;&quot;, &quot;m&quot;]</code> and <code class="code_inline-code__Bq7ot">[&quot;I&quot;, &quot;&#x27;m&quot;]</code>, which both add up to <code class="code_inline-code__Bq7ot">&quot;I&#x27;m&quot;</code>, but not
<code class="code_inline-code__Bq7ot">[&quot;I&quot;, &quot;&#x27;m&quot;]</code> and <code class="code_inline-code__Bq7ot">[&quot;I&quot;, &quot;am&quot;]</code>.</p></aside></section>
<section id="section-retokenization" class="section_root__k1hUl"><h2 id="retokenization" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#retokenization" class="heading-text typography_permalink__UiIRy">Merging and splitting <!-- --> </a></h2><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc#retokenize"><code class="code_inline-code__Bq7ot">Doc.retokenize</code></a> context manager lets you merge and
split tokens. Modifications to the tokenization are stored and performed all at
once when the context manager exits. To merge several tokens into one single
token, pass a <code class="code_inline-code__Bq7ot">Span</code> to <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc#retokenizer.merge"><code class="code_inline-code__Bq7ot">retokenizer.merge</code></a>. An
optional dictionary of <code class="code_inline-code__Bq7ot">attrs</code> lets you set attributes that will be assigned to
the merged token – for example, the lemma, part-of-speech tag or entity type. By
default, the merged token will receive the same attributes as the merged span’s
root.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">✏️ Things to try<!-- --> </span></h4>
<ol class="list_ol__aclSa">
<li class="list_li__sfx_z">Inspect the <code class="code_inline-code__Bq7ot">token.lemma_</code> attribute with and without setting the <code class="code_inline-code__Bq7ot">attrs</code>.
You’ll see that the lemma defaults to “New”, the lemma of the span’s root.</li>
<li class="list_li__sfx_z">Overwrite other attributes like the <code class="code_inline-code__Bq7ot">&quot;ENT_TYPE&quot;</code>. Since “New York” is also
recognized as a named entity, this change will also be reflected in the
<code class="code_inline-code__Bq7ot">doc.ents</code>.</li>
</ol>
</div></div></aside><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Tip: merging entities and noun phrases<!-- --> </span></h4>
<p>If you need to merge named entities or noun chunks, check out the built-in
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/pipeline-functions#merge_entities"><code class="code_inline-code__Bq7ot">merge_entities</code></a> and
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/pipeline-functions#merge_noun_chunks"><code class="code_inline-code__Bq7ot">merge_noun_chunks</code></a> pipeline
components. When added to your pipeline using <code class="code_inline-code__Bq7ot">nlp.add_pipe</code>, they’ll take
care of merging the spans automatically.</p>
</div></div></aside><p>If an attribute in the <code class="code_inline-code__Bq7ot">attrs</code> is a context-dependent token attribute, it will
be applied to the underlying <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token"><code class="code_inline-code__Bq7ot">Token</code></a>. For example <code class="code_inline-code__Bq7ot">LEMMA</code>, <code class="code_inline-code__Bq7ot">POS</code>
or <code class="code_inline-code__Bq7ot">DEP</code> only apply to a word in context, so they’re token attributes. If an
attribute is a context-independent lexical attribute, it will be applied to the
underlying <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/lexeme"><code class="code_inline-code__Bq7ot">Lexeme</code></a>, the entry in the vocabulary. For example,
<code class="code_inline-code__Bq7ot">LOWER</code> or <code class="code_inline-code__Bq7ot">IS_STOP</code> apply to all words of the same spelling, regardless of the
context.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Note on merging overlapping spans</span></h4><p>If you’re trying to merge spans that overlap, spaCy will raise an error because
it’s unclear how the result should look. Depending on the application, you may
want to match the shortest or longest possible span, so it’s up to you to filter
them. If you’re looking for the longest non-overlapping span, you can use the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#util.filter_spans"><code class="code_inline-code__Bq7ot">util.filter_spans</code></a> helper:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre></aside><h3 class="typography_heading__D82WZ typography_h3__mPKmB"><span class="heading-text">Splitting tokens<!-- --> </span></h3><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc#retokenizer.split"><code class="code_inline-code__Bq7ot">retokenizer.split</code></a> method allows splitting
one token into two or more tokens. This can be useful for cases where
tokenization rules alone aren’t sufficient. For example, you might want to split
“its” into the tokens “it” and “is” – but not the possessive pronoun “its”. You
can write rule-based logic that can find only the correct “its” to split, but by
that time, the <code class="code_inline-code__Bq7ot">Doc</code> will already be tokenized.</p><p>This process of splitting a token requires more settings, because you need to
specify the text of the individual tokens, optional per-token attributes and how
the tokens should be attached to the existing syntax tree. This can be done by
supplying a list of <code class="code_inline-code__Bq7ot">heads</code> – either the token to attach the newly split token
to, or a <code class="code_inline-code__Bq7ot">(token, subtoken)</code> tuple if the newly split token should be attached
to another subtoken. In this case, “New” should be attached to “York” (the
second split subtoken) and “York” should be attached to “in”.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">✏️ Things to try<!-- --> </span></h4>
<ol class="list_ol__aclSa">
<li class="list_li__sfx_z">Assign different attributes to the subtokens and compare the result.</li>
<li class="list_li__sfx_z">Change the heads so that “New” is attached to “in” and “York” is attached
to “New”.</li>
<li class="list_li__sfx_z">Split the token into three tokens instead of two – for example,
<code class="code_inline-code__Bq7ot">[&quot;New&quot;, &quot;Yo&quot;, &quot;rk&quot;]</code>.</li>
</ol>
</div></div></aside><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>Specifying the heads as a list of <code class="code_inline-code__Bq7ot">token</code> or <code class="code_inline-code__Bq7ot">(token, subtoken)</code> tuples allows
attaching split subtokens to other subtokens, without having to keep track of
the token indices after splitting.</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Token</th><th class="table_th__QJ9F8">Head</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">&quot;New&quot;</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">(doc[3], 1)</code></td><td class="table_td__rmpJx">Attach this token to the second subtoken (index <code class="code_inline-code__Bq7ot">1</code>) that <code class="code_inline-code__Bq7ot">doc[3]</code> will be split into, i.e. “York”.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">&quot;York&quot;</code></td><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">doc[2]</code></td><td class="table_td__rmpJx">Attach this token to <code class="code_inline-code__Bq7ot">doc[1]</code> in the original <code class="code_inline-code__Bq7ot">Doc</code>, i.e. “in”.</td></tr></tbody></table><p>If you don’t care about the heads (for example, if you’re only running the
tokenizer and not the parser), you can attach each subtoken to itself:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python code_wrap__b41os"></code></pre><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>When splitting tokens, the subtoken texts always have to match the original
token text – or, put differently <code class="code_inline-code__Bq7ot code_wrap__b41os">&quot;&quot;.join(subtokens) == token.text</code> always needs
to hold true. If this wasn’t the case, splitting tokens could easily end up
producing confusing and unexpected results that would contradict spaCy’s
non-destructive tokenization policy.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-diff language-diff"></code></pre></aside><h3 id="retokenization-extensions" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#retokenization-extensions" class="heading-text typography_permalink__UiIRy">Overwriting custom extension attributes <!-- --> </a></h3><p>If you’ve registered custom
<a class="link_root__1Me7D" href="/usage/processing-pipelines#custom-components-attributes">extension attributes</a>,
you can overwrite them during tokenization by providing a dictionary of
attribute names mapped to new values as the <code class="code_inline-code__Bq7ot">&quot;_&quot;</code> key in the <code class="code_inline-code__Bq7ot">attrs</code>. For
merging, you need to provide one dictionary of attributes for the resulting
merged token. For splitting, you need to provide a list of dictionaries with
custom attributes, one per split subtoken.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>To set extension attributes during retokenization, the attributes need to be
<strong>registered</strong> using the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#set_extension"><code class="code_inline-code__Bq7ot">Token.set_extension</code></a>
method and they need to be <strong>writable</strong>. This means that they should either have
a default value that can be overwritten, or a getter <em>and</em> setter. Method
extensions or extensions with only a getter are computed dynamically, so their
values can’t be overwritten. For more details, see the
<a class="link_root__1Me7D" href="/usage/processing-pipelines#custom-components-attributes">extension attribute docs</a>.</p></aside><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">✏️ Things to try<!-- --> </span></h4>
<ol class="list_ol__aclSa">
<li class="list_li__sfx_z">Add another custom extension – maybe <code class="code_inline-code__Bq7ot">&quot;music_style&quot;</code>? – and overwrite it.</li>
<li class="list_li__sfx_z">Change the extension attribute to use only a <code class="code_inline-code__Bq7ot">getter</code> function. You should
see that spaCy raises an error, because the attribute is not writable
anymore.</li>
<li class="list_li__sfx_z">Rewrite the code to split a token with <code class="code_inline-code__Bq7ot">retokenizer.split</code>. Remember that
you need to provide a list of extension attribute values as the <code class="code_inline-code__Bq7ot">&quot;_&quot;</code>
property, one for each split subtoken.</li>
</ol>
</div></div></aside><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre></section>
<section id="section-sbd" class="section_root__k1hUl"><h2 id="sbd" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#sbd" class="heading-text typography_permalink__UiIRy">Sentence Segmentation <!-- --> </a></h2><p>A <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> object’s sentences are available via the <code class="code_inline-code__Bq7ot">Doc.sents</code>
property. To view a <code class="code_inline-code__Bq7ot">Doc</code>’s sentences, you can iterate over the <code class="code_inline-code__Bq7ot">Doc.sents</code>, a
generator that yields <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/span"><code class="code_inline-code__Bq7ot">Span</code></a> objects. You can check whether a <code class="code_inline-code__Bq7ot">Doc</code>
has sentence boundaries by calling
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc#has_annotation"><code class="code_inline-code__Bq7ot">Doc.has_annotation</code></a> with the attribute name
<code class="code_inline-code__Bq7ot">&quot;SENT_START&quot;</code>.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>spaCy provides four alternatives for sentence segmentation:</p><ol class="list_ol__aclSa">
<li class="list_li__sfx_z"><a class="link_root__1Me7D" href="/usage/linguistic-features#sbd-parser">Dependency parser</a>: the statistical
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/dependencyparser"><code class="code_inline-code__Bq7ot">DependencyParser</code></a> provides the most accurate
sentence boundaries based on full dependency parses.</li>
<li class="list_li__sfx_z"><a class="link_root__1Me7D" href="/usage/linguistic-features#sbd-senter">Statistical sentence segmenter</a>: the statistical
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/sentencerecognizer"><code class="code_inline-code__Bq7ot">SentenceRecognizer</code></a> is a simpler and faster
alternative to the parser that only sets sentence boundaries.</li>
<li class="list_li__sfx_z"><a class="link_root__1Me7D" href="/usage/linguistic-features#sbd-component">Rule-based pipeline component</a>: the rule-based
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/sentencizer"><code class="code_inline-code__Bq7ot">Sentencizer</code></a> sets sentence boundaries using a
customizable list of sentence-final punctuation.</li>
<li class="list_li__sfx_z"><a class="link_root__1Me7D" href="/usage/linguistic-features#sbd-custom">Custom function</a>: your own custom function added to the
processing pipeline can set sentence boundaries by writing to
<code class="code_inline-code__Bq7ot">Token.is_sent_start</code>.</li>
</ol><h3 id="sbd-parser" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#sbd-parser" class="heading-text typography_permalink__UiIRy">Default: Using the dependency parse <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="To use this functionality, spaCy needs a trained pipeline that supports the following capabilities: parser">Needs model</span></h3><p>Unlike other libraries, spaCy uses the dependency parse to determine sentence
boundaries. This is usually the most accurate approach, but it requires a
<strong>trained pipeline</strong> that provides accurate predictions. If your texts are
closer to general-purpose news or web text, this should work well out-of-the-box
with spaCy’s provided trained pipelines. For social media or conversational text
that doesn’t follow the same rules, your application may benefit from a custom
trained or rule-based component.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>spaCy’s dependency parser respects already set boundaries, so you can preprocess
your <code class="code_inline-code__Bq7ot">Doc</code> using custom components <em>before</em> it’s parsed. Depending on your text,
this may also improve parse accuracy, since the parser is constrained to predict
parses consistent with the sentence boundaries.</p><h3 id="sbd-senter" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#sbd-senter" class="heading-text typography_permalink__UiIRy">Statistical sentence segmenter <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="This feature is new and was introduced in spaCy v3.0">v<!-- -->3.0</span><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="To use this functionality, spaCy needs a trained pipeline that supports the following capabilities: senter">Needs model</span></h3><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/sentencerecognizer"><code class="code_inline-code__Bq7ot">SentenceRecognizer</code></a> is a simple statistical
component that only provides sentence boundaries. Along with being faster and
smaller than the parser, its primary advantage is that it’s easier to train
because it only requires annotated sentence boundaries rather than full
dependency parses. spaCy’s <a class="link_root__1Me7D" href="/models">trained pipelines</a> include both a parser
and a trained sentence segmenter, which is
<a class="link_root__1Me7D" href="/usage/processing-pipelines#disabling">disabled</a> by default. If you only need
sentence boundaries and no parser, you can use the <code class="code_inline-code__Bq7ot">exclude</code> or <code class="code_inline-code__Bq7ot">disable</code>
argument on <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#spacy.load"><code class="code_inline-code__Bq7ot">spacy.load</code></a> to load the pipeline
without the parser and then enable the sentence recognizer explicitly with
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#enable_pipe"><code class="code_inline-code__Bq7ot">nlp.enable_pipe</code></a>.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">senter vs. parser<!-- --> </span></h4>
<p>The recall for the <code class="code_inline-code__Bq7ot">senter</code> is typically slightly lower than for the parser,
which is better at predicting sentence boundaries when punctuation is not
present.</p>
</div></div></aside><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h3 id="sbd-component" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#sbd-component" class="heading-text typography_permalink__UiIRy">Rule-based pipeline component <!-- --> </a></h3><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/sentencizer"><code class="code_inline-code__Bq7ot">Sentencizer</code></a> component is a
<a class="link_root__1Me7D" href="/usage/processing-pipelines">pipeline component</a> that splits sentences on
punctuation like <code class="code_inline-code__Bq7ot">.</code>, <code class="code_inline-code__Bq7ot">!</code> or <code class="code_inline-code__Bq7ot">?</code>. You can plug it into your pipeline if you only
need sentence boundaries without dependency parses.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><h3 id="sbd-custom" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#sbd-custom" class="heading-text typography_permalink__UiIRy">Custom rule-based strategy <!-- --> </a></h3><p>If you want to implement your own strategy that differs from the default
rule-based approach of splitting on sentences, you can also create a
<a class="link_root__1Me7D" href="/usage/processing-pipelines#custom-components">custom pipeline component</a> that
takes a <code class="code_inline-code__Bq7ot">Doc</code> object and sets the <code class="code_inline-code__Bq7ot">Token.is_sent_start</code> attribute on each
individual token. If set to <code class="code_inline-code__Bq7ot">False</code>, the token is explicitly marked as <em>not</em> the
start of a sentence. If set to <code class="code_inline-code__Bq7ot">None</code> (default), it’s treated as a missing value
and can still be overwritten by the parser.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>To prevent inconsistent state, you can only set boundaries <strong>before</strong> a document
is parsed (and <code class="code_inline-code__Bq7ot">doc.has_annotation(&quot;DEP&quot;)</code> is <code class="code_inline-code__Bq7ot">False</code>). To ensure that your
component is added in the right place, you can set <code class="code_inline-code__Bq7ot">before=&#x27;parser&#x27;</code> or
<code class="code_inline-code__Bq7ot">first=True</code> when adding it to the pipeline using
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#add_pipe"><code class="code_inline-code__Bq7ot">nlp.add_pipe</code></a>.</p></aside><p>Here’s an example of a component that implements a pre-processing rule for
splitting on <code class="code_inline-code__Bq7ot">&quot;...&quot;</code> tokens. The component is added before the parser, which is
then used to further segment the text. That’s possible, because <code class="code_inline-code__Bq7ot">is_sent_start</code>
is only set to <code class="code_inline-code__Bq7ot">True</code> for some of the tokens – all others still specify <code class="code_inline-code__Bq7ot">None</code>
for unset sentence boundaries. This approach can be useful if you want to
implement <strong>additional</strong> rules specific to your data, while still being able to
take advantage of dependency-based sentence segmentation.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre></section>
<section id="section-mappings-exceptions" class="section_root__k1hUl"><h2 id="mappings-exceptions" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#mappings-exceptions" class="heading-text typography_permalink__UiIRy">Mappings &amp; Exceptions <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="This feature is new and was introduced in spaCy v3.0">v<!-- -->3.0</span></h2><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/attributeruler"><code class="code_inline-code__Bq7ot">AttributeRuler</code></a> manages <strong>rule-based mappings and
exceptions</strong> for all token-level attributes. As the number of
<a class="link_root__1Me7D link_with-icon__NAVDA" href="/api#architecture-pipeline"><span class="link_source-text__VDP74">pipeline components</span></a> has grown from spaCy v2 to
v3, handling rules and exceptions in each component individually has become
impractical, so the <code class="code_inline-code__Bq7ot">AttributeRuler</code> provides a single component with a unified
pattern format for all token attribute mappings and exceptions.</p><p>The <code class="code_inline-code__Bq7ot">AttributeRuler</code> uses
<a class="link_root__1Me7D" href="/usage/rule-based-matching#adding-patterns"><code class="code_inline-code__Bq7ot">Matcher</code> patterns</a> to identify
tokens and then assigns them the provided attributes. If needed, the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/matcher"><code class="code_inline-code__Bq7ot">Matcher</code></a> patterns can include context around the target token.
For example, the attribute ruler can:</p><ul class="list_ul__fe_HF">
<li class="list_li__sfx_z">provide exceptions for any <strong>token attributes</strong></li>
<li class="list_li__sfx_z">map <strong>fine-grained tags</strong> to <strong>coarse-grained tags</strong> for languages without
statistical morphologizers (replacing the v2.x <code class="code_inline-code__Bq7ot">tag_map</code> in the
<a class="link_root__1Me7D" href="/usage/linguistic-features#language-data">language data</a>)</li>
<li class="list_li__sfx_z">map token <strong>surface form + fine-grained tags</strong> to <strong>morphological features</strong>
(replacing the v2.x <code class="code_inline-code__Bq7ot">morph_rules</code> in the <a class="link_root__1Me7D" href="/usage/linguistic-features#language-data">language data</a>)</li>
<li class="list_li__sfx_z">specify the <strong>tags for space tokens</strong> (replacing hard-coded behavior in the
tagger)</li>
</ul><p>The following example shows how the tag and POS <code class="code_inline-code__Bq7ot">NNP</code>/<code class="code_inline-code__Bq7ot">PROPN</code> can be specified
for the phrase <code class="code_inline-code__Bq7ot">&quot;The Who&quot;</code>, overriding the tags provided by the statistical
tagger and the POS tag map.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Migrating from spaCy v2.x</span></h4><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/attributeruler"><code class="code_inline-code__Bq7ot">AttributeRuler</code></a> can import a <strong>tag map and morph
rules</strong> in the v2.x format via its built-in methods or when the component is
initialized before training. See the
<a class="link_root__1Me7D" href="/usage/v3#migrating-training-mappings-exceptions">migration guide</a> for details.</p></aside></section>
<section id="section-vectors-similarity" class="section_root__k1hUl"><h2 id="vectors-similarity" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#vectors-similarity" class="heading-text typography_permalink__UiIRy">Word vectors and semantic similarity <!-- --> </a></h2><p>Similarity is determined by comparing <strong>word vectors</strong> or “word embeddings”,
multi-dimensional meaning representations of a word. Word vectors can be
generated using an algorithm like
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a> and usually look like this:</p>
<pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">banana.vector</h4><code class="code_code__CILJL language-python language-python"></code></pre>
<aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>To make them compact and fast, spaCy’s small <a class="link_root__1Me7D" href="/models">pipeline packages</a> (all
packages that end in <code class="code_inline-code__Bq7ot">sm</code>) <strong>don’t ship with word vectors</strong>, and only include
context-sensitive <strong>tensors</strong>. This means you can still use the <code class="code_inline-code__Bq7ot">similarity()</code>
methods to compare documents, spans and tokens – but the result won’t be as
good, and individual tokens won’t have any vectors assigned. So in order to use
<em>real</em> word vectors, you need to download a larger pipeline package:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-diff language-diff"></code></pre></aside>
<p>Pipeline packages that come with built-in word vectors make them available as
the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#vector"><code class="code_inline-code__Bq7ot">Token.vector</code></a> attribute.
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc#vector"><code class="code_inline-code__Bq7ot">Doc.vector</code></a> and <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/span#vector"><code class="code_inline-code__Bq7ot">Span.vector</code></a> will
default to an average of their token vectors. You can also check if a token has
a vector assigned, and get the L2 norm, which can be used to normalize vectors.</p>
<pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre>
<aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><strong>Text</strong>: The original token text.</li>
<li class="list_li__sfx_z"><strong>has vector</strong>: Does the token have a vector representation?</li>
<li class="list_li__sfx_z"><strong>Vector norm</strong>: The L2 norm of the token’s vector (the square root of the
sum of the values squared)</li>
<li class="list_li__sfx_z"><strong>OOV</strong>: Out-of-vocabulary</li>
</ul>
</div></div></aside>
<p>The words “dog”, “cat” and “banana” are all pretty common in English, so they’re
part of the pipeline’s vocabulary, and come with a vector. The word “afskfsd” on
the other hand is a lot less common and out-of-vocabulary – so its vector
representation consists of 300 dimensions of <code class="code_inline-code__Bq7ot">0</code>, which means it’s practically
nonexistent. If your application will benefit from a <strong>large vocabulary</strong> with
more vectors, you should consider using one of the larger pipeline packages or
loading in a full vector package, for example,
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/models/en#en_core_web_lg"><code class="code_inline-code__Bq7ot">en_core_web_lg</code></a>, which includes <strong>685k unique
vectors</strong>.</p>
<p>spaCy is able to compare two objects, and make a prediction of <strong>how similar
they are</strong>. Predicting similarity is useful for building recommendation systems
or flagging duplicates. For example, you can suggest a user content that’s
similar to what they’re currently looking at, or label a support ticket as a
duplicate if it’s very similar to an already existing one.</p>
<p>Each <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a>, <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/span"><code class="code_inline-code__Bq7ot">Span</code></a>, <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token"><code class="code_inline-code__Bq7ot">Token</code></a> and
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/lexeme"><code class="code_inline-code__Bq7ot">Lexeme</code></a> comes with a <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/token#similarity"><code class="code_inline-code__Bq7ot">.similarity</code></a>
method that lets you compare it with another object, and determine the
similarity. Of course similarity is always subjective – whether two words, spans
or documents are similar really depends on how you’re looking at it. spaCy’s
similarity implementation usually assumes a pretty general-purpose definition of
similarity.</p>
<aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">📝 Things to try<!-- --> </span></h4>
<ol class="list_ol__aclSa">
<li class="list_li__sfx_z">Compare two different tokens and try to find the two most <em>dissimilar</em>
tokens in the texts with the lowest similarity score (according to the
vectors).</li>
<li class="list_li__sfx_z">Compare the similarity of two <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/lexeme"><code class="code_inline-code__Bq7ot">Lexeme</code></a> objects, entries in
the vocabulary. You can get a lexeme via the <code class="code_inline-code__Bq7ot">.lex</code> attribute of a token.
You should see that the similarity results are identical to the token
similarity.</li>
</ol>
</div></div></aside>
<pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre>
<h3 id="similarity-expectations" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#similarity-expectations" class="heading-text typography_permalink__UiIRy">What to expect from similarity results <!-- --> </a></h3>
<p>Computing similarity scores can be helpful in many situations, but it’s also
important to maintain <strong>realistic expectations</strong> about what information it can
provide. Words can be related to each other in many ways, so a single
“similarity” score will always be a <strong>mix of different signals</strong>, and vectors
trained on different data can produce very different results that may not be
useful for your purpose. Here are some important considerations to keep in mind:</p>
<ul class="list_ul__fe_HF">
<li class="list_li__sfx_z">There’s no objective definition of similarity. Whether “I like burgers” and “I
like pasta” is similar <strong>depends on your application</strong>. Both talk about food
preferences, which makes them very similar – but if you’re analyzing mentions
of food, those sentences are pretty dissimilar, because they talk about very
different foods.</li>
<li class="list_li__sfx_z">The similarity of <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> and <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/span"><code class="code_inline-code__Bq7ot">Span</code></a> objects defaults
to the <strong>average</strong> of the token vectors. This means that the vector for “fast
food” is the average of the vectors for “fast” and “food”, which isn’t
necessarily representative of the phrase “fast food”.</li>
<li class="list_li__sfx_z">Vector averaging means that the vector of multiple tokens is <strong>insensitive to
the order</strong> of the words. Two documents expressing the same meaning with
dissimilar wording will return a lower similarity score than two documents
that happen to contain the same words while expressing different meanings.</li>
</ul>
<aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">💡</span>Tip: Check out sense2vec</span></h4><figure class="gatsby-resp-image-figure"><a class="link_root__1Me7D gatsby-resp-image-link embed_image-link__loaAa link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/sense2vec"><img class="embed_image__mSQUH" src="/images/sense2vec.jpg" alt="sense2vec Screenshot" width="650" height="auto"/></a></figure><p><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/sense2vec"><code class="code_inline-code__Bq7ot">sense2vec</code></a> is a library developed by
us that builds on top of spaCy and lets you train and query more interesting and
detailed word vectors. It combines noun phrases like “fast food” or “fair game”
and includes the part-of-speech tags and entity labels. The library also
includes annotation recipes for our annotation tool <a class="link_root__1Me7D" href="https://prodi.gy">Prodigy</a>
that let you evaluate vectors and create terminology lists. For more details,
check out <a class="link_root__1Me7D" href="https://explosion.ai/blog/sense2vec-reloaded">our blog post</a>. To
explore the semantic similarities across all Reddit comments of 2015 and 2019,
see the <a class="link_root__1Me7D" href="https://explosion.ai/demos/sense2vec">interactive demo</a>.</p></aside><h3 id="adding-vectors" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#adding-vectors" class="heading-text typography_permalink__UiIRy">Adding word vectors <!-- --> </a></h3><p>Custom word vectors can be trained using a number of open-source libraries, such
as <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://radimrehurek.com/gensim">Gensim</a>, <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://fasttext.cc">FastText</a>,
or Tomas Mikolov’s original
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://code.google.com/archive/p/word2vec/">Word2vec implementation</a>. Most
word vector libraries output an easy-to-read text-based format, where each line
consists of the word followed by its vector. For everyday use, we want to
convert the vectors into a binary format that loads faster and takes up less
space on disk. The easiest way to do this is the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-vectors"><code class="code_inline-code__Bq7ot">init vectors</code></a> command-line utility. This will output a
blank spaCy pipeline in the directory <code class="code_inline-code__Bq7ot">/tmp/la_vectors_wiki_lg</code>, giving you
access to some nice Latin vectors. You can then pass the directory path to
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#spacy.load"><code class="code_inline-code__Bq7ot">spacy.load</code></a> or use it in the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/data-formats#config-initialize"><code class="code_inline-code__Bq7ot">[initialize]</code></a> of your config when you
<a class="link_root__1Me7D" href="/usage/training">train</a> a model.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Usage example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><section class="accordion" id="custom-vectors-coverage"><div class="accordion_root__pPltq accordion_spaced__Ebyjn"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">How to optimize vector coverage</span><a class="link_root__1Me7D accordion_anchor__kidBh link_no-link-layout__RPvod" href="/usage/linguistic-features#custom-vectors-coverage">¶</a></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><p>To help you strike a good balance between coverage and memory usage, spaCy’s
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/vectors"><code class="code_inline-code__Bq7ot">Vectors</code></a> class lets you map <strong>multiple keys</strong> to the <strong>same
row</strong> of the table. If you’re using the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-vectors"><code class="code_inline-code__Bq7ot">spacy init vectors</code></a> command to create a vocabulary,
pruning the vectors will be taken care of automatically if you set the <code class="code_inline-code__Bq7ot">--prune</code>
flag. You can also do it manually in the following steps:</p><ol class="list_ol__aclSa">
<li class="list_li__sfx_z">Start with a <strong>word vectors package</strong> that covers a huge vocabulary. For
instance, the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/models/en#en_core_web_lg"><code class="code_inline-code__Bq7ot">en_core_web_lg</code></a> package provides
300-dimensional GloVe vectors for 685k terms of English.</li>
<li class="list_li__sfx_z">If your vocabulary has values set for the <code class="code_inline-code__Bq7ot">Lexeme.prob</code> attribute, the
lexemes will be sorted by descending probability to determine which vectors
to prune. Otherwise, lexemes will be sorted by their order in the <code class="code_inline-code__Bq7ot">Vocab</code>.</li>
<li class="list_li__sfx_z">Call <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/vocab#prune_vectors"><code class="code_inline-code__Bq7ot">Vocab.prune_vectors</code></a> with the number of
vectors you want to keep.</li>
</ol><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/vocab#prune_vectors"><code class="code_inline-code__Bq7ot">Vocab.prune_vectors</code></a> reduces the current vector
table to a given number of unique entries, and returns a dictionary containing
the removed words, mapped to <code class="code_inline-code__Bq7ot">(string, score)</code> tuples, where <code class="code_inline-code__Bq7ot">string</code> is the
entry the removed word was mapped to and <code class="code_inline-code__Bq7ot">score</code> the similarity score between
the two words.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Removed words</h4><code class="code_code__CILJL language-python language-python"></code></pre><p>In the example above, the vector for “Shore” was removed and remapped to the
vector of “coast”, which is deemed about 73% similar. “Leaving” was remapped to
the vector of “leaving”, which is identical. If you’re using the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-vectors"><code class="code_inline-code__Bq7ot">init vectors</code></a> command, you can set the <code class="code_inline-code__Bq7ot">--prune</code>
option to easily reduce the size of the vectors as you add them to a spaCy
pipeline:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><p>This will create a blank spaCy pipeline with vectors for the first 10,000 words
in the vectors. All other words in the vectors are mapped to the closest vector
among those retained.</p></div></div></section><h3 id="adding-individual-vectors" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#adding-individual-vectors" class="heading-text typography_permalink__UiIRy">Adding vectors individually <!-- --> </a></h3><p>The <code class="code_inline-code__Bq7ot">vector</code> attribute is a <strong>read-only</strong> numpy or cupy array (depending on
whether you’ve configured spaCy to use GPU memory), with dtype <code class="code_inline-code__Bq7ot">float32</code>. The
array is read-only so that spaCy can avoid unnecessary copy operations where
possible. You can modify the vectors via the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/vocab"><code class="code_inline-code__Bq7ot">Vocab</code></a> or
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/vectors"><code class="code_inline-code__Bq7ot">Vectors</code></a> table. Using the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/vocab#set_vector"><code class="code_inline-code__Bq7ot">Vocab.set_vector</code></a> method is often the easiest approach
if you have vectors in an arbitrary format, as you can read in the vectors with
your own logic, and just set them with a simple loop. This method is likely to
be slower than approaches that work with the whole vectors table at once, but
it’s a great approach for once-off conversions before you save out your <code class="code_inline-code__Bq7ot">nlp</code>
object to disk.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Adding vectors</h4><code class="code_code__CILJL language-python language-python"></code></pre></section>
<section id="section-language-data" class="section_root__k1hUl"><h2 id="language-data" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#language-data" class="heading-text typography_permalink__UiIRy">Language Data <!-- --> </a></h2><p>Every language is different – and usually full of <strong>exceptions and special
cases</strong>, especially amongst the most common words. Some of these exceptions are
shared across languages, while others are <strong>entirely specific</strong> – usually so
specific that they need to be hard-coded. The
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/lang"><code class="code_inline-code__Bq7ot">lang</code></a> module contains all language-specific data,
organized in simple Python files. This makes the data easy to update and extend.</p>
<p>The <strong>shared language data</strong> in the directory root includes rules that can be
generalized across languages – for example, rules for basic punctuation, emoji,
emoticons and single-letter abbreviations. The <strong>individual language data</strong> in a
submodule contains rules that are only relevant to a particular language. It
also takes care of putting together all components and creating the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language"><code class="code_inline-code__Bq7ot">Language</code></a> subclass – for example, <code class="code_inline-code__Bq7ot">English</code> or <code class="code_inline-code__Bq7ot">German</code>. The
values are defined in the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#defaults"><code class="code_inline-code__Bq7ot">Language.Defaults</code></a>.</p>
<aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside>
<table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>Stop words</strong><br/><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/lang/en/stop_words.py"><code class="code_inline-code__Bq7ot">stop_words.py</code></a></td><td class="table_td__rmpJx">List of most common words of a language that are often useful to filter out, for example “and” or “I”. Matching tokens will return <code class="code_inline-code__Bq7ot">True</code> for <code class="code_inline-code__Bq7ot">is_stop</code>.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>Tokenizer exceptions</strong><br/><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/lang/de/tokenizer_exceptions.py"><code class="code_inline-code__Bq7ot">tokenizer_exceptions.py</code></a></td><td class="table_td__rmpJx">Special-case rules for the tokenizer, for example, contractions like “can’t” and abbreviations with punctuation, like “U.K.”.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>Punctuation rules</strong><br/><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/lang/punctuation.py"><code class="code_inline-code__Bq7ot">punctuation.py</code></a></td><td class="table_td__rmpJx">Regular expressions for splitting tokens, e.g. on punctuation or special characters like emoji. Includes rules for prefixes, suffixes and infixes.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>Character classes</strong><br/><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/lang/char_classes.py"><code class="code_inline-code__Bq7ot">char_classes.py</code></a></td><td class="table_td__rmpJx">Character classes to be used in regular expressions, for example, Latin characters, quotes, hyphens or icons.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>Lexical attributes</strong><br/><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/lang/en/lex_attrs.py"><code class="code_inline-code__Bq7ot">lex_attrs.py</code></a></td><td class="table_td__rmpJx">Custom functions for setting lexical attributes on tokens, e.g. <code class="code_inline-code__Bq7ot">like_num</code>, which includes language-specific words like “ten” or “hundred”.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>Syntax iterators</strong><br/><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/lang/en/syntax_iterators.py"><code class="code_inline-code__Bq7ot">syntax_iterators.py</code></a></td><td class="table_td__rmpJx">Functions that compute views of a <code class="code_inline-code__Bq7ot">Doc</code> object based on its syntax. At the moment, only used for <a class="link_root__1Me7D" href="/usage/linguistic-features#noun-chunks">noun chunks</a>.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>Lemmatizer</strong><br/><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/lang/fr/lemmatizer.py"><code class="code_inline-code__Bq7ot">lemmatizer.py</code></a> <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spacy-lookups-data"><code class="code_inline-code__Bq7ot">spacy-lookups-data</code></a></td><td class="table_td__rmpJx">Custom lemmatizer implementation and lemmatization tables.</td></tr></tbody></table><h3 id="language-subclass" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#language-subclass" class="heading-text typography_permalink__UiIRy">Creating a custom language subclass <!-- --> </a></h3><p>If you want to customize multiple components of the language data or add support
for a custom language or domain-specific “dialect”, you can also implement your
own language subclass. The subclass should define two attributes: the <code class="code_inline-code__Bq7ot">lang</code>
(unique language code) and the <code class="code_inline-code__Bq7ot">Defaults</code> defining the language data. For an
overview of the available attributes that can be overwritten, see the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#defaults"><code class="code_inline-code__Bq7ot">Language.Defaults</code></a> documentation.</p><pre class="code_pre__kzg60"><div class="code_juniper-wrapper__Vfpma"><h4 class="code_juniper-title__ePkNN">Editable Code<span class="code_juniper-meta__aRELD">spaCy v<!-- -->3.5<!-- --> · Python 3 · via<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://mybinder.org/">Binder</a></span></h4><div class="code_juniper-cell__XJBck"><div class="cm-theme code_juniper-input__HKv_l"></div><button class="code_juniper-button__k_2FS">run</button></div></div></pre><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#registry"><code class="code_inline-code__Bq7ot">@spacy.registry.languages</code></a> decorator lets you
register a custom language class and assign it a string name. This means that
you can call <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#spacy.blank"><code class="code_inline-code__Bq7ot">spacy.blank</code></a> with your custom
language name, and even train pipelines with it and refer to it in your
<a class="link_root__1Me7D" href="/usage/training#config">training config</a>.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Config usage<!-- --> </span></h4>
<p>After registering your custom language class using the <code class="code_inline-code__Bq7ot">languages</code> registry,
you can refer to it in your <a class="link_root__1Me7D" href="/usage/training#config">training config</a>. This
means spaCy will train your pipeline using the custom subclass.</p>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
<p>In order to resolve <code class="code_inline-code__Bq7ot">&quot;custom_en&quot;</code> to your subclass, the registered function
needs to be available during training. You can load a Python file containing
the code using the <code class="code_inline-code__Bq7ot">--code</code> argument:</p>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Registering a custom language</h4><code class="code_code__CILJL language-python language-python code_wrap__b41os"></code></pre></section><div class="grid_root__EfDZl grid_spacing__fhBCv grid_half__xoJZs"><div style="margin-top:var(--spacing-lg)"><a class="link_root__1Me7D button_root__jwipc button_secondary__ukZAk" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/website/docs/usage/linguistic-features.mdx">Suggest edits</a></div><a class="link_root__1Me7D readnext_root__JNzwZ link_no-link-layout__RPvod" href="/usage/rule-based-matching"><span><span class="typography_label__l_oVJ">Read next</span>Rule-based Matching</span><span class="readnext_icon__jfRnJ"></span></a></div></article><div class="main_asides__RITE5" style="background-image:url(/_next/static/media/pattern_blue.d167bed5.png"></div><footer class="footer_root__zlkjP"><div class="grid_root__EfDZl footer_content__LaE1F grid_narrow__x_6xS grid_spacing__fhBCv grid_third__edHuB"><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">spaCy</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/usage">Usage</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/models">Models</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/api">API Reference</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://course.spacy.io">Online Course</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/custom-solutions">Custom Solutions</a></li></ul></section><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Community</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/universe">Universe</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/discussions">GitHub Discussions</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/issues">Issue Tracker</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="http://stackoverflow.com/questions/tagged/spacy">Stack Overflow</a></li></ul></section><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Connect</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://twitter.com/spacy_io">Twitter</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy">GitHub</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://youtube.com/c/ExplosionAI">YouTube</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/blog">Blog</a></li></ul></section><section class="footer_full___icln"><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Stay in the loop!</li><li>Receive updates about new releases, tutorials and more.</li><li><form id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" action="//spacy.us12.list-manage.com/subscribe/post?u=83b0498b1e7fa3c91ce68c3f1&amp;amp;id=ecc82e0493" method="post" target="_blank" novalidate=""><div style="position:absolute;left:-5000px" aria-hidden="true"><input type="text" name="b_83b0498b1e7fa3c91ce68c3f1_ecc82e0493" tabindex="-1" value=""/></div><div class="newsletter_root__uh6MU"><input class="newsletter_input___SMSB" id="mce-EMAIL" type="email" name="EMAIL" placeholder="Your email" aria-label="Your email"/><button class="newsletter_button__gKW8E" id="mc-embedded-subscribe" type="submit" name="subscribe">Sign up</button></div></form></li></ul></section></div><div class="footer_content__LaE1F footer_copy__rbjvc"><span>© 2016-<!-- -->2023<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai">Explosion</a></span><a class="link_root__1Me7D footer_logo__BthsJ link_no-link-layout__RPvod" aria-label="Explosion" href="https://explosion.ai"></a><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/legal">Legal / Imprint</a></div></footer></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"title":"Linguistic Features","next":{"slug":"/usage/rule-based-matching","title":"Rule-based Matching"},"menu":[["POS Tagging","pos-tagging"],["Morphology","morphology"],["Lemmatization","lemmatization"],["Dependency Parse","dependency-parse"],["Named Entities","named-entities"],["Entity Linking","entity-linking"],["Tokenization","tokenization"],["Merging \u0026 Splitting","retokenization"],["Sentence Segmentation","sbd"],["Mappings \u0026 Exceptions","mappings-exceptions"],["Vectors \u0026 Similarity","vectors-similarity"],["Language Data","language-data"]],"slug":"/usage/linguistic-features","mdx":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    section: \"section\",\n    p: \"p\",\n    a: \"a\",\n    h2: \"h2\",\n    strong: \"strong\",\n    table: \"table\",\n    thead: \"thead\",\n    tr: \"tr\",\n    th: \"th\",\n    tbody: \"tbody\",\n    td: \"td\",\n    blockquote: \"blockquote\",\n    h4: \"h4\",\n    ol: \"ol\",\n    li: \"li\",\n    pre: \"pre\",\n    code: \"code\",\n    h3: \"h3\",\n    em: \"em\",\n    ul: \"ul\",\n    hr: \"hr\",\n    img: \"img\"\n  }, _provideComponents(), props.components), {InlineCode, PosDeps101, Infobox, Tag, Iframe, NER101, Tokenization101, Accordion, Vectors101, LanguageData101} = _components;\n  if (!Accordion) _missingMdxReference(\"Accordion\", true);\n  if (!Iframe) _missingMdxReference(\"Iframe\", true);\n  if (!Infobox) _missingMdxReference(\"Infobox\", true);\n  if (!InlineCode) _missingMdxReference(\"InlineCode\", true);\n  if (!LanguageData101) _missingMdxReference(\"LanguageData101\", true);\n  if (!NER101) _missingMdxReference(\"NER101\", true);\n  if (!PosDeps101) _missingMdxReference(\"PosDeps101\", true);\n  if (!Tag) _missingMdxReference(\"Tag\", true);\n  if (!Tokenization101) _missingMdxReference(\"Tokenization101\", true);\n  if (!Vectors101) _missingMdxReference(\"Vectors101\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.section, {\n      children: _jsxs(_components.p, {\n        children: [\"Processing raw text intelligently is difficult: most words are rare, and it’s\\ncommon for words that look completely different to mean almost the same thing.\\nThe same words in a different order can mean something completely different.\\nEven splitting text into useful word-like units can be difficult in many\\nlanguages. While it’s possible to solve some problems starting from only the raw\\ncharacters, it’s usually better to use linguistic knowledge to add useful\\ninformation. That’s exactly what spaCy is designed to do: you put in raw text,\\nand get back a \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" object, that comes with a variety of\\nannotations.\"]\n      })\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-pos-tagging\",\n      children: [_jsx(_components.h2, {\n        id: \"pos-tagging\",\n        model: \"tagger, parser\",\n        children: \"Part-of-speech tagging \"\n      }), _jsx(PosDeps101, {}), _jsx(Infobox, {\n        title: \"Part-of-speech tag scheme\",\n        emoji: \"📖\",\n        children: _jsxs(_components.p, {\n          children: [\"For a list of the fine-grained and coarse-grained part-of-speech tags assigned\\nby spaCy’s models across different languages, see the label schemes documented\\nin the \", _jsx(_components.a, {\n            href: \"/models\",\n            children: \"models directory\"\n          }), \".\"]\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-morphology\",\n      children: [_jsx(_components.h2, {\n        id: \"morphology\",\n        children: \"Morphology \"\n      }), _jsxs(_components.p, {\n        children: [\"Inflectional morphology is the process by which a root form of a word is\\nmodified by adding prefixes or suffixes that specify its grammatical function\\nbut do not change its part-of-speech. We say that a \", _jsx(_components.strong, {\n          children: \"lemma\"\n        }), \" (root form) is\\n\", _jsx(_components.strong, {\n          children: \"inflected\"\n        }), \" (modified/combined) with one or more \", _jsx(_components.strong, {\n          children: \"morphological features\"\n        }), \" to\\ncreate a surface form. Here are some examples:\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Context\"\n            }), _jsx(_components.th, {\n              children: \"Surface\"\n            }), _jsx(_components.th, {\n              children: \"Lemma\"\n            }), _jsx(_components.th, {\n              children: \"POS\"\n            }), _jsx(_components.th, {\n              children: \"Morphological Features\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"I was reading the paper\"\n            }), _jsx(_components.td, {\n              children: \"reading\"\n            }), _jsx(_components.td, {\n              children: \"read\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VerbForm=Ger\"\n              })\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"I don’t watch the news, I read the paper\"\n            }), _jsx(_components.td, {\n              children: \"read\"\n            }), _jsx(_components.td, {\n              children: \"read\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"VerbForm=Fin\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"Mood=Ind\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"Tense=Pres\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"I read the paper yesterday\"\n            }), _jsx(_components.td, {\n              children: \"read\"\n            }), _jsx(_components.td, {\n              children: \"read\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"VerbForm=Fin\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"Mood=Ind\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"Tense=Past\"\n              })]\n            })]\n          })]\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"Morphological features are stored in the\\n\", _jsx(_components.a, {\n          href: \"/api/morphology#morphanalysis\",\n          children: _jsx(InlineCode, {\n            children: \"MorphAnalysis\"\n          })\n        }), \" under \", _jsx(InlineCode, {\n          children: \"Token.morph\"\n        }), \", which\\nallows you to access individual morphological features.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"📝 Things to try\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"Change “I” to “She”. You should see that the morphological features change\\nand express that it’s a pronoun in the third person.\"\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Inspect \", _jsx(InlineCode, {\n              children: \"token.morph\"\n            }), \" for the other tokens.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nprint(\\\"Pipeline:\\\", nlp.pipe_names)\\ndoc = nlp(\\\"I was reading the paper.\\\")\\ntoken = doc[0]  # 'I'\\nprint(token.morph)  # 'Case=Nom|Number=Sing|Person=1|PronType=Prs'\\nprint(token.morph.get(\\\"PronType\\\"))  # ['Prs']\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"morphologizer\",\n        version: \"3\",\n        model: \"morphologizer\",\n        children: \"Statistical morphology \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy’s statistical \", _jsx(_components.a, {\n          href: \"/api/morphologizer\",\n          children: _jsx(InlineCode, {\n            children: \"Morphologizer\"\n          })\n        }), \" component assigns the\\nmorphological features and coarse-grained part-of-speech tags as \", _jsx(InlineCode, {\n          children: \"Token.morph\"\n        }), \"\\nand \", _jsx(InlineCode, {\n          children: \"Token.pos\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"de_core_news_sm\\\")\\ndoc = nlp(\\\"Wo bist du?\\\") # English: 'Where are you?'\\nprint(doc[2].morph)  # 'Case=Nom|Number=Sing|Person=2|PronType=Prs'\\nprint(doc[2].pos_) # 'PRON'\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"rule-based-morphology\",\n        children: \"Rule-based morphology \"\n      }), _jsxs(_components.p, {\n        children: [\"For languages with relatively simple morphological systems like English, spaCy\\ncan assign morphological features through a rule-based approach, which uses the\\n\", _jsx(_components.strong, {\n          children: \"token text\"\n        }), \" and \", _jsx(_components.strong, {\n          children: \"fine-grained part-of-speech tags\"\n        }), \" to produce\\ncoarse-grained part-of-speech tags and morphological features.\"]\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"The part-of-speech tagger assigns each token a \", _jsx(_components.strong, {\n            children: \"fine-grained part-of-speech\\ntag\"\n          }), \". In the API, these tags are known as \", _jsx(InlineCode, {\n            children: \"Token.tag\"\n          }), \". They express the\\npart-of-speech (e.g. verb) and some amount of morphological information, e.g.\\nthat the verb is past tense (e.g. \", _jsx(InlineCode, {\n            children: \"VBD\"\n          }), \" for a past tense verb in the Penn\\nTreebank) .\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"For words whose coarse-grained POS is not set by a prior process, a\\n\", _jsx(_components.a, {\n            href: \"#mappings-exceptions\",\n            children: \"mapping table\"\n          }), \" maps the fine-grained tags to a\\ncoarse-grained POS tags and morphological features.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Where are you?\\\")\\nprint(doc[2].morph)  # 'Case=Nom|Person=2|PronType=Prs'\\nprint(doc[2].pos_)  # 'PRON'\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-lemmatization\",\n      children: [_jsx(_components.h2, {\n        id: \"lemmatization\",\n        model: \"lemmatizer\",\n        version: \"3\",\n        children: \"Lemmatization \"\n      }), _jsx(_components.p, {\n        children: \"spaCy provides two pipeline components for lemmatization:\"\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"The \", _jsx(_components.a, {\n            href: \"/api/lemmatizer\",\n            children: _jsx(InlineCode, {\n              children: \"Lemmatizer\"\n            })\n          }), \" component provides lookup and rule-based\\nlemmatization methods in a configurable component. An individual language can\\nextend the \", _jsx(InlineCode, {\n            children: \"Lemmatizer\"\n          }), \" as part of its \", _jsx(_components.a, {\n            href: \"#language-data\",\n            children: \"language data\"\n          }), \".\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"The \", _jsx(_components.a, {\n            href: \"/api/edittreelemmatizer\",\n            children: _jsx(InlineCode, {\n              children: \"EditTreeLemmatizer\"\n            })\n          }), \"\\n\", _jsx(Tag, {\n            variant: \"new\",\n            children: \"3.3\"\n          }), \" component provides a trainable lemmatizer.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\n# English pipelines include a rule-based lemmatizer\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nlemmatizer = nlp.get_pipe(\\\"lemmatizer\\\")\\nprint(lemmatizer.mode)  # 'rule'\\n\\ndoc = nlp(\\\"I was reading the paper.\\\")\\nprint([token.lemma_ for token in doc])\\n# ['I', 'be', 'read', 'the', 'paper', '.']\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Changed in v3.0\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Unlike spaCy v2, spaCy v3 models do \", _jsx(_components.em, {\n            children: \"not\"\n          }), \" provide lemmas by default or switch\\nautomatically between lookup and rule-based lemmas depending on whether a tagger\\nis in the pipeline. To have lemmas in a \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \", the pipeline needs to include a\\n\", _jsx(_components.a, {\n            href: \"/api/lemmatizer\",\n            children: _jsx(InlineCode, {\n              children: \"Lemmatizer\"\n            })\n          }), \" component. The lemmatizer component is\\nconfigured to use a single mode such as \", _jsx(InlineCode, {\n            children: \"\\\"lookup\\\"\"\n          }), \" or \", _jsx(InlineCode, {\n            children: \"\\\"rule\\\"\"\n          }), \" on\\ninitialization. The \", _jsx(InlineCode, {\n            children: \"\\\"rule\\\"\"\n          }), \" mode requires \", _jsx(InlineCode, {\n            children: \"Token.pos\"\n          }), \" to be set by a previous\\ncomponent.\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The data for spaCy’s lemmatizers is distributed in the package\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spacy-lookups-data\",\n          children: _jsx(InlineCode, {\n            children: \"spacy-lookups-data\"\n          })\n        }), \". The\\nprovided trained pipelines already include all the required tables, but if you\\nare creating new pipelines, you’ll probably want to install \", _jsx(InlineCode, {\n          children: \"spacy-lookups-data\"\n        }), \"\\nto provide the data when the lemmatizer is initialized.\"]\n      }), _jsx(_components.h3, {\n        id: \"lemmatizer-lookup\",\n        children: \"Lookup lemmatizer \"\n      }), _jsxs(_components.p, {\n        children: [\"For pipelines without a tagger or morphologizer, a lookup lemmatizer can be\\nadded to the pipeline as long as a lookup table is provided, typically through\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spacy-lookups-data\",\n          children: _jsx(InlineCode, {\n            children: \"spacy-lookups-data\"\n          })\n        }), \". The\\nlookup lemmatizer looks up the token surface form in the lookup table without\\nreference to the token’s part-of-speech or context.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# pip install -U spacy[lookups]\\nimport spacy\\n\\nnlp = spacy.blank(\\\"sv\\\")\\nnlp.add_pipe(\\\"lemmatizer\\\", config={\\\"mode\\\": \\\"lookup\\\"})\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"lemmatizer-rule\",\n        children: \"Rule-based lemmatizer \"\n      }), _jsxs(_components.p, {\n        children: [\"When training pipelines that include a component that assigns part-of-speech\\ntags (a morphologizer or a tagger with a \", _jsx(_components.a, {\n          href: \"#mappings-exceptions\",\n          children: \"POS mapping\"\n        }), \"), a\\nrule-based lemmatizer can be added using rule tables from\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spacy-lookups-data\",\n          children: _jsx(InlineCode, {\n            children: \"spacy-lookups-data\"\n          })\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# pip install -U spacy[lookups]\\nimport spacy\\n\\nnlp = spacy.blank(\\\"de\\\")\\n# Morphologizer (note: model is not yet trained!)\\nnlp.add_pipe(\\\"morphologizer\\\")\\n# Rule-based lemmatizer\\nnlp.add_pipe(\\\"lemmatizer\\\", config={\\\"mode\\\": \\\"rule\\\"})\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The rule-based deterministic lemmatizer maps the surface form to a lemma in\\nlight of the previously assigned coarse-grained part-of-speech and morphological\\ninformation, without consulting the context of the token. The rule-based\\nlemmatizer also accepts list-based exception files. For English, these are\\nacquired from \", _jsx(_components.a, {\n          href: \"https://wordnet.princeton.edu/\",\n          children: \"WordNet\"\n        }), \".\"]\n      }), _jsx(_components.h3, {\n        children: \"Trainable lemmatizer\"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/edittreelemmatizer\",\n          children: _jsx(InlineCode, {\n            children: \"EditTreeLemmatizer\"\n          })\n        }), \" can learn form-to-lemma\\ntransformations from a training corpus that includes lemma annotations. This\\nremoves the need to write language-specific rules and can (in many cases)\\nprovide higher accuracies than lookup and rule-based lemmatizers.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"import spacy\\n\\nnlp = spacy.blank(\\\"de\\\")\\nnlp.add_pipe(\\\"trainable_lemmatizer\\\", name=\\\"lemmatizer\\\")\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-dependency-parse\",\n      children: [_jsx(_components.h2, {\n        id: \"dependency-parse\",\n        model: \"parser\",\n        children: \"Dependency Parsing \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy features a fast and accurate syntactic dependency parser, and has a rich\\nAPI for navigating the tree. The parser also powers the sentence boundary\\ndetection, and lets you iterate over base noun phrases, or “chunks”. You can\\ncheck whether a \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" object has been parsed by calling\\n\", _jsx(InlineCode, {\n          children: \"doc.has_annotation(\\\"DEP\\\")\"\n        }), \", which checks whether the attribute \", _jsx(InlineCode, {\n          children: \"Token.dep\"\n        }), \" has\\nbeen set returns a boolean value. If the result is \", _jsx(InlineCode, {\n          children: \"False\"\n        }), \", the default sentence\\niterator will raise an exception.\"]\n      }), _jsx(Infobox, {\n        title: \"Dependency label scheme\",\n        emoji: \"📖\",\n        children: _jsxs(_components.p, {\n          children: [\"For a list of the syntactic dependency labels assigned by spaCy’s models across\\ndifferent languages, see the label schemes documented in the\\n\", _jsx(_components.a, {\n            href: \"/models\",\n            children: \"models directory\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"noun-chunks\",\n        children: \"Noun chunks \"\n      }), _jsxs(_components.p, {\n        children: [\"Noun chunks are “base noun phrases” – flat phrases that have a noun as their\\nhead. You can think of noun chunks as a noun plus the words describing the noun\\n– for example, “the lavish green grass” or “the world’s largest tech fund”. To\\nget the noun chunks in a document, simply iterate over\\n\", _jsx(_components.a, {\n          href: \"/api/doc#noun_chunks\",\n          children: _jsx(InlineCode, {\n            children: \"Doc.noun_chunks\"\n          })\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Autonomous cars shift insurance liability toward manufacturers\\\")\\nfor chunk in doc.noun_chunks:\\n    print(chunk.text, chunk.root.text, chunk.root.dep_,\\n            chunk.root.head.text)\\n\"\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Text:\"\n            }), \" The original noun chunk text.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Root text:\"\n            }), \" The original text of the word connecting the noun chunk to\\nthe rest of the parse.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Root dep:\"\n            }), \" Dependency relation connecting the root to its head.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Root head text:\"\n            }), \" The text of the root token’s head.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Text\"\n            }), _jsx(_components.th, {\n              children: \"root.text\"\n            }), _jsx(_components.th, {\n              children: \"root.dep_\"\n            }), _jsx(_components.th, {\n              children: \"root.head.text\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Autonomous cars\"\n            }), _jsx(_components.td, {\n              children: \"cars\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nsubj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"shift\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"insurance liability\"\n            }), _jsx(_components.td, {\n              children: \"liability\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"dobj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"shift\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"manufacturers\"\n            }), _jsx(_components.td, {\n              children: \"manufacturers\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"pobj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"toward\"\n            })]\n          })]\n        })]\n      }), _jsx(_components.h3, {\n        id: \"navigating\",\n        children: \"Navigating the parse tree \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy uses the terms \", _jsx(_components.strong, {\n          children: \"head\"\n        }), \" and \", _jsx(_components.strong, {\n          children: \"child\"\n        }), \" to describe the words \", _jsx(_components.strong, {\n          children: \"connected by\\na single arc\"\n        }), \" in the dependency tree. The term \", _jsx(_components.strong, {\n          children: \"dep\"\n        }), \" is used for the arc\\nlabel, which describes the type of syntactic relation that connects the child to\\nthe head. As with other attributes, the value of \", _jsx(InlineCode, {\n          children: \".dep\"\n        }), \" is a hash value. You can\\nget the string value with \", _jsx(InlineCode, {\n          children: \".dep_\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Autonomous cars shift insurance liability toward manufacturers\\\")\\nfor token in doc:\\n    print(token.text, token.dep_, token.head.text, token.head.pos_,\\n            [child for child in token.children])\\n\"\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Text:\"\n            }), \" The original token text.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Dep:\"\n            }), \" The syntactic relation connecting child to head.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Head text:\"\n            }), \" The original text of the token head.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Head POS:\"\n            }), \" The part-of-speech tag of the token head.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Children:\"\n            }), \" The immediate syntactic dependents of the token.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Text\"\n            }), _jsx(_components.th, {\n              children: \"Dep\"\n            }), _jsx(_components.th, {\n              children: \"Head text\"\n            }), _jsx(_components.th, {\n              children: \"Head POS\"\n            }), _jsx(_components.th, {\n              children: \"Children\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Autonomous\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"amod\"\n              })\n            }), _jsx(_components.td, {\n              children: \"cars\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NOUN\"\n              })\n            }), _jsx(_components.td, {})]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"cars\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nsubj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"shift\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Autonomous\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"shift\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"ROOT\"\n              })\n            }), _jsx(_components.td, {\n              children: \"shift\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsx(_components.td, {\n              children: \"cars, liability, toward\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"insurance\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"compound\"\n              })\n            }), _jsx(_components.td, {\n              children: \"liability\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NOUN\"\n              })\n            }), _jsx(_components.td, {})]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"liability\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"dobj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"shift\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsx(_components.td, {\n              children: \"insurance\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"toward\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"prep\"\n              })\n            }), _jsx(_components.td, {\n              children: \"shift\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NOUN\"\n              })\n            }), _jsx(_components.td, {\n              children: \"manufacturers\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"manufacturers\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"pobj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"toward\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"ADP\"\n              })\n            }), _jsx(_components.td, {})]\n          })]\n        })]\n      }), _jsx(Iframe, {\n        title: \"displaCy visualization of dependencies and entities 2\",\n        src: \"/images/displacy-long2.html\",\n        height: 450\n      }), _jsxs(_components.p, {\n        children: [\"Because the syntactic relations form a tree, every word has \", _jsx(_components.strong, {\n          children: \"exactly one\\nhead\"\n        }), \". You can therefore iterate over the arcs in the tree by iterating over\\nthe words in the sentence. This is usually the best way to match an arc of\\ninterest – from below:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.symbols import nsubj, VERB\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Autonomous cars shift insurance liability toward manufacturers\\\")\\n\\n# Finding a verb with a subject from below — good\\nverbs = set()\\nfor possible_subject in doc:\\n    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\\n        verbs.add(possible_subject.head)\\nprint(verbs)\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"If you try to match from above, you’ll have to iterate twice. Once for the head,\\nand then again through the children:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# Finding a verb with a subject from above — less good\\nverbs = []\\nfor possible_verb in doc:\\n    if possible_verb.pos == VERB:\\n        for possible_subject in possible_verb.children:\\n            if possible_subject.dep == nsubj:\\n                verbs.append(possible_verb)\\n                break\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"To iterate through the children, use the \", _jsx(InlineCode, {\n          children: \"token.children\"\n        }), \" attribute, which\\nprovides a sequence of \", _jsx(_components.a, {\n          href: \"/api/token\",\n          children: _jsx(InlineCode, {\n            children: \"Token\"\n          })\n        }), \" objects.\"]\n      }), _jsx(_components.h4, {\n        id: \"navigating-around\",\n        children: \"Iterating around the local tree \"\n      }), _jsxs(_components.p, {\n        children: [\"A few more convenience attributes are provided for iterating around the local\\ntree from the token. \", _jsx(_components.a, {\n          href: \"/api/token#lefts\",\n          children: _jsx(InlineCode, {\n            children: \"Token.lefts\"\n          })\n        }), \" and\\n\", _jsx(_components.a, {\n          href: \"/api/token#rights\",\n          children: _jsx(InlineCode, {\n            children: \"Token.rights\"\n          })\n        }), \" attributes provide sequences of syntactic\\nchildren that occur before and after the token. Both sequences are in sentence\\norder. There are also two integer-typed attributes,\\n\", _jsx(_components.a, {\n          href: \"/api/token#n_lefts\",\n          children: _jsx(InlineCode, {\n            children: \"Token.n_lefts\"\n          })\n        }), \" and\\n\", _jsx(_components.a, {\n          href: \"/api/token#n_rights\",\n          children: _jsx(InlineCode, {\n            children: \"Token.n_rights\"\n          })\n        }), \" that give the number of left and right\\nchildren.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"bright red apples on the tree\\\")\\nprint([token.text for token in doc[2].lefts])  # ['bright', 'red']\\nprint([token.text for token in doc[2].rights])  # ['on']\\nprint(doc[2].n_lefts)  # 2\\nprint(doc[2].n_rights)  # 1\\n\"\n        })\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"de_core_news_sm\\\")\\ndoc = nlp(\\\"schöne rote Äpfel auf dem Baum\\\")\\nprint([token.text for token in doc[2].lefts])  # ['schöne', 'rote']\\nprint([token.text for token in doc[2].rights])  # ['auf']\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"You can get a whole phrase by its syntactic head using the\\n\", _jsx(_components.a, {\n          href: \"/api/token#subtree\",\n          children: _jsx(InlineCode, {\n            children: \"Token.subtree\"\n          })\n        }), \" attribute. This returns an ordered\\nsequence of tokens. You can walk up the tree with the\\n\", _jsx(_components.a, {\n          href: \"/api/token#ancestors\",\n          children: _jsx(InlineCode, {\n            children: \"Token.ancestors\"\n          })\n        }), \" attribute, and check dominance with\\n\", _jsx(_components.a, {\n          href: \"/api/token#is_ancestor\",\n          children: _jsx(InlineCode, {\n            children: \"Token.is_ancestor\"\n          })\n        })]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Projective vs. non-projective\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"For the \", _jsx(_components.a, {\n            href: \"/models/en\",\n            children: \"default English pipelines\"\n          }), \", the parse tree is\\n\", _jsx(_components.strong, {\n            children: \"projective\"\n          }), \", which means that there are no crossing brackets. The tokens\\nreturned by \", _jsx(InlineCode, {\n            children: \".subtree\"\n          }), \" are therefore guaranteed to be contiguous. This is not\\ntrue for the German pipelines, which have many\\n\", _jsx(_components.a, {\n            href: \"https://explosion.ai/blog/german-model#word-order\",\n            children: \"non-projective dependencies\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Credit and mortgage account holders must submit their requests\\\")\\n\\nroot = [token for token in doc if token.head == token][0]\\nsubject = list(root.lefts)[0]\\nfor descendant in subject.subtree:\\n    assert subject is descendant or subject.is_ancestor(descendant)\\n    print(descendant.text, descendant.dep_, descendant.n_lefts,\\n            descendant.n_rights,\\n            [ancestor.text for ancestor in descendant.ancestors])\\n\"\n        })\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Text\"\n            }), _jsx(_components.th, {\n              children: \"Dep\"\n            }), _jsx(_components.th, {\n              children: \"n_lefts\"\n            }), _jsx(_components.th, {\n              children: \"n_rights\"\n            }), _jsx(_components.th, {\n              children: \"ancestors\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Credit\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nmod\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"2\"\n              })\n            }), _jsx(_components.td, {\n              children: \"holders, submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"and\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"cc\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: \"holders, submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"mortgage\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"compound\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: \"account, Credit, holders, submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"account\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"conj\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"1\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Credit, holders, submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"holders\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nsubj\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"1\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: \"submit\"\n            })]\n          })]\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"Finally, the \", _jsx(InlineCode, {\n          children: \".left_edge\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \".right_edge\"\n        }), \" attributes can be especially useful,\\nbecause they give you the first and last token of the subtree. This is the\\neasiest way to create a \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" object for a syntactic phrase. Note that\\n\", _jsx(InlineCode, {\n          children: \".right_edge\"\n        }), \" gives a token \", _jsx(_components.strong, {\n          children: \"within\"\n        }), \" the subtree – so if you use it as the\\nend-point of a range, don’t forget to \", _jsx(InlineCode, {\n          children: \"+1\"\n        }), \"!\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Credit and mortgage account holders must submit their requests\\\")\\nspan = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]\\nwith doc.retokenize() as retokenizer:\\n    retokenizer.merge(span)\\nfor token in doc:\\n    print(token.text, token.pos_, token.dep_, token.head.text)\\n\"\n        })\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Text\"\n            }), _jsx(_components.th, {\n              children: \"POS\"\n            }), _jsx(_components.th, {\n              children: \"Dep\"\n            }), _jsx(_components.th, {\n              children: \"Head text\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Credit and mortgage account holders\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NOUN\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nsubj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"must\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"aux\"\n              })\n            }), _jsx(_components.td, {\n              children: \"submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"submit\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"ROOT\"\n              })\n            }), _jsx(_components.td, {\n              children: \"submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"their\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"ADJ\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"poss\"\n              })\n            }), _jsx(_components.td, {\n              children: \"requests\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"requests\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NOUN\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"dobj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"submit\"\n            })]\n          })]\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"The dependency parse can be a useful tool for \", _jsx(_components.strong, {\n          children: \"information extraction\"\n        }), \",\\nespecially when combined with other predictions like\\n\", _jsx(_components.a, {\n          href: \"#named-entities\",\n          children: \"named entities\"\n        }), \". The following example extracts money and\\ncurrency values, i.e. entities labeled as \", _jsx(InlineCode, {\n          children: \"MONEY\"\n        }), \", and then uses the dependency\\nparse to find the noun phrase they are referring to – for example \", _jsx(InlineCode, {\n          children: \"\\\"Net income\\\"\"\n        }), \"\\n→ \", _jsx(InlineCode, {\n          children: \"\\\"$9.4 million\\\"\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\n# Merge noun phrases and entities for easier analysis\\nnlp.add_pipe(\\\"merge_entities\\\")\\nnlp.add_pipe(\\\"merge_noun_chunks\\\")\\n\\nTEXTS = [\\n    \\\"Net income was $9.4 million compared to the prior year of $2.7 million.\\\",\\n    \\\"Revenue exceeded twelve billion dollars, with a loss of $1b.\\\",\\n]\\nfor doc in nlp.pipe(TEXTS):\\n    for token in doc:\\n        if token.ent_type_ == \\\"MONEY\\\":\\n            # We have an attribute and direct object, so check for subject\\n            if token.dep_ in (\\\"attr\\\", \\\"dobj\\\"):\\n                subj = [w for w in token.head.lefts if w.dep_ == \\\"nsubj\\\"]\\n                if subj:\\n                    print(subj[0], \\\"--\u003e\\\", token)\\n            # We have a prepositional object with a preposition\\n            elif token.dep_ == \\\"pobj\\\" and token.head.dep_ == \\\"prep\\\":\\n                print(token.head.head, \\\"--\u003e\\\", token)\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Combining models and rules\",\n        emoji: \"📖\",\n        children: _jsxs(_components.p, {\n          children: [\"For more examples of how to write rule-based information extraction logic that\\ntakes advantage of the model’s predictions produced by the different components,\\nsee the usage guide on\\n\", _jsx(_components.a, {\n            href: \"/usage/rule-based-matching#models-rules\",\n            children: \"combining models and rules\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"displacy\",\n        children: \"Visualizing dependencies \"\n      }), _jsxs(_components.p, {\n        children: [\"The best way to understand spaCy’s dependency parser is interactively. To make\\nthis easier, spaCy comes with a visualization module. You can pass a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" or a\\nlist of \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" objects to displaCy and run\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#displacy.serve\",\n          children: _jsx(InlineCode, {\n            children: \"displacy.serve\"\n          })\n        }), \" to run the web server, or\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#displacy.render\",\n          children: _jsx(InlineCode, {\n            children: \"displacy.render\"\n          })\n        }), \" to generate the raw markup.\\nIf you want to know how to write rules that hook into some type of syntactic\\nconstruction, just plug the sentence into the visualizer and see how spaCy\\nannotates it.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy import displacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Autonomous cars shift insurance liability toward manufacturers\\\")\\n# Since this is an interactive Jupyter environment, we can use displacy.render here\\ndisplacy.render(doc, style='dep')\\n\"\n        })\n      }), _jsx(Infobox, {\n        children: _jsxs(_components.p, {\n          children: [\"For more details and examples, see the\\n\", _jsx(_components.a, {\n            href: \"/usage/visualizers\",\n            children: \"usage guide on visualizing spaCy\"\n          }), \". You can also test\\ndisplaCy in our \", _jsx(_components.a, {\n            href: \"https://explosion.ai/demos/displacy\",\n            children: \"online demo\"\n          }), \"..\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"disabling\",\n        children: \"Disabling the parser \"\n      }), _jsxs(_components.p, {\n        children: [\"In the \", _jsx(_components.a, {\n          href: \"/models\",\n          children: \"trained pipelines\"\n        }), \" provided by spaCy, the parser is loaded and\\nenabled by default as part of the\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines\",\n          children: \"standard processing pipeline\"\n        }), \". If you don’t need\\nany of the syntactic information, you should disable the parser. Disabling the\\nparser will make spaCy load and run much faster. If you want to load the parser,\\nbut need to disable it for specific documents, you can also control its use on\\nthe \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object. For more details, see the usage guide on\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines/#disabling\",\n          children: \"disabling pipeline components\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"nlp = spacy.load(\\\"en_core_web_sm\\\", disable=[\\\"parser\\\"])\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-named-entities\",\n      children: [_jsx(_components.h2, {\n        id: \"named-entities\",\n        children: \"Named Entity Recognition \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy features an extremely fast statistical entity recognition system, that\\nassigns labels to contiguous spans of tokens. The default\\n\", _jsx(_components.a, {\n          href: \"/models\",\n          children: \"trained pipelines\"\n        }), \" can identify a variety of named and numeric\\nentities, including companies, locations, organizations and products. You can\\nadd arbitrary classes to the entity recognition system, and update the model\\nwith new examples.\"]\n      }), _jsx(_components.h3, {\n        id: \"named-entities-101\",\n        children: \"Named Entity Recognition 101 \"\n      }), _jsx(NER101, {}), _jsx(_components.h3, {\n        id: \"accessing-ner\",\n        children: \"Accessing entity annotations and labels \"\n      }), _jsxs(_components.p, {\n        children: [\"The standard way to access entity annotations is the \", _jsx(_components.a, {\n          href: \"/api/doc#ents\",\n          children: _jsx(InlineCode, {\n            children: \"doc.ents\"\n          })\n        }), \"\\nproperty, which produces a sequence of \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \" objects. The entity\\ntype is accessible either as a hash value or as a string, using the attributes\\n\", _jsx(InlineCode, {\n          children: \"ent.label\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"ent.label_\"\n        }), \". The \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" object acts as a sequence of tokens, so\\nyou can iterate over the entity or index into it. You can also get the text form\\nof the whole entity, as though it were a single token.\"]\n      }), _jsxs(_components.p, {\n        children: [\"You can also access token entity annotations using the\\n\", _jsx(_components.a, {\n          href: \"/api/token#attributes\",\n          children: _jsx(InlineCode, {\n            children: \"token.ent_iob\"\n          })\n        }), \" and\\n\", _jsx(_components.a, {\n          href: \"/api/token#attributes\",\n          children: _jsx(InlineCode, {\n            children: \"token.ent_type\"\n          })\n        }), \" attributes. \", _jsx(InlineCode, {\n          children: \"token.ent_iob\"\n        }), \" indicates\\nwhether an entity starts, continues or ends on the tag. If no entity type is set\\non a token, it will return an empty string.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"IOB Scheme\"\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"I\"\n            }), \" – Token is \", _jsx(_components.strong, {\n              children: \"inside\"\n            }), \" an entity.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"O\"\n            }), \" – Token is \", _jsx(_components.strong, {\n              children: \"outside\"\n            }), \" an entity.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"B\"\n            }), \" – Token is the \", _jsx(_components.strong, {\n              children: \"beginning\"\n            }), \" of an entity.\"]\n          }), \"\\n\"]\n        }), \"\\n\", _jsx(_components.h4, {\n          children: \"BILUO Scheme\"\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"B\"\n            }), \" – Token is the \", _jsx(_components.strong, {\n              children: \"beginning\"\n            }), \" of a multi-token entity.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"I\"\n            }), \" – Token is \", _jsx(_components.strong, {\n              children: \"inside\"\n            }), \" a multi-token entity.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"L\"\n            }), \" – Token is the \", _jsx(_components.strong, {\n              children: \"last\"\n            }), \" token of a multi-token entity.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"U\"\n            }), \" – Token is a single-token \", _jsx(_components.strong, {\n              children: \"unit\"\n            }), \" entity.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"O\"\n            }), \" – Token is \", _jsx(_components.strong, {\n              children: \"outside\"\n            }), \" an entity.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"San Francisco considers banning sidewalk delivery robots\\\")\\n\\n# document level\\nents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\\nprint(ents)\\n\\n# token level\\nent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]\\nent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]\\nprint(ent_san)  # ['San', 'B', 'GPE']\\nprint(ent_francisco)  # ['Francisco', 'I', 'GPE']\\n\"\n        })\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Text\"\n            }), _jsx(_components.th, {\n              children: \"ent_iob\"\n            }), _jsx(_components.th, {\n              children: \"ent_iob_\"\n            }), _jsx(_components.th, {\n              children: \"ent_type_\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"San\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"3\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"B\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"GPE\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"beginning of an entity\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Francisco\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"1\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"I\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"GPE\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"inside an entity\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"considers\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"2\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"O\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"outside an entity\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"banning\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"2\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"O\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"outside an entity\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"sidewalk\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"2\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"O\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"outside an entity\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"delivery\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"2\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"O\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"outside an entity\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"robots\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"2\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"O\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"outside an entity\"\n            })]\n          })]\n        })]\n      }), _jsx(_components.h3, {\n        id: \"setting-entities\",\n        children: \"Setting entity annotations \"\n      }), _jsxs(_components.p, {\n        children: [\"To ensure that the sequence of token annotations remains consistent, you have to\\nset entity annotations \", _jsx(_components.strong, {\n          children: \"at the document level\"\n        }), \". However, you can’t write\\ndirectly to the \", _jsx(InlineCode, {\n          children: \"token.ent_iob\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"token.ent_type\"\n        }), \" attributes, so the easiest\\nway to set entities is to use the \", _jsx(_components.a, {\n          href: \"/api/doc#set_ents\",\n          children: _jsx(InlineCode, {\n            children: \"doc.set_ents\"\n          })\n        }), \" function\\nand create the new entity as a \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.tokens import Span\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"fb is hiring a new vice president of global policy\\\")\\nents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\\nprint('Before', ents)\\n# The model didn't recognize \\\"fb\\\" as an entity :(\\n\\n# Create a span for the new entity\\nfb_ent = Span(doc, 0, 1, label=\\\"ORG\\\")\\norig_ents = list(doc.ents)\\n\\n# Option 1: Modify the provided entity spans, leaving the rest unmodified\\ndoc.set_ents([fb_ent], default=\\\"unmodified\\\")\\n\\n# Option 2: Assign a complete list of ents to doc.ents\\ndoc.ents = orig_ents + [fb_ent]\\n\\nents = [(e.text, e.start, e.end, e.label_) for e in doc.ents]\\nprint('After', ents)\\n# [('fb', 0, 1, 'ORG')] 🎉\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Keep in mind that \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" is initialized with the start and end \", _jsx(_components.strong, {\n          children: \"token\"\n        }), \"\\nindices, not the character offsets. To create a span from character offsets, use\\n\", _jsx(_components.a, {\n          href: \"/api/doc#char_span\",\n          children: _jsx(InlineCode, {\n            children: \"Doc.char_span\"\n          })\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"fb_ent = doc.char_span(0, 2, label=\\\"ORG\\\")\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"setting-from-array\",\n        children: \"Setting entity annotations from array \"\n      }), _jsxs(_components.p, {\n        children: [\"You can also assign entity annotations using the\\n\", _jsx(_components.a, {\n          href: \"/api/doc#from_array\",\n          children: _jsx(InlineCode, {\n            children: \"doc.from_array\"\n          })\n        }), \" method. To do this, you should include\\nboth the \", _jsx(InlineCode, {\n          children: \"ENT_TYPE\"\n        }), \" and the \", _jsx(InlineCode, {\n          children: \"ENT_IOB\"\n        }), \" attributes in the array you’re importing\\nfrom.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import numpy\\nimport spacy\\nfrom spacy.attrs import ENT_IOB, ENT_TYPE\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp.make_doc(\\\"London is a big city in the United Kingdom.\\\")\\nprint(\\\"Before\\\", doc.ents)  # []\\n\\nheader = [ENT_IOB, ENT_TYPE]\\nattr_array = numpy.zeros((len(doc), len(header)), dtype=\\\"uint64\\\")\\nattr_array[0, 0] = 3  # B\\nattr_array[0, 1] = doc.vocab.strings[\\\"GPE\\\"]\\ndoc.from_array(header, attr_array)\\nprint(\\\"After\\\", doc.ents)  # [London]\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"setting-cython\",\n        children: \"Setting entity annotations in Cython \"\n      }), _jsxs(_components.p, {\n        children: [\"Finally, you can always write to the underlying struct if you compile a\\n\", _jsx(_components.a, {\n          href: \"http://cython.org/\",\n          children: \"Cython\"\n        }), \" function. This is easy to do, and allows you to\\nwrite efficient native code.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# cython: infer_types=True\\nfrom spacy.typedefs cimport attr_t\\nfrom spacy.tokens.doc cimport Doc\\n\\ncpdef set_entity(Doc doc, int start, int end, attr_t ent_type):\\n    for i in range(start, end):\\n        doc.c[i].ent_type = ent_type\\n    doc.c[start].ent_iob = 3\\n    for i in range(start+1, end):\\n        doc.c[i].ent_iob = 2\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Obviously, if you write directly to the array of \", _jsx(InlineCode, {\n          children: \"TokenC*\"\n        }), \" structs, you’ll have\\nresponsibility for ensuring that the data is left in a consistent state.\"]\n      }), _jsx(_components.h3, {\n        id: \"entity-types\",\n        children: \"Built-in entity types \"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Tip: Understanding entity types\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"You can also use \", _jsx(InlineCode, {\n            children: \"spacy.explain()\"\n          }), \" to get the description for the string\\nrepresentation of an entity label. For example, \", _jsx(InlineCode, {\n            children: \"spacy.explain(\\\"LANGUAGE\\\")\"\n          }), \"\\nwill return “any named language”.\"]\n        }), \"\\n\"]\n      }), _jsx(Infobox, {\n        title: \"Annotation scheme\",\n        children: _jsxs(_components.p, {\n          children: [\"For details on the entity types available in spaCy’s trained pipelines, see the\\n“label scheme” sections of the individual models in the\\n\", _jsx(_components.a, {\n            href: \"/models\",\n            children: \"models directory\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"displacy\",\n        children: \"Visualizing named entities \"\n      }), _jsxs(_components.p, {\n        children: [\"The\\n\", _jsxs(_components.a, {\n          href: \"https://explosion.ai/demos/displacy-ent\",\n          children: [\"displaCy \", _jsx(\"sup\", {\n            children: \"ENT\"\n          }), \" visualizer\"]\n        }), \"\\nlets you explore an entity recognition model’s behavior interactively. If you’re\\ntraining a model, it’s very useful to run the visualization yourself. To help\\nyou do that, spaCy comes with a visualization module. You can pass a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" or a\\nlist of \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" objects to displaCy and run\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#displacy.serve\",\n          children: _jsx(InlineCode, {\n            children: \"displacy.serve\"\n          })\n        }), \" to run the web server, or\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#displacy.render\",\n          children: _jsx(InlineCode, {\n            children: \"displacy.render\"\n          })\n        }), \" to generate the raw markup.\"]\n      }), _jsxs(_components.p, {\n        children: [\"For more details and examples, see the\\n\", _jsx(_components.a, {\n          href: \"/usage/visualizers\",\n          children: \"usage guide on visualizing spaCy\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Named Entity example\",\n          children: \"import spacy\\nfrom spacy import displacy\\n\\ntext = \\\"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\\\"\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(text)\\ndisplacy.serve(doc, style=\\\"ent\\\")\\n\"\n        })\n      }), _jsx(Iframe, {\n        title: \"displaCy visualizer for entities\",\n        src: \"/images/displacy-ent2.html\",\n        height: 180\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-entity-linking\",\n      children: [_jsx(_components.h2, {\n        id: \"entity-linking\",\n        children: \"Entity Linking \"\n      }), _jsxs(_components.p, {\n        children: [\"To ground the named entities into the “real world”, spaCy provides functionality\\nto perform entity linking, which resolves a textual entity to a unique\\nidentifier from a knowledge base (KB). You can create your own\\n\", _jsx(_components.a, {\n          href: \"/api/kb\",\n          children: _jsx(InlineCode, {\n            children: \"KnowledgeBase\"\n          })\n        }), \" and \", _jsx(_components.a, {\n          href: \"/usage/training\",\n          children: \"train\"\n        }), \" a new\\n\", _jsx(_components.a, {\n          href: \"/api/entitylinker\",\n          children: _jsx(InlineCode, {\n            children: \"EntityLinker\"\n          })\n        }), \" using that custom knowledge base.\"]\n      }), _jsx(_components.h3, {\n        id: \"entity-linking-accessing\",\n        model: \"entity linking\",\n        children: \"Accessing entity identifiers \"\n      }), _jsxs(_components.p, {\n        children: [\"The annotated KB identifier is accessible as either a hash value or as a string,\\nusing the attributes \", _jsx(InlineCode, {\n          children: \"ent.kb_id\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"ent.kb_id_\"\n        }), \" of a \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \"\\nobject, or the \", _jsx(InlineCode, {\n          children: \"ent_kb_id\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"ent_kb_id_\"\n        }), \" attributes of a\\n\", _jsx(_components.a, {\n          href: \"/api/token\",\n          children: _jsx(InlineCode, {\n            children: \"Token\"\n          })\n        }), \" object.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"my_custom_el_pipeline\\\")\\ndoc = nlp(\\\"Ada Lovelace was born in London\\\")\\n\\n# Document level\\nents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]\\nprint(ents)  # [('Ada Lovelace', 'PERSON', 'Q7259'), ('London', 'GPE', 'Q84')]\\n\\n# Token level\\nent_ada_0 = [doc[0].text, doc[0].ent_type_, doc[0].ent_kb_id_]\\nent_ada_1 = [doc[1].text, doc[1].ent_type_, doc[1].ent_kb_id_]\\nent_london_5 = [doc[5].text, doc[5].ent_type_, doc[5].ent_kb_id_]\\nprint(ent_ada_0)  # ['Ada', 'PERSON', 'Q7259']\\nprint(ent_ada_1)  # ['Lovelace', 'PERSON', 'Q7259']\\nprint(ent_london_5)  # ['London', 'GPE', 'Q84']\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-tokenization\",\n      children: [_jsx(_components.h2, {\n        id: \"tokenization\",\n        children: \"Tokenization \"\n      }), _jsxs(_components.p, {\n        children: [\"Tokenization is the task of splitting a text into meaningful segments, called\\n\", _jsx(_components.em, {\n          children: \"tokens\"\n        }), \". The input to the tokenizer is a unicode text, and the output is a\\n\", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" object. To construct a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object, you need a\\n\", _jsx(_components.a, {\n          href: \"/api/vocab\",\n          children: _jsx(InlineCode, {\n            children: \"Vocab\"\n          })\n        }), \" instance, a sequence of \", _jsx(InlineCode, {\n          children: \"word\"\n        }), \" strings, and optionally a\\nsequence of \", _jsx(InlineCode, {\n          children: \"spaces\"\n        }), \" booleans, which allow you to maintain alignment of the\\ntokens into the original string.\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"spaCy’s tokenization is \", _jsx(_components.strong, {\n            children: \"non-destructive\"\n          }), \", which means that you’ll always be\\nable to reconstruct the original input from the tokenized output. Whitespace\\ninformation is preserved in the tokens and no information is added or removed\\nduring tokenization. This is kind of a core principle of spaCy’s \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \" object:\\n\", _jsx(InlineCode, {\n            children: \"doc.text == input_text\"\n          }), \" should always hold true.\"]\n        })\n      }), _jsx(Tokenization101, {}), _jsxs(Accordion, {\n        title: \"Algorithm details: How spaCy's tokenizer works\",\n        id: \"how-tokenizer-works\",\n        spaced: true,\n        children: [_jsx(_components.p, {\n          children: \"spaCy introduces a novel tokenization algorithm that gives a better balance\\nbetween performance, ease of definition and ease of alignment into the original\\nstring.\"\n        }), _jsx(_components.p, {\n          children: \"After consuming a prefix or suffix, we consult the special cases again. We want\\nthe special cases to handle things like “don’t” in English, and we want the same\\nrule to work for “(don’t)!“. We do this by splitting off the open bracket, then\\nthe exclamation, then the closed bracket, and finally matching the special case.\\nHere’s an implementation of the algorithm in Python optimized for readability\\nrather than performance:\"\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"def tokenizer_pseudo_code(\\n    text,\\n    special_cases,\\n    prefix_search,\\n    suffix_search,\\n    infix_finditer,\\n    token_match,\\n    url_match\\n):\\n    tokens = []\\n    for substring in text.split():\\n        suffixes = []\\n        while substring:\\n            if substring in special_cases:\\n                tokens.extend(special_cases[substring])\\n                substring = \\\"\\\"\\n                continue\\n            while prefix_search(substring) or suffix_search(substring):\\n                if token_match(substring):\\n                    tokens.append(substring)\\n                    substring = \\\"\\\"\\n                    break\\n                if substring in special_cases:\\n                    tokens.extend(special_cases[substring])\\n                    substring = \\\"\\\"\\n                    break\\n                if prefix_search(substring):\\n                    split = prefix_search(substring).end()\\n                    tokens.append(substring[:split])\\n                    substring = substring[split:]\\n                    if substring in special_cases:\\n                        continue\\n                if suffix_search(substring):\\n                    split = suffix_search(substring).start()\\n                    suffixes.append(substring[split:])\\n                    substring = substring[:split]\\n            if token_match(substring):\\n                tokens.append(substring)\\n                substring = \\\"\\\"\\n            elif url_match(substring):\\n                tokens.append(substring)\\n                substring = \\\"\\\"\\n            elif substring in special_cases:\\n                tokens.extend(special_cases[substring])\\n                substring = \\\"\\\"\\n            elif list(infix_finditer(substring)):\\n                infixes = infix_finditer(substring)\\n                offset = 0\\n                for match in infixes:\\n                    if offset == 0 and match.start() == 0:\\n                        continue\\n                    tokens.append(substring[offset : match.start()])\\n                    tokens.append(substring[match.start() : match.end()])\\n                    offset = match.end()\\n                if substring[offset:]:\\n                    tokens.append(substring[offset:])\\n                substring = \\\"\\\"\\n            elif substring:\\n                tokens.append(substring)\\n                substring = \\\"\\\"\\n        tokens.extend(reversed(suffixes))\\n    for match in matcher(special_cases, text):\\n        tokens.replace(match, special_cases[match])\\n    return tokens\\n\"\n          })\n        }), _jsx(_components.p, {\n          children: \"The algorithm can be summarized as follows:\"\n        }), _jsxs(_components.ol, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"Iterate over space-separated substrings.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Check whether we have an explicitly defined special case for this substring.\\nIf we do, use it.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Look for a token match. If there is a match, stop processing and keep this\\ntoken.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Check whether we have an explicitly defined special case for this substring.\\nIf we do, use it.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Otherwise, try to consume one prefix. If we consumed a prefix, go back to #3,\\nso that the token match and special cases always get priority.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"If we didn’t consume a prefix, try to consume a suffix and then go back to\\n#3.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"If we can’t consume a prefix or a suffix, look for a URL match.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"If there’s no URL match, then look for a special case.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Look for “infixes” – stuff like hyphens etc. and split the substring into\\ntokens on all infixes.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Once we can’t consume any more of the string, handle it as a single token.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Make a final pass over the text to check for special cases that include\\nspaces or that were missed due to the incremental processing of affixes.\"\n          }), \"\\n\"]\n        })]\n      }), _jsxs(_components.p, {\n        children: [_jsx(_components.strong, {\n          children: \"Global\"\n        }), \" and \", _jsx(_components.strong, {\n          children: \"language-specific\"\n        }), \" tokenizer data is supplied via the language\\ndata in \", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spaCy/tree/master/spacy/lang\",\n          children: _jsx(InlineCode, {\n            children: \"spacy/lang\"\n          })\n        }), \". The tokenizer exceptions\\ndefine special cases like “don’t” in English, which needs to be split into two\\ntokens: \", _jsx(InlineCode, {\n          children: \"{ORTH: \\\"do\\\"}\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"{ORTH: \\\"n't\\\", NORM: \\\"not\\\"}\"\n        }), \". The prefixes, suffixes\\nand infixes mostly define punctuation rules – for example, when to split off\\nperiods (at the end of a sentence), and when to leave tokens containing periods\\nintact (abbreviations like “U.S.”).\"]\n      }), _jsx(Accordion, {\n        title: \"Should I change the language data or add custom tokenizer rules?\",\n        id: \"lang-data-vs-tokenizer\",\n        children: _jsxs(_components.p, {\n          children: [\"Tokenization rules that are specific to one language, but can be \", _jsx(_components.strong, {\n            children: \"generalized\\nacross that language\"\n          }), \", should ideally live in the language data in\\n\", _jsx(_components.a, {\n            href: \"https://github.com/explosion/spaCy/tree/master/spacy/lang\",\n            children: _jsx(InlineCode, {\n              children: \"spacy/lang\"\n            })\n          }), \" – we always appreciate pull requests!\\nAnything that’s specific to a domain or text type – like financial trading\\nabbreviations or Bavarian youth slang – should be added as a special case rule\\nto your tokenizer instance. If you’re dealing with a lot of customizations, it\\nmight make sense to create an entirely custom subclass.\"]\n        })\n      }), _jsx(_components.hr, {}), _jsx(_components.h3, {\n        id: \"special-cases\",\n        children: \"Adding special case tokenization rules \"\n      }), _jsxs(_components.p, {\n        children: [\"Most domains have at least some idiosyncrasies that require custom tokenization\\nrules. This could be very certain expressions, or abbreviations only used in\\nthis specific field. Here’s how to add a special case rule to an existing\\n\", _jsx(_components.a, {\n          href: \"/api/tokenizer\",\n          children: _jsx(InlineCode, {\n            children: \"Tokenizer\"\n          })\n        }), \" instance:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.symbols import ORTH\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"gimme that\\\")  # phrase to tokenize\\nprint([w.text for w in doc])  # ['gimme', 'that']\\n\\n# Add special case rule\\nspecial_case = [{ORTH: \\\"gim\\\"}, {ORTH: \\\"me\\\"}]\\nnlp.tokenizer.add_special_case(\\\"gimme\\\", special_case)\\n\\n# Check new tokenization\\nprint([w.text for w in nlp(\\\"gimme that\\\")])  # ['gim', 'me', 'that']\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"The special case doesn’t have to match an entire whitespace-delimited substring.\\nThe tokenizer will incrementally split off punctuation, and keep looking up the\\nremaining substring. The special case rules also have precedence over the\\npunctuation splitting.\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"assert \\\"gimme\\\" not in [w.text for w in nlp(\\\"gimme!\\\")]\\nassert \\\"gimme\\\" not in [w.text for w in nlp('(\\\"...gimme...?\\\")')]\\n\\nnlp.tokenizer.add_special_case(\\\"...gimme...?\\\", [{\\\"ORTH\\\": \\\"...gimme...?\\\"}])\\nassert len(nlp(\\\"...gimme...?\\\")) == 1\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"tokenizer-debug\",\n        version: \"2.2.3\",\n        children: \"Debugging the tokenizer \"\n      }), _jsxs(_components.p, {\n        children: [\"A working implementation of the pseudo-code above is available for debugging as\\n\", _jsx(_components.a, {\n          href: \"/api/tokenizer#explain\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.tokenizer.explain(text)\"\n          })\n        }), \". It returns a list of\\ntuples showing which tokenizer rule or pattern was matched for each token. The\\ntokens produced are identical to \", _jsx(InlineCode, {\n          children: \"nlp.tokenizer()\"\n        }), \" except for whitespace tokens:\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Expected output\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            lang: \"none\",\n            children: \"\\\"      PREFIX\\nLet    SPECIAL-1\\n's     SPECIAL-2\\ngo     TOKEN\\n!      SUFFIX\\n\\\"      SUFFIX\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\n\\nnlp = English()\\ntext = '''\\\"Let's go!\\\"'''\\ndoc = nlp(text)\\ntok_exp = nlp.tokenizer.explain(text)\\nassert [t.text for t in doc if not t.is_space] == [t[1] for t in tok_exp]\\nfor t in tok_exp:\\n    print(t[1], \\\"\\\\\\\\t\\\", t[0])\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"native-tokenizers\",\n        children: \"Customizing spaCy’s Tokenizer class \"\n      }), _jsx(_components.p, {\n        children: \"Let’s imagine you wanted to create a tokenizer for a new language or specific\\ndomain. There are six things you may need to define:\"\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"A dictionary of \", _jsx(_components.strong, {\n            children: \"special cases\"\n          }), \". This handles things like contractions,\\nunits of measurement, emoticons, certain abbreviations, etc.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"A function \", _jsx(InlineCode, {\n            children: \"prefix_search\"\n          }), \", to handle \", _jsx(_components.strong, {\n            children: \"preceding punctuation\"\n          }), \", such as open\\nquotes, open brackets, etc.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"A function \", _jsx(InlineCode, {\n            children: \"suffix_search\"\n          }), \", to handle \", _jsx(_components.strong, {\n            children: \"succeeding punctuation\"\n          }), \", such as\\ncommas, periods, close quotes, etc.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"A function \", _jsx(InlineCode, {\n            children: \"infix_finditer\"\n          }), \", to handle non-whitespace separators, such as\\nhyphens etc.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"An optional boolean function \", _jsx(InlineCode, {\n            children: \"token_match\"\n          }), \" matching strings that should never\\nbe split, overriding the infix rules. Useful for things like numbers.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"An optional boolean function \", _jsx(InlineCode, {\n            children: \"url_match\"\n          }), \", which is similar to \", _jsx(InlineCode, {\n            children: \"token_match\"\n          }), \"\\nexcept that prefixes and suffixes are removed before applying the match.\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"You shouldn’t usually need to create a \", _jsx(InlineCode, {\n          children: \"Tokenizer\"\n        }), \" subclass. Standard usage is\\nto use \", _jsx(InlineCode, {\n          children: \"re.compile()\"\n        }), \" to build a regular expression object, and pass its\\n\", _jsx(InlineCode, {\n          children: \".search()\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \".finditer()\"\n        }), \" methods:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import re\\nimport spacy\\nfrom spacy.tokenizer import Tokenizer\\n\\nspecial_cases = {\\\":)\\\": [{\\\"ORTH\\\": \\\":)\\\"}]}\\nprefix_re = re.compile(r'''^[\\\\\\\\[\\\\\\\\(\\\"']''')\\nsuffix_re = re.compile(r'''[\\\\\\\\]\\\\\\\\)\\\"']$''')\\ninfix_re = re.compile(r'''[-~]''')\\nsimple_url_re = re.compile(r'''^https?://''')\\n\\ndef custom_tokenizer(nlp):\\n    return Tokenizer(nlp.vocab, rules=special_cases,\\n                                prefix_search=prefix_re.search,\\n                                suffix_search=suffix_re.search,\\n                                infix_finditer=infix_re.finditer,\\n                                url_match=simple_url_re.match)\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nnlp.tokenizer = custom_tokenizer(nlp)\\ndoc = nlp(\\\"hello-world. :)\\\")\\nprint([t.text for t in doc]) # ['hello', '-', 'world.', ':)']\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"If you need to subclass the tokenizer instead, the relevant methods to\\nspecialize are \", _jsx(InlineCode, {\n          children: \"find_prefix\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"find_suffix\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"find_infix\"\n        }), \".\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"When customizing the prefix, suffix and infix handling, remember that you’re\\npassing in \", _jsx(_components.strong, {\n            children: \"functions\"\n          }), \" for spaCy to execute, e.g. \", _jsx(InlineCode, {\n            children: \"prefix_re.search\"\n          }), \" – not\\njust the regular expressions. This means that your functions also need to define\\nhow the rules should be applied. For example, if you’re adding your own prefix\\nrules, you need to make sure they’re only applied to characters at the\\n\", _jsx(_components.strong, {\n            children: \"beginning of a token\"\n          }), \", e.g. by adding \", _jsx(InlineCode, {\n            children: \"^\"\n          }), \". Similarly, suffix rules should\\nonly be applied at the \", _jsx(_components.strong, {\n            children: \"end of a token\"\n          }), \", so your expression should end with a\\n\", _jsx(InlineCode, {\n            children: \"$\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h4, {\n        id: \"native-tokenizer-additions\",\n        children: \"Modifying existing rule sets \"\n      }), _jsxs(_components.p, {\n        children: [\"In many situations, you don’t necessarily need entirely custom rules. Sometimes\\nyou just want to add another character to the prefixes, suffixes or infixes. The\\ndefault prefix, suffix and infix rules are available via the \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object’s\\n\", _jsx(InlineCode, {\n          children: \"Defaults\"\n        }), \" and the \", _jsx(InlineCode, {\n          children: \"Tokenizer\"\n        }), \" attributes such as\\n\", _jsx(_components.a, {\n          href: \"/api/tokenizer#attributes\",\n          children: _jsx(InlineCode, {\n            children: \"Tokenizer.suffix_search\"\n          })\n        }), \" are writable, so you can\\noverwrite them with compiled regular expression objects using modified default\\nrules. spaCy ships with utility functions to help you compile the regular\\nexpressions – for example,\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#util.compile_suffix_regex\",\n          children: _jsx(InlineCode, {\n            children: \"compile_suffix_regex\"\n          })\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"suffixes = nlp.Defaults.suffixes + [r'''-+$''',]\\nsuffix_regex = spacy.util.compile_suffix_regex(suffixes)\\nnlp.tokenizer.suffix_search = suffix_regex.search\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"Similarly, you can remove a character from the default suffixes:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"suffixes = list(nlp.Defaults.suffixes)\\nsuffixes.remove(\\\"\\\\\\\\\\\\\\\\[\\\")\\nsuffix_regex = spacy.util.compile_suffix_regex(suffixes)\\nnlp.tokenizer.suffix_search = suffix_regex.search\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"Tokenizer.suffix_search\"\n        }), \" attribute should be a function which takes a\\nunicode string and returns a \", _jsx(_components.strong, {\n          children: \"regex match object\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"None\"\n        }), \". Usually we use\\nthe \", _jsx(InlineCode, {\n          children: \".search\"\n        }), \" attribute of a compiled regex object, but you can use some other\\nfunction that behaves the same way.\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"If you’ve loaded a trained pipeline, writing to the\\n\", _jsx(_components.a, {\n            href: \"/api/language#defaults\",\n            children: _jsx(InlineCode, {\n              children: \"nlp.Defaults\"\n            })\n          }), \" or \", _jsx(InlineCode, {\n            children: \"English.Defaults\"\n          }), \" directly won’t\\nwork, since the regular expressions are read from the pipeline data and will be\\ncompiled when you load it. If you modify \", _jsx(InlineCode, {\n            children: \"nlp.Defaults\"\n          }), \", you’ll only see the\\neffect if you call \", _jsx(_components.a, {\n            href: \"/api/top-level#spacy.blank\",\n            children: _jsx(InlineCode, {\n              children: \"spacy.blank\"\n            })\n          }), \". If you want to\\nmodify the tokenizer loaded from a trained pipeline, you should modify\\n\", _jsx(InlineCode, {\n            children: \"nlp.tokenizer\"\n          }), \" directly. If you’re training your own pipeline, you can register\\n\", _jsx(_components.a, {\n            href: \"/usage/training/#custom-code-nlp-callbacks\",\n            children: \"callbacks\"\n          }), \" to modify the \", _jsx(InlineCode, {\n            children: \"nlp\"\n          }), \"\\nobject before training.\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The prefix, infix and suffix rule sets include not only individual characters\\nbut also detailed regular expressions that take the surrounding context into\\naccount. For example, there is a regular expression that treats a hyphen between\\nletters as an infix. If you do not want the tokenizer to split on hyphens\\nbetween letters, you can modify the existing infix definition from\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spaCy/tree/master/spacy/lang/punctuation.py\",\n          children: _jsx(InlineCode, {\n            children: \"lang/punctuation.py\"\n          })\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\\nfrom spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\\nfrom spacy.util import compile_infix_regex\\n\\n# Default tokenizer\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"mother-in-law\\\")\\nprint([t.text for t in doc]) # ['mother', '-', 'in', '-', 'law']\\n\\n# Modify tokenizer infix patterns\\ninfixes = (\\n    LIST_ELLIPSES\\n    + LIST_ICONS\\n    + [\\n        r\\\"(?\u003c=[0-9])[+\\\\\\\\-\\\\\\\\*^](?=[0-9-])\\\",\\n        r\\\"(?\u003c=[{al}{q}])\\\\\\\\.(?=[{au}{q}])\\\".format(\\n            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\\n        ),\\n        r\\\"(?\u003c=[{a}]),(?=[{a}])\\\".format(a=ALPHA),\\n        # ✅ Commented out regex that splits on hyphens between letters:\\n        # r\\\"(?\u003c=[{a}])(?:{h})(?=[{a}])\\\".format(a=ALPHA, h=HYPHENS),\\n        r\\\"(?\u003c=[{a}0-9])[:\u003c\u003e=/](?=[{a}])\\\".format(a=ALPHA),\\n    ]\\n)\\n\\ninfix_re = compile_infix_regex(infixes)\\nnlp.tokenizer.infix_finditer = infix_re.finditer\\ndoc = nlp(\\\"mother-in-law\\\")\\nprint([t.text for t in doc]) # ['mother-in-law']\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"For an overview of the default regular expressions, see\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spaCy/tree/master/spacy/lang/punctuation.py\",\n          children: _jsx(InlineCode, {\n            children: \"lang/punctuation.py\"\n          })\n        }), \" and\\nlanguage-specific definitions such as\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spaCy/tree/master/spacy/lang/de/punctuation.py\",\n          children: _jsx(InlineCode, {\n            children: \"lang/de/punctuation.py\"\n          })\n        }), \" for\\nGerman.\"]\n      }), _jsx(_components.h3, {\n        id: \"custom-tokenizer\",\n        children: \"Hooking a custom tokenizer into the pipeline \"\n      }), _jsxs(_components.p, {\n        children: [\"The tokenizer is the first component of the processing pipeline and the only one\\nthat can’t be replaced by writing to \", _jsx(InlineCode, {\n          children: \"nlp.pipeline\"\n        }), \". This is because it has a\\ndifferent signature from all the other components: it takes a text and returns a\\n\", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \", whereas all other components expect to already receive a\\ntokenized \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \".\"]\n      }), _jsx(_components.img, {\n        src: \"/images/pipeline.svg\",\n        alt: \"The processing pipeline\"\n      }), _jsxs(_components.p, {\n        children: [\"To overwrite the existing tokenizer, you need to replace \", _jsx(InlineCode, {\n          children: \"nlp.tokenizer\"\n        }), \" with a\\ncustom function that takes a text and returns a \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Creating a Doc\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"Constructing a \", _jsx(_components.a, {\n            href: \"/api/doc\",\n            children: _jsx(InlineCode, {\n              children: \"Doc\"\n            })\n          }), \" object manually requires at least two\\narguments: the shared \", _jsx(InlineCode, {\n            children: \"Vocab\"\n          }), \" and a list of words. Optionally, you can pass in\\na list of \", _jsx(InlineCode, {\n            children: \"spaces\"\n          }), \" values indicating whether the token at this position is\\nfollowed by a space (default \", _jsx(InlineCode, {\n            children: \"True\"\n          }), \"). See the section on\\n\", _jsx(_components.a, {\n            href: \"#own-annotations\",\n            children: \"pre-tokenized text\"\n          }), \" for more info.\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"words = [\\\"Let\\\", \\\"'s\\\", \\\"go\\\", \\\"!\\\"]\\nspaces = [False, True, False, False]\\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"nlp = spacy.blank(\\\"en\\\")\\nnlp.tokenizer = my_tokenizer\\n\"\n        })\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Argument\"\n            }), _jsx(_components.th, {\n              children: \"Type\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"text\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"str\"\n              })\n            }), _jsx(_components.td, {\n              children: \"The raw text to tokenize.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.strong, {\n                children: \"RETURNS\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"/api/doc\",\n                children: _jsx(InlineCode, {\n                  children: \"Doc\"\n                })\n              })\n            }), _jsx(_components.td, {\n              children: \"The tokenized document.\"\n            })]\n          })]\n        })]\n      }), _jsx(_components.h4, {\n        id: \"custom-tokenizer-example\",\n        children: \"Example 1: Basic whitespace tokenizer \"\n      }), _jsxs(_components.p, {\n        children: [\"Here’s an example of the most basic whitespace tokenizer. It takes the shared\\nvocab, so it can construct \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" objects. When it’s called on a text, it returns\\na \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object consisting of the text split on single space characters. We can\\nthen overwrite the \", _jsx(InlineCode, {\n          children: \"nlp.tokenizer\"\n        }), \" attribute with an instance of our custom\\ntokenizer.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.tokens import Doc\\n\\nclass WhitespaceTokenizer:\\n    def __init__(self, vocab):\\n        self.vocab = vocab\\n\\n    def __call__(self, text):\\n        words = text.split(\\\" \\\")\\n        spaces = [True] * len(words)\\n        # Avoid zero-length tokens\\n        for i, word in enumerate(words):\\n            if word == \\\"\\\":\\n                words[i] = \\\" \\\"\\n                spaces[i] = False\\n        # Remove the final trailing space\\n        if words[-1] == \\\" \\\":\\n            words = words[0:-1]\\n            spaces = spaces[0:-1]\\n        else:\\n           spaces[-1] = False\\n\\n        return Doc(self.vocab, words=words, spaces=spaces)\\n\\nnlp = spacy.blank(\\\"en\\\")\\nnlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\\ndoc = nlp(\\\"What's happened to me? he thought. It wasn't a dream.\\\")\\nprint([token.text for token in doc])\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"custom-tokenizer-example2\",\n        children: \"Example 2: Third-party tokenizers (BERT word pieces) \"\n      }), _jsxs(_components.p, {\n        children: [\"You can use the same approach to plug in any other third-party tokenizers. Your\\ncustom callable just needs to return a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object with the tokens produced by\\nyour tokenizer. In this example, the wrapper uses the \", _jsx(_components.strong, {\n          children: \"BERT word piece\\ntokenizer\"\n        }), \", provided by the\\n\", _jsx(_components.a, {\n          href: \"https://github.com/huggingface/tokenizers\",\n          children: _jsx(InlineCode, {\n            children: \"tokenizers\"\n          })\n        }), \" library. The tokens\\navailable in the \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object returned by spaCy now match the exact word pieces\\nproduced by the tokenizer.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"💡 Tip: spacy-transformers\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"If you’re working with transformer models like BERT, check out the\\n\", _jsx(_components.a, {\n            href: \"https://github.com/explosion/spacy-transformers\",\n            children: _jsx(InlineCode, {\n              children: \"spacy-transformers\"\n            })\n          }), \"\\nextension package and \", _jsx(_components.a, {\n            href: \"/usage/embeddings-transformers\",\n            children: \"documentation\"\n          }), \". It\\nincludes a pipeline component for using pretrained transformer weights and\\n\", _jsx(_components.strong, {\n            children: \"training transformer models\"\n          }), \" in spaCy, as well as helpful utilities for\\naligning word pieces to linguistic tokenization.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Custom BERT word piece tokenizer\",\n          children: \"from tokenizers import BertWordPieceTokenizer\\nfrom spacy.tokens import Doc\\nimport spacy\\n\\nclass BertTokenizer:\\n    def __init__(self, vocab, vocab_file, lowercase=True):\\n        self.vocab = vocab\\n        self._tokenizer = BertWordPieceTokenizer(vocab_file, lowercase=lowercase)\\n\\n    def __call__(self, text):\\n        tokens = self._tokenizer.encode(text)\\n        words = []\\n        spaces = []\\n        for i, (text, (start, end)) in enumerate(zip(tokens.tokens, tokens.offsets)):\\n            words.append(text)\\n            if i \u003c len(tokens.tokens) - 1:\\n                # If next start != current end we assume a space in between\\n                next_start, next_end = tokens.offsets[i + 1]\\n                spaces.append(next_start \u003e end)\\n            else:\\n                spaces.append(True)\\n        return Doc(self.vocab, words=words, spaces=spaces)\\n\\nnlp = spacy.blank(\\\"en\\\")\\nnlp.tokenizer = BertTokenizer(nlp.vocab, \\\"bert-base-uncased-vocab.txt\\\")\\ndoc = nlp(\\\"Justin Drew Bieber is a Canadian singer, songwriter, and actor.\\\")\\nprint(doc.text, [token.text for token in doc])\\n# [CLS]justin drew bi##eber is a canadian singer, songwriter, and actor.[SEP]\\n# ['[CLS]', 'justin', 'drew', 'bi', '##eber', 'is', 'a', 'canadian', 'singer',\\n#  ',', 'songwriter', ',', 'and', 'actor', '.', '[SEP]']\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Important note on tokenization and models\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Keep in mind that your models’ results may be less accurate if the tokenization\\nduring training differs from the tokenization at runtime. So if you modify a\\ntrained pipeline’s tokenization afterwards, it may produce very different\\npredictions. You should therefore train your pipeline with the \", _jsx(_components.strong, {\n            children: \"same\\ntokenizer\"\n          }), \" it will be using at runtime. See the docs on\\n\", _jsx(_components.a, {\n            href: \"#custom-tokenizer-training\",\n            children: \"training with custom tokenization\"\n          }), \" for details.\"]\n        })\n      }), _jsx(_components.h4, {\n        id: \"custom-tokenizer-training\",\n        version: \"3\",\n        children: \"Training with custom tokenization \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy’s \", _jsx(_components.a, {\n          href: \"/usage/training#config\",\n          children: \"training config\"\n        }), \" describes the settings,\\nhyperparameters, pipeline and tokenizer used for constructing and training the\\npipeline. The \", _jsx(InlineCode, {\n          children: \"[nlp.tokenizer]\"\n        }), \" block refers to a \", _jsx(_components.strong, {\n          children: \"registered function\"\n        }), \" that\\ntakes the \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object and returns a tokenizer. Here, we’re registering a\\nfunction called \", _jsx(InlineCode, {\n          children: \"whitespace_tokenizer\"\n        }), \" in the\\n\", _jsxs(_components.a, {\n          href: \"/api/top-level#registry\",\n          children: [_jsx(InlineCode, {\n            children: \"@tokenizers\"\n          }), \" registry\"]\n        }), \". To make sure spaCy knows how\\nto construct your tokenizer during training, you can pass in your Python file by\\nsetting \", _jsx(InlineCode, {\n          children: \"--code functions.py\"\n        }), \" when you run \", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[nlp.tokenizer]\\n@tokenizers = \\\"whitespace_tokenizer\\\"\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"functions.py\",\n          highlight: \"1\",\n          children: \"@spacy.registry.tokenizers(\\\"whitespace_tokenizer\\\")\\ndef create_whitespace_tokenizer():\\n    def create_tokenizer(nlp):\\n        return WhitespaceTokenizer(nlp.vocab)\\n\\n    return create_tokenizer\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Registered functions can also take arguments that are then passed in from the\\nconfig. This allows you to quickly change and keep track of different settings.\\nHere, the registered function called \", _jsx(InlineCode, {\n          children: \"bert_word_piece_tokenizer\"\n        }), \" takes two\\narguments: the path to a vocabulary file and whether to lowercase the text. The\\nPython type hints \", _jsx(InlineCode, {\n          children: \"str\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"bool\"\n        }), \" ensure that the received values have the\\ncorrect type.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[nlp.tokenizer]\\n@tokenizers = \\\"bert_word_piece_tokenizer\\\"\\nvocab_file = \\\"bert-base-uncased-vocab.txt\\\"\\nlowercase = true\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"functions.py\",\n          highlight: \"1\",\n          children: \"@spacy.registry.tokenizers(\\\"bert_word_piece_tokenizer\\\")\\ndef create_whitespace_tokenizer(vocab_file: str, lowercase: bool):\\n    def create_tokenizer(nlp):\\n        return BertWordPieceTokenizer(nlp.vocab, vocab_file, lowercase)\\n\\n    return create_tokenizer\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"To avoid hard-coding local paths into your config file, you can also set the\\nvocab path on the CLI by using the \", _jsx(InlineCode, {\n          children: \"--nlp.tokenizer.vocab_file\"\n        }), \"\\n\", _jsx(_components.a, {\n          href: \"/usage/training#config-overrides\",\n          children: \"override\"\n        }), \" when you run\\n\", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \". For more details on using registered functions,\\nsee the docs in \", _jsx(_components.a, {\n          href: \"/usage/training#custom-code\",\n          children: \"training with custom code\"\n        }), \".\"]\n      }), _jsx(Infobox, {\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Remember that a registered function should always be a function that spaCy\\n\", _jsx(_components.strong, {\n            children: \"calls to create something\"\n          }), \", not the “something” itself. In this case, it\\n\", _jsx(_components.strong, {\n            children: \"creates a function\"\n          }), \" that takes the \", _jsx(InlineCode, {\n            children: \"nlp\"\n          }), \" object and returns a callable that\\ntakes a text and returns a \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h4, {\n        id: \"own-annotations\",\n        children: \"Using pre-tokenized text \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy generally assumes by default that your data is \", _jsx(_components.strong, {\n          children: \"raw text\"\n        }), \". However,\\nsometimes your data is partially annotated, e.g. with pre-existing tokenization,\\npart-of-speech tags, etc. The most common situation is that you have\\n\", _jsx(_components.strong, {\n          children: \"pre-defined tokenization\"\n        }), \". If you have a list of strings, you can create a\\n\", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" object directly. Optionally, you can also specify a list of\\nboolean values, indicating whether each word is followed by a space.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"✏️ Things to try\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"Change a boolean value in the list of \", _jsx(InlineCode, {\n              children: \"spaces\"\n            }), \". You should see it reflected\\nin the \", _jsx(InlineCode, {\n              children: \"doc.text\"\n            }), \" and whether the token is followed by a space.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Remove \", _jsx(InlineCode, {\n              children: \"spaces=spaces\"\n            }), \" from the \", _jsx(InlineCode, {\n              children: \"Doc\"\n            }), \". You should see that every token is\\nnow followed by a space.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Copy-paste a random sentence from the internet and manually construct a\\n\", _jsx(InlineCode, {\n              children: \"Doc\"\n            }), \" with \", _jsx(InlineCode, {\n              children: \"words\"\n            }), \" and \", _jsx(InlineCode, {\n              children: \"spaces\"\n            }), \" so that the \", _jsx(InlineCode, {\n              children: \"doc.text\"\n            }), \" matches the original\\ninput text.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.tokens import Doc\\n\\nnlp = spacy.blank(\\\"en\\\")\\nwords = [\\\"Hello\\\", \\\",\\\", \\\"world\\\", \\\"!\\\"]\\nspaces = [False, True, False, False]\\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\\nprint(doc.text)\\nprint([(t.text, t.text_with_ws, t.whitespace_) for t in doc])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"If provided, the spaces list must be the \", _jsx(_components.strong, {\n          children: \"same length\"\n        }), \" as the words list. The\\nspaces list affects the \", _jsx(InlineCode, {\n          children: \"doc.text\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"span.text\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"token.idx\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"span.start_char\"\n        }), \"\\nand \", _jsx(InlineCode, {\n          children: \"span.end_char\"\n        }), \" attributes. If you don’t provide a \", _jsx(InlineCode, {\n          children: \"spaces\"\n        }), \" sequence, spaCy\\nwill assume that all words are followed by a space. Once you have a\\n\", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" object, you can write to its attributes to set the\\npart-of-speech tags, syntactic dependencies, named entities and other\\nattributes.\"]\n      }), _jsx(_components.h4, {\n        id: \"aligning-tokenization\",\n        children: \"Aligning tokenization \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy’s tokenization is non-destructive and uses language-specific rules\\noptimized for compatibility with treebank annotations. Other tools and resources\\ncan sometimes tokenize things differently – for example, \", _jsx(InlineCode, {\n          children: \"\\\"I'm\\\"\"\n        }), \" →\\n\", _jsx(InlineCode, {\n          children: \"[\\\"I\\\", \\\"'\\\", \\\"m\\\"]\"\n        }), \" instead of \", _jsx(InlineCode, {\n          children: \"[\\\"I\\\", \\\"'m\\\"]\"\n        }), \".\"]\n      }), _jsxs(_components.p, {\n        children: [\"In situations like that, you often want to align the tokenization so that you\\ncan merge annotations from different sources together, or take vectors predicted\\nby a\\n\", _jsx(_components.a, {\n          href: \"https://github.com/huggingface/pytorch-transformers\",\n          children: \"pretrained BERT model\"\n        }), \" and\\napply them to spaCy tokens. spaCy’s \", _jsx(_components.a, {\n          href: \"/api/example#alignment-object\",\n          children: _jsx(InlineCode, {\n            children: \"Alignment\"\n          })\n        }), \"\\nobject allows the one-to-one mappings of token indices in both directions as\\nwell as taking into account indices where multiple tokens align to one single\\ntoken.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"✏️ Things to try\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"Change the capitalization in one of the token lists – for example,\\n\", _jsx(InlineCode, {\n              children: \"\\\"obama\\\"\"\n            }), \" to \", _jsx(InlineCode, {\n              children: \"\\\"Obama\\\"\"\n            }), \". You’ll see that the alignment is case-insensitive.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Change \", _jsx(InlineCode, {\n              children: \"\\\"podcasts\\\"\"\n            }), \" in \", _jsx(InlineCode, {\n              children: \"other_tokens\"\n            }), \" to \", _jsx(InlineCode, {\n              children: \"\\\"pod\\\", \\\"casts\\\"\"\n            }), \". You should see\\nthat there are now two tokens of length 2 in \", _jsx(InlineCode, {\n              children: \"y2x\"\n            }), \", one corresponding to\\n“‘s”, and one to “podcasts”.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Make \", _jsx(InlineCode, {\n              children: \"other_tokens\"\n            }), \" and \", _jsx(InlineCode, {\n              children: \"spacy_tokens\"\n            }), \" identical. You’ll see that all\\ntokens now correspond 1-to-1.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.training import Alignment\\n\\nother_tokens = [\\\"i\\\", \\\"listened\\\", \\\"to\\\", \\\"obama\\\", \\\"'\\\", \\\"s\\\", \\\"podcasts\\\", \\\".\\\"]\\nspacy_tokens = [\\\"i\\\", \\\"listened\\\", \\\"to\\\", \\\"obama\\\", \\\"'s\\\", \\\"podcasts\\\", \\\".\\\"]\\nalign = Alignment.from_strings(other_tokens, spacy_tokens)\\nprint(f\\\"a -\u003e b, lengths: {align.x2y.lengths}\\\")  # array([1, 1, 1, 1, 1, 1, 1, 1])\\nprint(f\\\"a -\u003e b, mapping: {align.x2y.data}\\\")  # array([0, 1, 2, 3, 4, 4, 5, 6]) : two tokens both refer to \\\"'s\\\"\\nprint(f\\\"b -\u003e a, lengths: {align.y2x.lengths}\\\")  # array([1, 1, 1, 1, 2, 1, 1])   : the token \\\"'s\\\" refers to two tokens\\nprint(f\\\"b -\u003e a, mappings: {align.y2x.data}\\\")  # array([0, 1, 2, 3, 4, 5, 6, 7])\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"Here are some insights from the alignment information generated in the example\\nabove:\"\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"The one-to-one mappings for the first four tokens are identical, which means\\nthey map to each other. This makes sense because they’re also identical in the\\ninput: \", _jsx(InlineCode, {\n            children: \"\\\"i\\\"\"\n          }), \", \", _jsx(InlineCode, {\n            children: \"\\\"listened\\\"\"\n          }), \", \", _jsx(InlineCode, {\n            children: \"\\\"to\\\"\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"\\\"obama\\\"\"\n          }), \".\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"The value of \", _jsx(InlineCode, {\n            children: \"x2y.data[6]\"\n          }), \" is \", _jsx(InlineCode, {\n            children: \"5\"\n          }), \", which means that \", _jsx(InlineCode, {\n            children: \"other_tokens[6]\"\n          }), \"\\n(\", _jsx(InlineCode, {\n            children: \"\\\"podcasts\\\"\"\n          }), \") aligns to \", _jsx(InlineCode, {\n            children: \"spacy_tokens[5]\"\n          }), \" (also \", _jsx(InlineCode, {\n            children: \"\\\"podcasts\\\"\"\n          }), \").\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(InlineCode, {\n            children: \"x2y.data[4]\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"x2y.data[5]\"\n          }), \" are both \", _jsx(InlineCode, {\n            children: \"4\"\n          }), \", which means that both tokens 4\\nand 5 of \", _jsx(InlineCode, {\n            children: \"other_tokens\"\n          }), \" (\", _jsx(InlineCode, {\n            children: \"\\\"'\\\"\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"\\\"s\\\"\"\n          }), \") align to token 4 of \", _jsx(InlineCode, {\n            children: \"spacy_tokens\"\n          }), \"\\n(\", _jsx(InlineCode, {\n            children: \"\\\"'s\\\"\"\n          }), \").\"]\n        }), \"\\n\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"The current implementation of the alignment algorithm assumes that both\\ntokenizations add up to the same string. For example, you’ll be able to align\\n\", _jsx(InlineCode, {\n            children: \"[\\\"I\\\", \\\"'\\\", \\\"m\\\"]\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"[\\\"I\\\", \\\"'m\\\"]\"\n          }), \", which both add up to \", _jsx(InlineCode, {\n            children: \"\\\"I'm\\\"\"\n          }), \", but not\\n\", _jsx(InlineCode, {\n            children: \"[\\\"I\\\", \\\"'m\\\"]\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"[\\\"I\\\", \\\"am\\\"]\"\n          }), \".\"]\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-retokenization\",\n      children: [_jsx(_components.h2, {\n        id: \"retokenization\",\n        version: \"2.1\",\n        children: \"Merging and splitting \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/doc#retokenize\",\n          children: _jsx(InlineCode, {\n            children: \"Doc.retokenize\"\n          })\n        }), \" context manager lets you merge and\\nsplit tokens. Modifications to the tokenization are stored and performed all at\\nonce when the context manager exits. To merge several tokens into one single\\ntoken, pass a \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" to \", _jsx(_components.a, {\n          href: \"/api/doc#retokenizer.merge\",\n          children: _jsx(InlineCode, {\n            children: \"retokenizer.merge\"\n          })\n        }), \". An\\noptional dictionary of \", _jsx(InlineCode, {\n          children: \"attrs\"\n        }), \" lets you set attributes that will be assigned to\\nthe merged token – for example, the lemma, part-of-speech tag or entity type. By\\ndefault, the merged token will receive the same attributes as the merged span’s\\nroot.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"✏️ Things to try\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"Inspect the \", _jsx(InlineCode, {\n              children: \"token.lemma_\"\n            }), \" attribute with and without setting the \", _jsx(InlineCode, {\n              children: \"attrs\"\n            }), \".\\nYou’ll see that the lemma defaults to “New”, the lemma of the span’s root.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Overwrite other attributes like the \", _jsx(InlineCode, {\n              children: \"\\\"ENT_TYPE\\\"\"\n            }), \". Since “New York” is also\\nrecognized as a named entity, this change will also be reflected in the\\n\", _jsx(InlineCode, {\n              children: \"doc.ents\"\n            }), \".\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"I live in New York\\\")\\nprint(\\\"Before:\\\", [token.text for token in doc])\\n\\nwith doc.retokenize() as retokenizer:\\n    retokenizer.merge(doc[3:5], attrs={\\\"LEMMA\\\": \\\"new york\\\"})\\nprint(\\\"After:\\\", [token.text for token in doc])\\n\"\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Tip: merging entities and noun phrases\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"If you need to merge named entities or noun chunks, check out the built-in\\n\", _jsx(_components.a, {\n            href: \"/api/pipeline-functions#merge_entities\",\n            children: _jsx(InlineCode, {\n              children: \"merge_entities\"\n            })\n          }), \" and\\n\", _jsx(_components.a, {\n            href: \"/api/pipeline-functions#merge_noun_chunks\",\n            children: _jsx(InlineCode, {\n              children: \"merge_noun_chunks\"\n            })\n          }), \" pipeline\\ncomponents. When added to your pipeline using \", _jsx(InlineCode, {\n            children: \"nlp.add_pipe\"\n          }), \", they’ll take\\ncare of merging the spans automatically.\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"If an attribute in the \", _jsx(InlineCode, {\n          children: \"attrs\"\n        }), \" is a context-dependent token attribute, it will\\nbe applied to the underlying \", _jsx(_components.a, {\n          href: \"/api/token\",\n          children: _jsx(InlineCode, {\n            children: \"Token\"\n          })\n        }), \". For example \", _jsx(InlineCode, {\n          children: \"LEMMA\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"POS\"\n        }), \"\\nor \", _jsx(InlineCode, {\n          children: \"DEP\"\n        }), \" only apply to a word in context, so they’re token attributes. If an\\nattribute is a context-independent lexical attribute, it will be applied to the\\nunderlying \", _jsx(_components.a, {\n          href: \"/api/lexeme\",\n          children: _jsx(InlineCode, {\n            children: \"Lexeme\"\n          })\n        }), \", the entry in the vocabulary. For example,\\n\", _jsx(InlineCode, {\n          children: \"LOWER\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"IS_STOP\"\n        }), \" apply to all words of the same spelling, regardless of the\\ncontext.\"]\n      }), _jsxs(Infobox, {\n        variant: \"warning\",\n        title: \"Note on merging overlapping spans\",\n        children: [_jsxs(_components.p, {\n          children: [\"If you’re trying to merge spans that overlap, spaCy will raise an error because\\nit’s unclear how the result should look. Depending on the application, you may\\nwant to match the shortest or longest possible span, so it’s up to you to filter\\nthem. If you’re looking for the longest non-overlapping span, you can use the\\n\", _jsx(_components.a, {\n            href: \"/api/top-level#util.filter_spans\",\n            children: _jsx(InlineCode, {\n              children: \"util.filter_spans\"\n            })\n          }), \" helper:\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"doc = nlp(\\\"I live in Berlin Kreuzberg\\\")\\nspans = [doc[3:5], doc[3:4], doc[4:5]]\\nfiltered_spans = filter_spans(spans)\\n\"\n          })\n        })]\n      }), _jsx(_components.h3, {\n        children: \"Splitting tokens\"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/doc#retokenizer.split\",\n          children: _jsx(InlineCode, {\n            children: \"retokenizer.split\"\n          })\n        }), \" method allows splitting\\none token into two or more tokens. This can be useful for cases where\\ntokenization rules alone aren’t sufficient. For example, you might want to split\\n“its” into the tokens “it” and “is” – but not the possessive pronoun “its”. You\\ncan write rule-based logic that can find only the correct “its” to split, but by\\nthat time, the \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" will already be tokenized.\"]\n      }), _jsxs(_components.p, {\n        children: [\"This process of splitting a token requires more settings, because you need to\\nspecify the text of the individual tokens, optional per-token attributes and how\\nthe tokens should be attached to the existing syntax tree. This can be done by\\nsupplying a list of \", _jsx(InlineCode, {\n          children: \"heads\"\n        }), \" – either the token to attach the newly split token\\nto, or a \", _jsx(InlineCode, {\n          children: \"(token, subtoken)\"\n        }), \" tuple if the newly split token should be attached\\nto another subtoken. In this case, “New” should be attached to “York” (the\\nsecond split subtoken) and “York” should be attached to “in”.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"✏️ Things to try\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"Assign different attributes to the subtokens and compare the result.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Change the heads so that “New” is attached to “in” and “York” is attached\\nto “New”.\"\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Split the token into three tokens instead of two – for example,\\n\", _jsx(InlineCode, {\n              children: \"[\\\"New\\\", \\\"Yo\\\", \\\"rk\\\"]\"\n            }), \".\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy import displacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"I live in NewYork\\\")\\nprint(\\\"Before:\\\", [token.text for token in doc])\\ndisplacy.render(doc)  # displacy.serve if you're not in a Jupyter environment\\n\\nwith doc.retokenize() as retokenizer:\\n    heads = [(doc[3], 1), doc[2]]\\n    attrs = {\\\"POS\\\": [\\\"PROPN\\\", \\\"PROPN\\\"], \\\"DEP\\\": [\\\"pobj\\\", \\\"compound\\\"]}\\n    retokenizer.split(doc[3], [\\\"New\\\", \\\"York\\\"], heads=heads, attrs=attrs)\\nprint(\\\"After:\\\", [token.text for token in doc])\\ndisplacy.render(doc)  # displacy.serve if you're not in a Jupyter environment\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Specifying the heads as a list of \", _jsx(InlineCode, {\n          children: \"token\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"(token, subtoken)\"\n        }), \" tuples allows\\nattaching split subtokens to other subtokens, without having to keep track of\\nthe token indices after splitting.\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Token\"\n            }), _jsx(_components.th, {\n              children: \"Head\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"New\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"(doc[3], 1)\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attach this token to the second subtoken (index \", _jsx(InlineCode, {\n                children: \"1\"\n              }), \") that \", _jsx(InlineCode, {\n                children: \"doc[3]\"\n              }), \" will be split into, i.e. “York”.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"York\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"doc[2]\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attach this token to \", _jsx(InlineCode, {\n                children: \"doc[1]\"\n              }), \" in the original \", _jsx(InlineCode, {\n                children: \"Doc\"\n              }), \", i.e. “in”.\"]\n            })]\n          })]\n        })]\n      }), _jsx(_components.p, {\n        children: \"If you don’t care about the heads (for example, if you’re only running the\\ntokenizer and not the parser), you can attach each subtoken to itself:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          highlight: \"3\",\n          children: \"doc = nlp(\\\"I live in NewYorkCity\\\")\\nwith doc.retokenize() as retokenizer:\\n    heads = [(doc[3], 0), (doc[3], 1), (doc[3], 2)]\\n    retokenizer.split(doc[3], [\\\"New\\\", \\\"York\\\", \\\"City\\\"], heads=heads)\\n\"\n        })\n      }), _jsxs(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: [_jsxs(_components.p, {\n          children: [\"When splitting tokens, the subtoken texts always have to match the original\\ntoken text – or, put differently \", _jsx(InlineCode, {\n            children: \"\\\"\\\".join(subtokens) == token.text\"\n          }), \" always needs\\nto hold true. If this wasn’t the case, splitting tokens could easily end up\\nproducing confusing and unexpected results that would contradict spaCy’s\\nnon-destructive tokenization policy.\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-diff\",\n            lang: \"diff\",\n            children: \"doc = nlp(\\\"I live in L.A.\\\")\\nwith doc.retokenize() as retokenizer:\\n-    retokenizer.split(doc[3], [\\\"Los\\\", \\\"Angeles\\\"], heads=[(doc[3], 1), doc[2]])\\n+    retokenizer.split(doc[3], [\\\"L.\\\", \\\"A.\\\"], heads=[(doc[3], 1), doc[2]])\\n\"\n          })\n        })]\n      }), _jsx(_components.h3, {\n        id: \"retokenization-extensions\",\n        children: \"Overwriting custom extension attributes \"\n      }), _jsxs(_components.p, {\n        children: [\"If you’ve registered custom\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components-attributes\",\n          children: \"extension attributes\"\n        }), \",\\nyou can overwrite them during tokenization by providing a dictionary of\\nattribute names mapped to new values as the \", _jsx(InlineCode, {\n          children: \"\\\"_\\\"\"\n        }), \" key in the \", _jsx(InlineCode, {\n          children: \"attrs\"\n        }), \". For\\nmerging, you need to provide one dictionary of attributes for the resulting\\nmerged token. For splitting, you need to provide a list of dictionaries with\\ncustom attributes, one per split subtoken.\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"To set extension attributes during retokenization, the attributes need to be\\n\", _jsx(_components.strong, {\n            children: \"registered\"\n          }), \" using the \", _jsx(_components.a, {\n            href: \"/api/token#set_extension\",\n            children: _jsx(InlineCode, {\n              children: \"Token.set_extension\"\n            })\n          }), \"\\nmethod and they need to be \", _jsx(_components.strong, {\n            children: \"writable\"\n          }), \". This means that they should either have\\na default value that can be overwritten, or a getter \", _jsx(_components.em, {\n            children: \"and\"\n          }), \" setter. Method\\nextensions or extensions with only a getter are computed dynamically, so their\\nvalues can’t be overwritten. For more details, see the\\n\", _jsx(_components.a, {\n            href: \"/usage/processing-pipelines/#custom-components-attributes\",\n            children: \"extension attribute docs\"\n          }), \".\"]\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"✏️ Things to try\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"Add another custom extension – maybe \", _jsx(InlineCode, {\n              children: \"\\\"music_style\\\"\"\n            }), \"? – and overwrite it.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Change the extension attribute to use only a \", _jsx(InlineCode, {\n              children: \"getter\"\n            }), \" function. You should\\nsee that spaCy raises an error, because the attribute is not writable\\nanymore.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Rewrite the code to split a token with \", _jsx(InlineCode, {\n              children: \"retokenizer.split\"\n            }), \". Remember that\\nyou need to provide a list of extension attribute values as the \", _jsx(InlineCode, {\n              children: \"\\\"_\\\"\"\n            }), \"\\nproperty, one for each split subtoken.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.tokens import Token\\n\\n# Register a custom token attribute, token._.is_musician\\nToken.set_extension(\\\"is_musician\\\", default=False)\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"I like David Bowie\\\")\\nprint(\\\"Before:\\\", [(token.text, token._.is_musician) for token in doc])\\n\\nwith doc.retokenize() as retokenizer:\\n    retokenizer.merge(doc[2:4], attrs={\\\"_\\\": {\\\"is_musician\\\": True}})\\nprint(\\\"After:\\\", [(token.text, token._.is_musician) for token in doc])\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-sbd\",\n      children: [_jsx(_components.h2, {\n        id: \"sbd\",\n        children: \"Sentence Segmentation \"\n      }), _jsxs(_components.p, {\n        children: [\"A \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" object’s sentences are available via the \", _jsx(InlineCode, {\n          children: \"Doc.sents\"\n        }), \"\\nproperty. To view a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \"’s sentences, you can iterate over the \", _jsx(InlineCode, {\n          children: \"Doc.sents\"\n        }), \", a\\ngenerator that yields \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \" objects. You can check whether a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \"\\nhas sentence boundaries by calling\\n\", _jsx(_components.a, {\n          href: \"/api/doc#has_annotation\",\n          children: _jsx(InlineCode, {\n            children: \"Doc.has_annotation\"\n          })\n        }), \" with the attribute name\\n\", _jsx(InlineCode, {\n          children: \"\\\"SENT_START\\\"\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"This is a sentence. This is another sentence.\\\")\\nassert doc.has_annotation(\\\"SENT_START\\\")\\nfor sent in doc.sents:\\n    print(sent.text)\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"spaCy provides four alternatives for sentence segmentation:\"\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.a, {\n            href: \"#sbd-parser\",\n            children: \"Dependency parser\"\n          }), \": the statistical\\n\", _jsx(_components.a, {\n            href: \"/api/dependencyparser\",\n            children: _jsx(InlineCode, {\n              children: \"DependencyParser\"\n            })\n          }), \" provides the most accurate\\nsentence boundaries based on full dependency parses.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.a, {\n            href: \"#sbd-senter\",\n            children: \"Statistical sentence segmenter\"\n          }), \": the statistical\\n\", _jsx(_components.a, {\n            href: \"/api/sentencerecognizer\",\n            children: _jsx(InlineCode, {\n              children: \"SentenceRecognizer\"\n            })\n          }), \" is a simpler and faster\\nalternative to the parser that only sets sentence boundaries.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.a, {\n            href: \"#sbd-component\",\n            children: \"Rule-based pipeline component\"\n          }), \": the rule-based\\n\", _jsx(_components.a, {\n            href: \"/api/sentencizer\",\n            children: _jsx(InlineCode, {\n              children: \"Sentencizer\"\n            })\n          }), \" sets sentence boundaries using a\\ncustomizable list of sentence-final punctuation.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.a, {\n            href: \"#sbd-custom\",\n            children: \"Custom function\"\n          }), \": your own custom function added to the\\nprocessing pipeline can set sentence boundaries by writing to\\n\", _jsx(InlineCode, {\n            children: \"Token.is_sent_start\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), _jsx(_components.h3, {\n        id: \"sbd-parser\",\n        model: \"parser\",\n        children: \"Default: Using the dependency parse \"\n      }), _jsxs(_components.p, {\n        children: [\"Unlike other libraries, spaCy uses the dependency parse to determine sentence\\nboundaries. This is usually the most accurate approach, but it requires a\\n\", _jsx(_components.strong, {\n          children: \"trained pipeline\"\n        }), \" that provides accurate predictions. If your texts are\\ncloser to general-purpose news or web text, this should work well out-of-the-box\\nwith spaCy’s provided trained pipelines. For social media or conversational text\\nthat doesn’t follow the same rules, your application may benefit from a custom\\ntrained or rule-based component.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"This is a sentence. This is another sentence.\\\")\\nfor sent in doc.sents:\\n    print(sent.text)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"spaCy’s dependency parser respects already set boundaries, so you can preprocess\\nyour \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" using custom components \", _jsx(_components.em, {\n          children: \"before\"\n        }), \" it’s parsed. Depending on your text,\\nthis may also improve parse accuracy, since the parser is constrained to predict\\nparses consistent with the sentence boundaries.\"]\n      }), _jsx(_components.h3, {\n        id: \"sbd-senter\",\n        model: \"senter\",\n        version: \"3\",\n        children: \"Statistical sentence segmenter \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/sentencerecognizer\",\n          children: _jsx(InlineCode, {\n            children: \"SentenceRecognizer\"\n          })\n        }), \" is a simple statistical\\ncomponent that only provides sentence boundaries. Along with being faster and\\nsmaller than the parser, its primary advantage is that it’s easier to train\\nbecause it only requires annotated sentence boundaries rather than full\\ndependency parses. spaCy’s \", _jsx(_components.a, {\n          href: \"/models\",\n          children: \"trained pipelines\"\n        }), \" include both a parser\\nand a trained sentence segmenter, which is\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#disabling\",\n          children: \"disabled\"\n        }), \" by default. If you only need\\nsentence boundaries and no parser, you can use the \", _jsx(InlineCode, {\n          children: \"exclude\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"disable\"\n        }), \"\\nargument on \", _jsx(_components.a, {\n          href: \"/api/top-level#spacy.load\",\n          children: _jsx(InlineCode, {\n            children: \"spacy.load\"\n          })\n        }), \" to load the pipeline\\nwithout the parser and then enable the sentence recognizer explicitly with\\n\", _jsx(_components.a, {\n          href: \"/api/language#enable_pipe\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.enable_pipe\"\n          })\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"senter vs. parser\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"The recall for the \", _jsx(InlineCode, {\n            children: \"senter\"\n          }), \" is typically slightly lower than for the parser,\\nwhich is better at predicting sentence boundaries when punctuation is not\\npresent.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\", exclude=[\\\"parser\\\"])\\nnlp.enable_pipe(\\\"senter\\\")\\ndoc = nlp(\\\"This is a sentence. This is another sentence.\\\")\\nfor sent in doc.sents:\\n    print(sent.text)\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"sbd-component\",\n        children: \"Rule-based pipeline component \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/sentencizer\",\n          children: _jsx(InlineCode, {\n            children: \"Sentencizer\"\n          })\n        }), \" component is a\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines\",\n          children: \"pipeline component\"\n        }), \" that splits sentences on\\npunctuation like \", _jsx(InlineCode, {\n          children: \".\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"!\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"?\"\n        }), \". You can plug it into your pipeline if you only\\nneed sentence boundaries without dependency parses.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.lang.en import English\\n\\nnlp = English()  # just the language with no pipeline\\nnlp.add_pipe(\\\"sentencizer\\\")\\ndoc = nlp(\\\"This is a sentence. This is another sentence.\\\")\\nfor sent in doc.sents:\\n    print(sent.text)\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"sbd-custom\",\n        children: \"Custom rule-based strategy \"\n      }), _jsxs(_components.p, {\n        children: [\"If you want to implement your own strategy that differs from the default\\nrule-based approach of splitting on sentences, you can also create a\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components\",\n          children: \"custom pipeline component\"\n        }), \" that\\ntakes a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object and sets the \", _jsx(InlineCode, {\n          children: \"Token.is_sent_start\"\n        }), \" attribute on each\\nindividual token. If set to \", _jsx(InlineCode, {\n          children: \"False\"\n        }), \", the token is explicitly marked as \", _jsx(_components.em, {\n          children: \"not\"\n        }), \" the\\nstart of a sentence. If set to \", _jsx(InlineCode, {\n          children: \"None\"\n        }), \" (default), it’s treated as a missing value\\nand can still be overwritten by the parser.\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"To prevent inconsistent state, you can only set boundaries \", _jsx(_components.strong, {\n            children: \"before\"\n          }), \" a document\\nis parsed (and \", _jsx(InlineCode, {\n            children: \"doc.has_annotation(\\\"DEP\\\")\"\n          }), \" is \", _jsx(InlineCode, {\n            children: \"False\"\n          }), \"). To ensure that your\\ncomponent is added in the right place, you can set \", _jsx(InlineCode, {\n            children: \"before='parser'\"\n          }), \" or\\n\", _jsx(InlineCode, {\n            children: \"first=True\"\n          }), \" when adding it to the pipeline using\\n\", _jsx(_components.a, {\n            href: \"/api/language#add_pipe\",\n            children: _jsx(InlineCode, {\n              children: \"nlp.add_pipe\"\n            })\n          }), \".\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Here’s an example of a component that implements a pre-processing rule for\\nsplitting on \", _jsx(InlineCode, {\n          children: \"\\\"...\\\"\"\n        }), \" tokens. The component is added before the parser, which is\\nthen used to further segment the text. That’s possible, because \", _jsx(InlineCode, {\n          children: \"is_sent_start\"\n        }), \"\\nis only set to \", _jsx(InlineCode, {\n          children: \"True\"\n        }), \" for some of the tokens – all others still specify \", _jsx(InlineCode, {\n          children: \"None\"\n        }), \"\\nfor unset sentence boundaries. This approach can be useful if you want to\\nimplement \", _jsx(_components.strong, {\n          children: \"additional\"\n        }), \" rules specific to your data, while still being able to\\ntake advantage of dependency-based sentence segmentation.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.language import Language\\nimport spacy\\n\\ntext = \\\"this is a sentence...hello...and another sentence.\\\"\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(text)\\nprint(\\\"Before:\\\", [sent.text for sent in doc.sents])\\n\\n@Language.component(\\\"set_custom_boundaries\\\")\\ndef set_custom_boundaries(doc):\\n    for token in doc[:-1]:\\n        if token.text == \\\"...\\\":\\n            doc[token.i + 1].is_sent_start = True\\n    return doc\\n\\nnlp.add_pipe(\\\"set_custom_boundaries\\\", before=\\\"parser\\\")\\ndoc = nlp(text)\\nprint(\\\"After:\\\", [sent.text for sent in doc.sents])\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-mappings-exceptions\",\n      children: [_jsx(_components.h2, {\n        id: \"mappings-exceptions\",\n        version: \"3\",\n        children: \"Mappings \u0026 Exceptions \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/attributeruler\",\n          children: _jsx(InlineCode, {\n            children: \"AttributeRuler\"\n          })\n        }), \" manages \", _jsx(_components.strong, {\n          children: \"rule-based mappings and\\nexceptions\"\n        }), \" for all token-level attributes. As the number of\\n\", _jsx(_components.a, {\n          href: \"/api/#architecture-pipeline\",\n          children: \"pipeline components\"\n        }), \" has grown from spaCy v2 to\\nv3, handling rules and exceptions in each component individually has become\\nimpractical, so the \", _jsx(InlineCode, {\n          children: \"AttributeRuler\"\n        }), \" provides a single component with a unified\\npattern format for all token attribute mappings and exceptions.\"]\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"AttributeRuler\"\n        }), \" uses\\n\", _jsxs(_components.a, {\n          href: \"/usage/rule-based-matching#adding-patterns\",\n          children: [_jsx(InlineCode, {\n            children: \"Matcher\"\n          }), \" patterns\"]\n        }), \" to identify\\ntokens and then assigns them the provided attributes. If needed, the\\n\", _jsx(_components.a, {\n          href: \"/api/matcher\",\n          children: _jsx(InlineCode, {\n            children: \"Matcher\"\n          })\n        }), \" patterns can include context around the target token.\\nFor example, the attribute ruler can:\"]\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"provide exceptions for any \", _jsx(_components.strong, {\n            children: \"token attributes\"\n          })]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"map \", _jsx(_components.strong, {\n            children: \"fine-grained tags\"\n          }), \" to \", _jsx(_components.strong, {\n            children: \"coarse-grained tags\"\n          }), \" for languages without\\nstatistical morphologizers (replacing the v2.x \", _jsx(InlineCode, {\n            children: \"tag_map\"\n          }), \" in the\\n\", _jsx(_components.a, {\n            href: \"#language-data\",\n            children: \"language data\"\n          }), \")\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"map token \", _jsx(_components.strong, {\n            children: \"surface form + fine-grained tags\"\n          }), \" to \", _jsx(_components.strong, {\n            children: \"morphological features\"\n          }), \"\\n(replacing the v2.x \", _jsx(InlineCode, {\n            children: \"morph_rules\"\n          }), \" in the \", _jsx(_components.a, {\n            href: \"#language-data\",\n            children: \"language data\"\n          }), \")\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"specify the \", _jsx(_components.strong, {\n            children: \"tags for space tokens\"\n          }), \" (replacing hard-coded behavior in the\\ntagger)\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"The following example shows how the tag and POS \", _jsx(InlineCode, {\n          children: \"NNP\"\n        }), \"/\", _jsx(InlineCode, {\n          children: \"PROPN\"\n        }), \" can be specified\\nfor the phrase \", _jsx(InlineCode, {\n          children: \"\\\"The Who\\\"\"\n        }), \", overriding the tags provided by the statistical\\ntagger and the POS tag map.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ntext = \\\"I saw The Who perform. Who did you see?\\\"\\ndoc1 = nlp(text)\\nprint(doc1[2].tag_, doc1[2].pos_)  # DT DET\\nprint(doc1[3].tag_, doc1[3].pos_)  # WP PRON\\n\\n# Add attribute ruler with exception for \\\"The Who\\\" as NNP/PROPN NNP/PROPN\\nruler = nlp.get_pipe(\\\"attribute_ruler\\\")\\n# Pattern to match \\\"The Who\\\"\\npatterns = [[{\\\"LOWER\\\": \\\"the\\\"}, {\\\"TEXT\\\": \\\"Who\\\"}]]\\n# The attributes to assign to the matched token\\nattrs = {\\\"TAG\\\": \\\"NNP\\\", \\\"POS\\\": \\\"PROPN\\\"}\\n# Add rules to the attribute ruler\\nruler.add(patterns=patterns, attrs=attrs, index=0)  # \\\"The\\\" in \\\"The Who\\\"\\nruler.add(patterns=patterns, attrs=attrs, index=1)  # \\\"Who\\\" in \\\"The Who\\\"\\n\\ndoc2 = nlp(text)\\nprint(doc2[2].tag_, doc2[2].pos_)  # NNP PROPN\\nprint(doc2[3].tag_, doc2[3].pos_)  # NNP PROPN\\n# The second \\\"Who\\\" remains unmodified\\nprint(doc2[5].tag_, doc2[5].pos_)  # WP PRON\\n\"\n        })\n      }), _jsx(Infobox, {\n        variant: \"warning\",\n        title: \"Migrating from spaCy v2.x\",\n        children: _jsxs(_components.p, {\n          children: [\"The \", _jsx(_components.a, {\n            href: \"/api/attributeruler\",\n            children: _jsx(InlineCode, {\n              children: \"AttributeRuler\"\n            })\n          }), \" can import a \", _jsx(_components.strong, {\n            children: \"tag map and morph\\nrules\"\n          }), \" in the v2.x format via its built-in methods or when the component is\\ninitialized before training. See the\\n\", _jsx(_components.a, {\n            href: \"/usage/v3#migrating-training-mappings-exceptions\",\n            children: \"migration guide\"\n          }), \" for details.\"]\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-vectors-similarity\",\n      children: [_jsx(_components.h2, {\n        id: \"vectors-similarity\",\n        children: \"Word vectors and semantic similarity \"\n      }), _jsx(Vectors101, {}), _jsx(_components.h3, {\n        id: \"adding-vectors\",\n        children: \"Adding word vectors \"\n      }), _jsxs(_components.p, {\n        children: [\"Custom word vectors can be trained using a number of open-source libraries, such\\nas \", _jsx(_components.a, {\n          href: \"https://radimrehurek.com/gensim\",\n          children: \"Gensim\"\n        }), \", \", _jsx(_components.a, {\n          href: \"https://fasttext.cc\",\n          children: \"FastText\"\n        }), \",\\nor Tomas Mikolov’s original\\n\", _jsx(_components.a, {\n          href: \"https://code.google.com/archive/p/word2vec/\",\n          children: \"Word2vec implementation\"\n        }), \". Most\\nword vector libraries output an easy-to-read text-based format, where each line\\nconsists of the word followed by its vector. For everyday use, we want to\\nconvert the vectors into a binary format that loads faster and takes up less\\nspace on disk. The easiest way to do this is the\\n\", _jsx(_components.a, {\n          href: \"/api/cli#init-vectors\",\n          children: _jsx(InlineCode, {\n            children: \"init vectors\"\n          })\n        }), \" command-line utility. This will output a\\nblank spaCy pipeline in the directory \", _jsx(InlineCode, {\n          children: \"/tmp/la_vectors_wiki_lg\"\n        }), \", giving you\\naccess to some nice Latin vectors. You can then pass the directory path to\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#spacy.load\",\n          children: _jsx(InlineCode, {\n            children: \"spacy.load\"\n          })\n        }), \" or use it in the\\n\", _jsx(_components.a, {\n          href: \"/api/data-formats#config-initialize\",\n          children: _jsx(InlineCode, {\n            children: \"[initialize]\"\n          })\n        }), \" of your config when you\\n\", _jsx(_components.a, {\n          href: \"/usage/training\",\n          children: \"train\"\n        }), \" a model.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Usage example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"nlp_latin = spacy.load(\\\"/tmp/la_vectors_wiki_lg\\\")\\ndoc1 = nlp_latin(\\\"Caecilius est in horto\\\")\\ndoc2 = nlp_latin(\\\"servus est in atrio\\\")\\ndoc1.similarity(doc2)\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.la.300.vec.gz\\n$ python -m spacy init vectors en cc.la.300.vec.gz /tmp/la_vectors_wiki_lg\\n\"\n        })\n      }), _jsxs(Accordion, {\n        title: \"How to optimize vector coverage\",\n        id: \"custom-vectors-coverage\",\n        spaced: true,\n        children: [_jsxs(_components.p, {\n          children: [\"To help you strike a good balance between coverage and memory usage, spaCy’s\\n\", _jsx(_components.a, {\n            href: \"/api/vectors\",\n            children: _jsx(InlineCode, {\n              children: \"Vectors\"\n            })\n          }), \" class lets you map \", _jsx(_components.strong, {\n            children: \"multiple keys\"\n          }), \" to the \", _jsx(_components.strong, {\n            children: \"same\\nrow\"\n          }), \" of the table. If you’re using the\\n\", _jsx(_components.a, {\n            href: \"/api/cli#init-vectors\",\n            children: _jsx(InlineCode, {\n              children: \"spacy init vectors\"\n            })\n          }), \" command to create a vocabulary,\\npruning the vectors will be taken care of automatically if you set the \", _jsx(InlineCode, {\n            children: \"--prune\"\n          }), \"\\nflag. You can also do it manually in the following steps:\"]\n        }), _jsxs(_components.ol, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"Start with a \", _jsx(_components.strong, {\n              children: \"word vectors package\"\n            }), \" that covers a huge vocabulary. For\\ninstance, the \", _jsx(_components.a, {\n              href: \"/models/en#en_core_web_lg\",\n              children: _jsx(InlineCode, {\n                children: \"en_core_web_lg\"\n              })\n            }), \" package provides\\n300-dimensional GloVe vectors for 685k terms of English.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"If your vocabulary has values set for the \", _jsx(InlineCode, {\n              children: \"Lexeme.prob\"\n            }), \" attribute, the\\nlexemes will be sorted by descending probability to determine which vectors\\nto prune. Otherwise, lexemes will be sorted by their order in the \", _jsx(InlineCode, {\n              children: \"Vocab\"\n            }), \".\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Call \", _jsx(_components.a, {\n              href: \"/api/vocab#prune_vectors\",\n              children: _jsx(InlineCode, {\n                children: \"Vocab.prune_vectors\"\n              })\n            }), \" with the number of\\nvectors you want to keep.\"]\n          }), \"\\n\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"nlp = spacy.load(\\\"en_core_web_lg\\\")\\nn_vectors = 105000  # number of vectors to keep\\nremoved_words = nlp.vocab.prune_vectors(n_vectors)\\n\\nassert len(nlp.vocab.vectors) \u003c= n_vectors  # unique vectors have been pruned\\nassert nlp.vocab.vectors.n_keys \u003e n_vectors  # but not the total entries\\n\"\n          })\n        }), _jsxs(_components.p, {\n          children: [_jsx(_components.a, {\n            href: \"/api/vocab#prune_vectors\",\n            children: _jsx(InlineCode, {\n              children: \"Vocab.prune_vectors\"\n            })\n          }), \" reduces the current vector\\ntable to a given number of unique entries, and returns a dictionary containing\\nthe removed words, mapped to \", _jsx(InlineCode, {\n            children: \"(string, score)\"\n          }), \" tuples, where \", _jsx(InlineCode, {\n            children: \"string\"\n          }), \" is the\\nentry the removed word was mapped to and \", _jsx(InlineCode, {\n            children: \"score\"\n          }), \" the similarity score between\\nthe two words.\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            title: \"Removed words\",\n            children: \"{\\n    \\\"Shore\\\": (\\\"coast\\\", 0.732257),\\n    \\\"Precautionary\\\": (\\\"caution\\\", 0.490973),\\n    \\\"hopelessness\\\": (\\\"sadness\\\", 0.742366),\\n    \\\"Continous\\\": (\\\"continuous\\\", 0.732549),\\n    \\\"Disemboweled\\\": (\\\"corpse\\\", 0.499432),\\n    \\\"biostatistician\\\": (\\\"scientist\\\", 0.339724),\\n    \\\"somewheres\\\": (\\\"somewheres\\\", 0.402736),\\n    \\\"observing\\\": (\\\"observe\\\", 0.823096),\\n    \\\"Leaving\\\": (\\\"leaving\\\", 1.0),\\n}\\n\"\n          })\n        }), _jsxs(_components.p, {\n          children: [\"In the example above, the vector for “Shore” was removed and remapped to the\\nvector of “coast”, which is deemed about 73% similar. “Leaving” was remapped to\\nthe vector of “leaving”, which is identical. If you’re using the\\n\", _jsx(_components.a, {\n            href: \"/api/cli#init-vectors\",\n            children: _jsx(InlineCode, {\n              children: \"init vectors\"\n            })\n          }), \" command, you can set the \", _jsx(InlineCode, {\n            children: \"--prune\"\n          }), \"\\noption to easily reduce the size of the vectors as you add them to a spaCy\\npipeline:\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-bash\",\n            lang: \"bash\",\n            children: \"$ python -m spacy init vectors en la.300d.vec.tgz /tmp/la_vectors_web_md --prune 10000\\n\"\n          })\n        }), _jsx(_components.p, {\n          children: \"This will create a blank spaCy pipeline with vectors for the first 10,000 words\\nin the vectors. All other words in the vectors are mapped to the closest vector\\namong those retained.\"\n        })]\n      }), _jsx(_components.h3, {\n        id: \"adding-individual-vectors\",\n        children: \"Adding vectors individually \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"vector\"\n        }), \" attribute is a \", _jsx(_components.strong, {\n          children: \"read-only\"\n        }), \" numpy or cupy array (depending on\\nwhether you’ve configured spaCy to use GPU memory), with dtype \", _jsx(InlineCode, {\n          children: \"float32\"\n        }), \". The\\narray is read-only so that spaCy can avoid unnecessary copy operations where\\npossible. You can modify the vectors via the \", _jsx(_components.a, {\n          href: \"/api/vocab\",\n          children: _jsx(InlineCode, {\n            children: \"Vocab\"\n          })\n        }), \" or\\n\", _jsx(_components.a, {\n          href: \"/api/vectors\",\n          children: _jsx(InlineCode, {\n            children: \"Vectors\"\n          })\n        }), \" table. Using the\\n\", _jsx(_components.a, {\n          href: \"/api/vocab#set_vector\",\n          children: _jsx(InlineCode, {\n            children: \"Vocab.set_vector\"\n          })\n        }), \" method is often the easiest approach\\nif you have vectors in an arbitrary format, as you can read in the vectors with\\nyour own logic, and just set them with a simple loop. This method is likely to\\nbe slower than approaches that work with the whole vectors table at once, but\\nit’s a great approach for once-off conversions before you save out your \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \"\\nobject to disk.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Adding vectors\",\n          children: \"from spacy.vocab import Vocab\\n\\nvector_data = {\\n    \\\"dog\\\": numpy.random.uniform(-1, 1, (300,)),\\n    \\\"cat\\\": numpy.random.uniform(-1, 1, (300,)),\\n    \\\"orange\\\": numpy.random.uniform(-1, 1, (300,))\\n}\\nvocab = Vocab()\\nfor word, vector in vector_data.items():\\n    vocab.set_vector(word, vector)\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-language-data\",\n      children: [_jsx(_components.h2, {\n        id: \"language-data\",\n        children: \"Language Data \"\n      }), _jsx(LanguageData101, {}), _jsx(_components.h3, {\n        id: \"language-subclass\",\n        children: \"Creating a custom language subclass \"\n      }), _jsxs(_components.p, {\n        children: [\"If you want to customize multiple components of the language data or add support\\nfor a custom language or domain-specific “dialect”, you can also implement your\\nown language subclass. The subclass should define two attributes: the \", _jsx(InlineCode, {\n          children: \"lang\"\n        }), \"\\n(unique language code) and the \", _jsx(InlineCode, {\n          children: \"Defaults\"\n        }), \" defining the language data. For an\\noverview of the available attributes that can be overwritten, see the\\n\", _jsx(_components.a, {\n          href: \"/api/language#defaults\",\n          children: _jsx(InlineCode, {\n            children: \"Language.Defaults\"\n          })\n        }), \" documentation.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\n\\nclass CustomEnglishDefaults(English.Defaults):\\n    stop_words = set([\\\"custom\\\", \\\"stop\\\"])\\n\\nclass CustomEnglish(English):\\n    lang = \\\"custom_en\\\"\\n    Defaults = CustomEnglishDefaults\\n\\nnlp1 = English()\\nnlp2 = CustomEnglish()\\n\\nprint(nlp1.lang, [token.is_stop for token in nlp1(\\\"custom stop\\\")])\\nprint(nlp2.lang, [token.is_stop for token in nlp2(\\\"custom stop\\\")])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/top-level#registry\",\n          children: _jsx(InlineCode, {\n            children: \"@spacy.registry.languages\"\n          })\n        }), \" decorator lets you\\nregister a custom language class and assign it a string name. This means that\\nyou can call \", _jsx(_components.a, {\n          href: \"/api/top-level#spacy.blank\",\n          children: _jsx(InlineCode, {\n            children: \"spacy.blank\"\n          })\n        }), \" with your custom\\nlanguage name, and even train pipelines with it and refer to it in your\\n\", _jsx(_components.a, {\n          href: \"/usage/training#config\",\n          children: \"training config\"\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Config usage\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"After registering your custom language class using the \", _jsx(InlineCode, {\n            children: \"languages\"\n          }), \" registry,\\nyou can refer to it in your \", _jsx(_components.a, {\n            href: \"/usage/training#config\",\n            children: \"training config\"\n          }), \". This\\nmeans spaCy will train your pipeline using the custom subclass.\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[nlp]\\nlang = \\\"custom_en\\\"\\n\"\n          })\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"In order to resolve \", _jsx(InlineCode, {\n            children: \"\\\"custom_en\\\"\"\n          }), \" to your subclass, the registered function\\nneeds to be available during training. You can load a Python file containing\\nthe code using the \", _jsx(InlineCode, {\n            children: \"--code\"\n          }), \" argument:\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-bash\",\n            lang: \"bash\",\n            children: \"python -m spacy train config.cfg --code code.py\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Registering a custom language\",\n          highlight: \"7,12-13\",\n          children: \"import spacy\\nfrom spacy.lang.en import English\\n\\nclass CustomEnglishDefaults(English.Defaults):\\n    stop_words = set([\\\"custom\\\", \\\"stop\\\"])\\n\\n@spacy.registry.languages(\\\"custom_en\\\")\\nclass CustomEnglish(English):\\n    lang = \\\"custom_en\\\"\\n    Defaults = CustomEnglishDefaults\\n\\n# This now works! 🎉\\nnlp = spacy.blank(\\\"custom_en\\\")\\n\"\n        })\n      })]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{"title":"Linguistic Features","next":"/usage/rule-based-matching","menu":[["POS Tagging","pos-tagging"],["Morphology","morphology"],["Lemmatization","lemmatization"],["Dependency Parse","dependency-parse"],["Named Entities","named-entities"],["Entity Linking","entity-linking"],["Tokenization","tokenization"],["Merging \u0026 Splitting","retokenization"],["Sentence Segmentation","sbd"],["Mappings \u0026 Exceptions","mappings-exceptions"],["Vectors \u0026 Similarity","vectors-similarity"],["Language Data","language-data"]]},"scope":{}},"sectionTitle":"Usage Documentation","theme":"blue","section":"usage","apiDetails":{"stringName":null,"baseClass":null,"trainable":null},"isIndex":false},"__N_SSG":true},"page":"/[...listPathPage]","query":{"listPathPage":["usage","linguistic-features"]},"buildId":"Ugre-usgT1EZhnSeYcBR9","isFallback":false,"dynamicIds":[728,5492],"gsp":true,"scriptLoader":[]}</script></body></html>