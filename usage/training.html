<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="sitemap" type="application/xml" href="/sitemap.xml"/><link rel="shortcut icon" href="/icons/icon-192x192.png"/><link rel="manifest" href="/manifest.webmanifest"/><meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=5.0, shrink-to-fit=no, viewport-fit=cover"/><meta name="theme-color" content="#09a3d5"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png"/><title>Training Pipelines &amp; Models · spaCy Usage Documentation</title><meta name="description" content="Train and update components on your own data and integrate custom models"/><meta property="og:title" content="Training Pipelines &amp; Models · spaCy Usage Documentation"/><meta property="og:description" content="Train and update components on your own data and integrate custom models"/><meta property="og:type" content="website"/><meta property="og:site_name" content="Training Pipelines &amp; Models"/><meta property="og:image" content="https://spacy.io/_next/static/media/social_default.96b04585.jpg"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image" content="https://spacy.io/_next/static/media/social_default.96b04585.jpg"/><meta name="twitter:creator" content="@spacy_io"/><meta name="twitter:site" content="@spacy_io"/><meta name="twitter:title" content="Training Pipelines &amp; Models · spaCy Usage Documentation"/><meta name="twitter:description" content="Train and update components on your own data and integrate custom models"/><meta name="docsearch:language" content="en"/><meta name="next-head-count" content="24"/><link rel="preload" href="/_next/static/css/8f0b94edbc18d62d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8f0b94edbc18d62d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e6995e0e8addcf99.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e6995e0e8addcf99.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script defer="" src="/_next/static/chunks/262.c647d33d06232ef6.js"></script><script defer="" src="/_next/static/chunks/728.cf6ba0da2700fa1b.js"></script><script src="/_next/static/chunks/webpack-8161fc2bb14cec39.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-a0f603ce323043fd.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eb3ea1261af64e73.js" defer=""></script><script src="/_next/static/chunks/94-57434c8b7a6c3878.js" defer=""></script><script src="/_next/static/chunks/128-76b45627a109219b.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...listPathPage%5D-45eea57fe8c2902c.js" defer=""></script><script src="/_next/static/Ugre-usgT1EZhnSeYcBR9/_buildManifest.js" defer=""></script><script src="/_next/static/Ugre-usgT1EZhnSeYcBR9/_ssgManifest.js" defer=""></script></head><body class="theme-blue"><div id="__next"><div class="theme-blue"><nav class="navigation_root__yPL8O"><span class="navigation_has-alert__s0Drf"><a class="link_root__1Me7D link_no-link-layout__RPvod" aria-label="spaCy" href="/"><h1 class="navigation_title__pm49s">spaCy</h1></a> <span class="navigation_alert__ZOXon"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/usage/v3-5"><strong>💥 Out now:</strong> spaCy v3.5</a></span></span><div class="navigation_menu__ZMJxN"><select class="dropdown_root__3uiQq navigation_dropdown__4j4pI"><option value="title" disabled="">Menu</option><option value="/usage" selected="">Usage</option><option value="/models">Models</option><option value="/api">API</option><option value="/universe">Universe</option></select><ul class="navigation_list__DCzqi"><li class="navigation_item__ln1O1 navigation_is-active__RjVJG"><a class="link_root__1Me7D link_no-link-layout__RPvod" tabindex="-1" href="/usage">Usage</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/models">Models</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/api">API</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/universe">Universe</a></li><li class="navigation_item__ln1O1 navigation_github__MpFNv"><span><a href="https://github.com/explosion/spaCy" data-size="large" data-show-count="true" aria-label="Star spaCy on GitHub"></a></span></li></ul><div class="navigation_search__BKZCn"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div><progress class="progress_root__9huWN" value="0" max="100"></progress></nav><menu class="sidebar sidebar_root__s2No7"><h1 hidden="" aria-hidden="true" class="h0 sidebar_active-heading___dkf9">Guides</h1><div class="sidebar_dropdown__vyqjz"><select class="dropdown_root__3uiQq sidebar_dropdown-select__Nwbq9"><option disabled="">Select page...</option><option value="/usage">Get started<!-- --> › <!-- -->Installation</option><option value="/usage/models">Get started<!-- --> › <!-- -->Models &amp; Languages</option><option value="/usage/facts-figures">Get started<!-- --> › <!-- -->Facts &amp; Figures</option><option value="/usage/spacy-101">Get started<!-- --> › <!-- -->spaCy 101</option><option value="/usage/v3">Get started<!-- --> › <!-- -->New in v3.0</option><option value="/usage/v3-1">Get started<!-- --> › <!-- -->New in v3.1</option><option value="/usage/v3-2">Get started<!-- --> › <!-- -->New in v3.2</option><option value="/usage/v3-3">Get started<!-- --> › <!-- -->New in v3.3</option><option value="/usage/v3-4">Get started<!-- --> › <!-- -->New in v3.4</option><option value="/usage/v3-5">Get started<!-- --> › <!-- -->New in v3.5</option><option value="/usage/linguistic-features">Guides<!-- --> › <!-- -->Linguistic Features</option><option value="/usage/rule-based-matching">Guides<!-- --> › <!-- -->Rule-based Matching</option><option value="/usage/processing-pipelines">Guides<!-- --> › <!-- -->Processing Pipelines</option><option value="/usage/embeddings-transformers">Guides<!-- --> › <!-- -->Embeddings &amp; Transformers</option><option value="/usage/training" selected="">Guides<!-- --> › <!-- -->Training Models</option><option value="/usage/layers-architectures">Guides<!-- --> › <!-- -->Layers &amp; Model Architectures</option><option value="/usage/projects">Guides<!-- --> › <!-- -->spaCy Projects</option><option value="/usage/saving-loading">Guides<!-- --> › <!-- -->Saving &amp; Loading</option><option value="/usage/visualizers">Guides<!-- --> › <!-- -->Visualizers</option><option value="https://github.com/explosion/projects">Resources<!-- --> › <!-- -->Project Templates</option><option value="https://v2.spacy.io">Resources<!-- --> › <!-- -->v2.x Documentation</option><option value="https://explosion.ai/custom-solutions">Resources<!-- --> › <!-- -->Custom Solutions</option></select></div><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Get started</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage">Installation</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/models">Models &amp; Languages</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/facts-figures">Facts &amp; Figures</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/spacy-101">spaCy 101</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3">New in v3.0</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-1">New in v3.1</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-2">New in v3.2</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-3">New in v3.3</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-4">New in v3.4</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/v3-5">New in v3.5</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Guides</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/linguistic-features">Linguistic Features</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/rule-based-matching">Rule-based Matching</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/processing-pipelines">Processing Pipelines</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/embeddings-transformers">Embeddings &amp; Transformers<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP sidebar_is-active__yVTtL is-active" href="/usage/training">Training Models<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a><ul class="sidebar_crumbs__NhM2y"><li class="sidebar_crumb__tiiDl sidebar_crumb-active__zq8BI"><a href="#basics">Introduction</a></li><li class="sidebar_crumb__tiiDl"><a href="#quickstart">Quickstart</a></li><li class="sidebar_crumb__tiiDl"><a href="#config">Config System</a></li><li class="sidebar_crumb__tiiDl"><a href="#training-data">Training Data</a></li><li class="sidebar_crumb__tiiDl"><a href="#config-custom">Custom Training</a></li><li class="sidebar_crumb__tiiDl"><a href="#custom-functions">Custom Functions</a></li><li class="sidebar_crumb__tiiDl"><a href="#initialization">Initialization</a></li><li class="sidebar_crumb__tiiDl"><a href="#data">Data Utilities</a></li><li class="sidebar_crumb__tiiDl"><a href="#parallel-training">Parallel Training</a></li><li class="sidebar_crumb__tiiDl"><a href="#api">Internal API</a></li></ul></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/layers-architectures">Layers &amp; Model Architectures<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/projects">spaCy Projects<span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/saving-loading">Saving &amp; Loading</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/usage/visualizers">Visualizers</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Resources</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/projects">Project Templates</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="https://v2.spacy.io">v2.x Documentation</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="https://explosion.ai/custom-solutions">Custom Solutions</a></li></ul></menu><main class="main_root__7f6Tj main_with-sidebar__uH1df main_with-asides__ikQT6"><article class="main_content__8zFCH"><header class="title_root__pS2WQ"><h1 id="_title" class="typography_heading__D82WZ typography_h1__b7dt9 title_h1__l3CW1"><span class="heading-text">Training Pipelines &amp; Models<!-- --> </span></h1><div class="heading-teaser title_teaser__QhwCH">Train and update components on your own data and integrate custom models</div></header><section id="section-basics" class="section_root__k1hUl"><p>spaCy’s tagger, parser, text categorizer and many other components are powered
by <strong>statistical models</strong>. Every “decision” these components make – for example,
which part-of-speech tag to assign, or whether a word is a named entity – is a
<strong>prediction</strong> based on the model’s current <strong>weight values</strong>. The weight values
are estimated based on examples the model has seen during <strong>training</strong>. To train
a model, you first need training data – examples of text, and the labels you
want the model to predict. This could be a part-of-speech tag, a named entity or
any other information.</p>
<p>Training is an iterative process in which the model’s predictions are compared
against the reference annotations in order to estimate the <strong>gradient of the
loss</strong>. The gradient of the loss is then used to calculate the gradient of the
weights through <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/backprop101">backpropagation</a>. The
gradients indicate how the weight values should be changed so that the model’s
predictions become more similar to the reference labels over time.</p>
<aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><strong>Training data:</strong> Examples and their annotations.</li>
<li class="list_li__sfx_z"><strong>Text:</strong> The input text the model should predict a label for.</li>
<li class="list_li__sfx_z"><strong>Label:</strong> The label the model should predict.</li>
<li class="list_li__sfx_z"><strong>Gradient:</strong> The direction and rate of change for a numeric value.
Minimising the gradient of the weights should result in predictions that are
closer to the reference labels on the training data.</li>
</ul>
</div></div></aside>
<figure class="gatsby-resp-image-figure"><img class="embed_image__mSQUH" src="/images/training.svg" alt="The training process" width="650" height="auto"/></figure>
<p>When training a model, we don’t just want it to memorize our examples – we want
it to come up with a theory that can be <strong>generalized across unseen data</strong>.
After all, we don’t just want the model to learn that this one instance of
“Amazon” right here is a company – we want it to learn that “Amazon”, in
contexts <em>like this</em>, is most likely a company. That’s why the training data
should always be representative of the data we want to process. A model trained
on Wikipedia, where sentences in the first person are extremely rare, will
likely perform badly on Twitter. Similarly, a model trained on romantic novels
will likely perform badly on legal text.</p>
<p>This also means that in order to know how the model is performing, and whether
it’s learning the right things, you don’t only need <strong>training data</strong> – you’ll
also need <strong>evaluation data</strong>. If you only test the model with the data it was
trained on, you’ll have no idea how well it’s generalizing. If you want to train
a model from scratch, you usually need at least a few hundred examples for both
training and evaluation.</p><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span>Tip: Try the Prodigy annotation tool</span></h4><figure class="gatsby-resp-image-figure"><a class="link_root__1Me7D gatsby-resp-image-link embed_image-link__loaAa link_no-link-layout__RPvod" href="https://prodi.gy"><img class="embed_image__mSQUH" src="/images/prodigy.jpg" alt="Prodigy: Radically efficient machine teaching" width="650" height="auto"/></a></figure><p>If you need to label a lot of data, check out <a class="link_root__1Me7D" href="https://prodi.gy">Prodigy</a>, a
new, active learning-powered annotation tool we’ve developed. Prodigy is fast
and extensible, and comes with a modern <strong>web application</strong> that helps you
collect training data faster. It integrates seamlessly with spaCy, pre-selects
the <strong>most relevant examples</strong> for annotation, and lets you train and evaluate
ready-to-use spaCy pipelines.</p></aside></section>
<section id="section-quickstart" class="section_root__k1hUl"><h2 id="quickstart" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#quickstart" class="heading-text typography_permalink__UiIRy">Quickstart <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH">new</span></h2><p>The recommended way to train your spaCy pipelines is via the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a> command on the command line. It only needs a
single <a class="link_root__1Me7D" href="/usage/training#config"><code class="code_inline-code__Bq7ot">config.cfg</code></a> <strong>configuration file</strong> that includes all settings
and hyperparameters. You can optionally <a class="link_root__1Me7D" href="/usage/training#config-overrides">overwrite</a> settings
on the command line, and load in a Python file to register
<a class="link_root__1Me7D" href="/usage/training#custom-code">custom functions</a> and architectures. This quickstart widget helps
you generate a starter config with the <strong>recommended settings</strong> for your
specific use case. It’s also available in spaCy as the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-config"><code class="code_inline-code__Bq7ot">init config</code></a> command.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><p>Upgrade to the <a class="link_root__1Me7D" href="/usage">latest version of spaCy</a> to use the quickstart widget.
For earlier releases, follow the CLI instructions to generate a compatible
config.</p></aside><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Instructions: widget<!-- --> </span></h4>
<ol class="list_ol__aclSa">
<li class="list_li__sfx_z">Select your requirements and settings.</li>
<li class="list_li__sfx_z">Use the buttons at the bottom to save the result to your clipboard or a
file <code class="code_inline-code__Bq7ot">base_config.cfg</code>.</li>
<li class="list_li__sfx_z">Run <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-fill-config"><code class="code_inline-code__Bq7ot">init fill-config</code></a> to create a full
config.</li>
<li class="list_li__sfx_z">Run <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">train</code></a> with your config and data.</li>
</ol>
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Instructions: CLI<!-- --> </span></h4>
<ol class="list_ol__aclSa">
<li class="list_li__sfx_z">Run the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-config"><code class="code_inline-code__Bq7ot">init config</code></a> command and specify your
requirements and settings as CLI arguments.</li>
<li class="list_li__sfx_z">Run <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">train</code></a> with the exported config and data.</li>
</ol>
</div></div></aside><p>After you’ve saved the starter config to a file <code class="code_inline-code__Bq7ot">base_config.cfg</code>, you can use
the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-fill-config"><code class="code_inline-code__Bq7ot">init fill-config</code></a> command to fill in the
remaining defaults. Training configs should always be <strong>complete and without
hidden defaults</strong>, to keep your experiments reproducible.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Tip: Debug your data<!-- --> </span></h4>
<p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#debug-data"><code class="code_inline-code__Bq7ot">debug data</code> command</a> lets you analyze and validate
your training and development data, get useful stats, and find problems like
invalid entity annotations, cyclic dependencies, low data labels and more.</p>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre>
</div></div></aside><p>Instead of exporting your starter config from the quickstart widget and
auto-filling it, you can also use the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-config"><code class="code_inline-code__Bq7ot">init config</code></a>
command and specify your requirement and settings as CLI arguments. You can now
add your data and run <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">train</code></a> with your config. See the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#convert"><code class="code_inline-code__Bq7ot">convert</code></a> command for details on how to convert your data to
spaCy’s binary <code class="code_inline-code__Bq7ot">.spacy</code> format. You can either include the data paths in the
<code class="code_inline-code__Bq7ot">[paths]</code> section of your config, or pass them in via the command line.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Tip: Enable your GPU<!-- --> </span></h4>
<p>Use the <code class="code_inline-code__Bq7ot">--gpu-id</code> option to select the GPU:</p>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre>
</div></div></aside><section class="accordion" id="quickstart-source"><div class="accordion_root__pPltq accordion_spaced__Ebyjn"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">How are the config recommendations generated?</span><a class="link_root__1Me7D accordion_anchor__kidBh link_no-link-layout__RPvod" href="/usage/training#quickstart-source">¶</a></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><p>The recommended config settings generated by the quickstart widget and the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-config"><code class="code_inline-code__Bq7ot">init config</code></a> command are based on some general <strong>best
practices</strong> and things we’ve found to work well in our experiments. The goal is
to provide you with the most <strong>useful defaults</strong>.</p><p>Under the hood, the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/cli/templates/quickstart_training.jinja"><code class="code_inline-code__Bq7ot">quickstart_training.jinja</code></a>
template defines the different combinations – for example, which parameters to
change if the pipeline should optimize for efficiency vs. accuracy. The file
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/cli/templates/quickstart_training_recommendations.yml"><code class="code_inline-code__Bq7ot code_wrap__b41os">quickstart_training_recommendations.yml</code></a>
collects the recommended settings and available resources for each language
including the different transformer weights. For some languages, we include
different transformer recommendations, depending on whether you want the model
to be more efficient or more accurate. The recommendations will be <strong>evolving</strong>
as we run more experiments.</p></div></div></section><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">🪐</span>Get started with a project template<!-- -->:<!-- --> <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/projects/tree/v3/pipelines/tagger_parser_ud"><code class="code_inline-code__Bq7ot">pipelines/tagger_parser_ud</code></a></span></h4><p>The easiest way to get started is to clone a <a class="link_root__1Me7D" href="/usage/projects">project template</a>
and run it – for example, this end-to-end template that lets you train a
<strong>part-of-speech tagger</strong> and <strong>dependency parser</strong> on a Universal Dependencies
treebank.</p><div class="copy_root__9E6qI"><span class="copy_prefix__p_JKI">$</span><textarea readonly="" class="copy_textarea__ATeHi" rows="1" aria-label="Example bash command to start with an end-to-end template">python -m spacy project clone pipelines/tagger_parser_ud</textarea></div></aside></section>
<section id="section-config" class="section_root__k1hUl"><h2 id="config" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#config" class="heading-text typography_permalink__UiIRy">Training config system <!-- --> </a></h2><p>Training config files include all <strong>settings and hyperparameters</strong> for training
your pipeline. Instead of providing lots of arguments on the command line, you
only need to pass your <code class="code_inline-code__Bq7ot">config.cfg</code> file to <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a>.
Under the hood, the training config uses the
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/usage-config">configuration system</a> provided by our
machine learning library <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai">Thinc</a>. This also makes it easy to
integrate custom models and architectures, written in your framework of choice.
Some of the main advantages and features of spaCy’s training config are:</p><ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><strong>Structured sections.</strong> The config is grouped into sections, and nested
sections are defined using the <code class="code_inline-code__Bq7ot">.</code> notation. For example, <code class="code_inline-code__Bq7ot">[components.ner]</code>
defines the settings for the pipeline’s named entity recognizer. The config
can be loaded as a Python dict.</li>
<li class="list_li__sfx_z"><strong>References to registered functions.</strong> Sections can refer to registered
functions like <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/architectures"><span class="link_source-text__VDP74">model architectures</span></a>,
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-optimizers">optimizers</a> or
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-schedules">schedules</a> and define arguments that are
passed into them. You can also
<a class="link_root__1Me7D" href="/usage/training#custom-functions">register your own functions</a> to define custom
architectures or methods, reference them in your config and tweak their
parameters.</li>
<li class="list_li__sfx_z"><strong>Interpolation.</strong> If you have hyperparameters or other settings used by
multiple components, define them once and reference them as
<a class="link_root__1Me7D" href="/usage/training#config-interpolation">variables</a>.</li>
<li class="list_li__sfx_z"><strong>Reproducibility with no hidden defaults.</strong> The config file is the “single
source of truth” and includes all settings.</li>
<li class="list_li__sfx_z"><strong>Automated checks and validation.</strong> When you load a config, spaCy checks if
the settings are complete and if all values have the correct types. This lets
you catch potential mistakes early. In your custom architectures, you can use
Python <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/typing.html">type hints</a> to tell the
config which types of data to expect.</li>
</ul><pre class="code_pre__kzg60"><header class="code_header__Jc0Q1"><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/default_config.cfg"><code class="code_inline-code__Bq7ot code_inline-code-dark__ZEbch">explosion/spaCy/master/spacy/default_config.cfg</code></a></header><code class="code_code__CILJL code_code__CILJL code_max-height__093k6 code_code__CILJL language-ini language-ini code_max-height__093k6 language-ini"></code></pre><p>Under the hood, the config is parsed into a dictionary. It’s divided into
sections and subsections, indicated by the square brackets and dot notation. For
example, <code class="code_inline-code__Bq7ot">[training]</code> is a section and <code class="code_inline-code__Bq7ot">[training.batch_size]</code> a subsection.
Subsections can define values, just like a dictionary, or use the <code class="code_inline-code__Bq7ot">@</code> syntax to
refer to <a class="link_root__1Me7D" href="/usage/training#config-functions">registered functions</a>. This allows the config to
not just define static settings, but also construct objects like architectures,
schedules, optimizers or any other custom components. The main top-level
sections of a config file are:</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Section</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">nlp</code></td><td class="table_td__rmpJx">Definition of the <code class="code_inline-code__Bq7ot">nlp</code> object, its tokenizer and <a class="link_root__1Me7D" href="/usage/processing-pipelines">processing pipeline</a> component names.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">components</code></td><td class="table_td__rmpJx">Definitions of the <a class="link_root__1Me7D" href="/usage/processing-pipelines">pipeline components</a> and their models.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">paths</code></td><td class="table_td__rmpJx">Paths to data and other assets. Re-used across the config as variables, e.g. <code class="code_inline-code__Bq7ot">${paths.train}</code>, and can be <a class="link_root__1Me7D" href="/usage/training#config-overrides">overwritten</a> on the CLI.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">system</code></td><td class="table_td__rmpJx">Settings related to system and hardware. Re-used across the config as variables, e.g. <code class="code_inline-code__Bq7ot">${system.seed}</code>, and can be <a class="link_root__1Me7D" href="/usage/training#config-overrides">overwritten</a> on the CLI.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">training</code></td><td class="table_td__rmpJx">Settings and controls for the training and evaluation process.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">pretraining</code></td><td class="table_td__rmpJx">Optional settings and controls for the <a class="link_root__1Me7D" href="/usage/embeddings-transformers#pretraining">language model pretraining</a>.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">initialize</code></td><td class="table_td__rmpJx">Data resources and arguments passed to components when <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#initialize"><code class="code_inline-code__Bq7ot">nlp.initialize</code></a> is called before training (but not at runtime).</td></tr></tbody></table><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">📖</span>Config format and settings</span></h4><p>For a full overview of spaCy’s config format and settings, see the
<a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/data-formats#config"><span class="link_source-text__VDP74">data format documentation</span></a> and
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/usage-config">Thinc’s config system docs</a>. The settings
available for the different architectures are documented with the
<a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/architectures"><span class="link_source-text__VDP74">model architectures API</span></a>. See the Thinc documentation for
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-optimizers">optimizers</a> and
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-schedules">schedules</a>.</p></aside><figure class="embed_root__mluJp embed_responsive__k3do3 embed_ratio16x9__iHV71"><iframe class="embed_iframe__YqdML" title="BWhh3r6W-qE" src="https://www.youtube-nocookie.com/embed/BWhh3r6W-qE" frameBorder="0" height="500" allowfullscreen=""></iframe></figure><h3 id="config-lifecycle" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#config-lifecycle" class="heading-text typography_permalink__UiIRy">Config lifecycle at runtime and training <!-- --> </a></h3><p>A pipeline’s <code class="code_inline-code__Bq7ot">config.cfg</code> is considered the “single source of truth”, both at
<strong>training</strong> and <strong>runtime</strong>. Under the hood,
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#from_config"><code class="code_inline-code__Bq7ot">Language.from_config</code></a> takes care of constructing
the <code class="code_inline-code__Bq7ot">nlp</code> object using the settings defined in the config. An <code class="code_inline-code__Bq7ot">nlp</code> object’s
config is available as <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#config"><code class="code_inline-code__Bq7ot">nlp.config</code></a> and it includes all
information about the pipeline, as well as the settings used to train and
initialize it.</p><figure class="gatsby-resp-image-figure"><img class="embed_image__mSQUH" src="/images/lifecycle.svg" alt="Illustration of pipeline lifecycle" width="650" height="auto"/></figure><p>At runtime spaCy will only use the <code class="code_inline-code__Bq7ot">[nlp]</code> and <code class="code_inline-code__Bq7ot">[components]</code> blocks of the
config and load all data, including tokenization rules, model weights and other
resources from the pipeline directory. The <code class="code_inline-code__Bq7ot">[training]</code> block contains the
settings for training the model and is only used during training. Similarly, the
<code class="code_inline-code__Bq7ot">[initialize]</code> block defines how the initial <code class="code_inline-code__Bq7ot">nlp</code> object should be set up
before training and whether it should be initialized with vectors or pretrained
tok2vec weights, or any other data needed by the components.</p><p>The initialization settings are only loaded and used when
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#initialize"><code class="code_inline-code__Bq7ot">nlp.initialize</code></a> is called (typically right before
training). This allows you to set up your pipeline using local data resources
and custom functions, and preserve the information in your config – but without
requiring it to be available at runtime. You can also use this mechanism to
provide data paths to custom pipeline components and custom tokenizers – see the
section on <a class="link_root__1Me7D" href="/usage/training#initialization">custom initialization</a> for details.</p><h3 id="config-overrides" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#config-overrides" class="heading-text typography_permalink__UiIRy">Overwriting config settings on the command line <!-- --> </a></h3><p>The config system means that you can define all settings <strong>in one place</strong> and in
a consistent format. There are no command-line arguments that need to be set,
and no hidden defaults. However, there can still be scenarios where you may want
to override config settings when you run <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a>. This
includes <strong>file paths</strong> to vectors or other resources that shouldn’t be
hard-coded in a config file, or <strong>system-dependent settings</strong>.</p><p>For cases like this, you can set additional command-line options starting with
<code class="code_inline-code__Bq7ot">--</code> that correspond to the config section and value to override. For example,
<code class="code_inline-code__Bq7ot code_wrap__b41os">--paths.train ./corpus/train.spacy</code> sets the <code class="code_inline-code__Bq7ot">train</code> value in the <code class="code_inline-code__Bq7ot">[paths]</code>
block.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><p>Only existing sections and values in the config can be overwritten. At the end
of the training, the final filled <code class="code_inline-code__Bq7ot">config.cfg</code> is exported with your pipeline,
so you’ll always have a record of the settings that were used, including your
overrides. Overrides are added before <a class="link_root__1Me7D" href="/usage/training#config-interpolation">variables</a> are
resolved, by the way – so if you need to use a value in multiple places,
reference it across your config and override it on the CLI once.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">💡 Tip: Verbose logging<!-- --> </span></h4>
<p>If you’re using config overrides, you can set the <code class="code_inline-code__Bq7ot">--verbose</code> flag on
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a> to make spaCy log more info, including which
overrides were set via the CLI and environment variables.</p>
</div></div></aside><h4 id="config-overrides-env" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#config-overrides-env" class="heading-text typography_permalink__UiIRy">Adding overrides via environment variables <!-- --> </a></h4><p>Instead of defining the overrides as CLI arguments, you can also use the
<code class="code_inline-code__Bq7ot">SPACY_CONFIG_OVERRIDES</code> environment variable using the same argument syntax.
This is especially useful if you’re training models as part of an automated
process. Environment variables <strong>take precedence</strong> over CLI overrides and values
defined in the config file.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><h3 id="config-stdin" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#config-stdin" class="heading-text typography_permalink__UiIRy">Reading from standard input <!-- --> </a></h3><p>Setting the config path to <code class="code_inline-code__Bq7ot">-</code> on the command line lets you read the config from
standard input and pipe it forward from a different process, like
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-config"><code class="code_inline-code__Bq7ot">init config</code></a> or your own custom script. This is
especially useful for quick experiments, as it lets you generate a config on the
fly without having to save to and load from disk.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">💡 Tip: Writing to stdout<!-- --> </span></h4>
<p>When you run <code class="code_inline-code__Bq7ot">init config</code>, you can set the output path to <code class="code_inline-code__Bq7ot">-</code> to write to
stdout. In a custom script, you can print the string config, e.g.
<code class="code_inline-code__Bq7ot">print(nlp.config.to_str())</code>.</p>
</div></div></aside><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><h3 id="config-interpolation" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#config-interpolation" class="heading-text typography_permalink__UiIRy">Using variable interpolation <!-- --> </a></h3><p>Another very useful feature of the config system is that it supports variable
interpolation for both <strong>values and sections</strong>. This means that you only need to
define a setting once and can reference it across your config using the
<code class="code_inline-code__Bq7ot">${section.value}</code> syntax. In this example, the value of <code class="code_inline-code__Bq7ot">seed</code> is reused within
the <code class="code_inline-code__Bq7ot">[training]</code> block, and the whole block of <code class="code_inline-code__Bq7ot">[training.optimizer]</code> is reused
in <code class="code_inline-code__Bq7ot">[pretraining]</code> and will become <code class="code_inline-code__Bq7ot">pretraining.optimizer</code>.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">config.cfg (excerpt)</h4><code class="code_code__CILJL language-ini language-ini code_wrap__b41os"></code></pre><p>You can also use variables inside strings. In that case, it works just like
f-strings in Python. If the value of a variable is not a string, it’s converted
to a string.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">💡</span>Tip: Override variables on the CLI</span></h4><p>If you need to change certain values between training runs, you can define them
once, reference them as variables and then <a class="link_root__1Me7D" href="/usage/training#config-overrides">override</a> them on
the CLI. For example, <code class="code_inline-code__Bq7ot">--paths.root /other/root</code> will change the value of <code class="code_inline-code__Bq7ot">root</code>
in the block <code class="code_inline-code__Bq7ot">[paths]</code> and the change will be reflected across all other values
that reference this variable.</p></aside></section>
<section id="section-training-data" class="section_root__k1hUl"><h2 id="training-data" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#training-data" class="heading-text typography_permalink__UiIRy">Preparing Training Data <!-- --> </a></h2><p>Training data for NLP projects comes in many different formats. For some common
formats such as CoNLL, spaCy provides <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/cli#convert"><span class="link_source-text__VDP74">converters</span></a> you can use
from the command line. In other cases you’ll have to prepare the training data
yourself.</p><p>When converting training data for use in spaCy, the main thing is to create
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> objects just like the results you want as output from the
pipeline. For example, if you’re creating an NER pipeline, loading your
annotations and setting them as the <code class="code_inline-code__Bq7ot">.ents</code> property on a <code class="code_inline-code__Bq7ot">Doc</code> is all you need
to worry about. On disk the annotations will be saved as a
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/docbin"><code class="code_inline-code__Bq7ot">DocBin</code></a> in the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/data-formats#binary-training"><code class="code_inline-code__Bq7ot">.spacy</code> format</a>, but the details of that
are handled automatically.</p><p>Here’s an example of creating a <code class="code_inline-code__Bq7ot">.spacy</code> file from some NER annotations.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">preprocess.py</h4><code class="code_code__CILJL language-python language-python"></code></pre><p>For more examples of how to convert training data from a wide variety of formats
for use with spaCy, look at the preprocessing steps in the
<a class="link_root__1Me7D link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/projects/tree/v3/tutorials"><span class="link_source-text__VDP74">tutorial projects</span></a>.</p><section class="accordion" id="json-annotations"><div class="accordion_root__pPltq accordion_spaced__Ebyjn"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">What about the spaCy JSON format?</span><a class="link_root__1Me7D accordion_anchor__kidBh link_no-link-layout__RPvod" href="/usage/training#json-annotations">¶</a></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><p>In spaCy v2, the recommended way to store training data was in
<a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/data-formats#json-input"><span class="link_source-text__VDP74">a particular JSON format</span></a>, but in v3 this format
is deprecated. It’s fine as a readable storage format, but there’s no need to
convert your data to JSON before creating a <code class="code_inline-code__Bq7ot">.spacy</code> file.</p></div></div></section></section>
<section id="section-config-custom" class="section_root__k1hUl"><h2 id="config-custom" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#config-custom" class="heading-text typography_permalink__UiIRy">Customizing the pipeline and training <!-- --> </a></h2><h3 id="config-components" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#config-components" class="heading-text typography_permalink__UiIRy">Defining pipeline components <!-- --> </a></h3><p>You typically train a <a class="link_root__1Me7D" href="/usage/processing-pipelines">pipeline</a> of <strong>one or more
components</strong>. The <code class="code_inline-code__Bq7ot">[components]</code> block in the config defines the available
pipeline components and how they should be created – either by a built-in or
custom <a class="link_root__1Me7D" href="/usage/processing-pipelines#built-in">factory</a>, or
<a class="link_root__1Me7D" href="/usage/processing-pipelines#sourced-components">sourced</a> from an existing
trained pipeline. For example, <code class="code_inline-code__Bq7ot">[components.parser]</code> defines the component named
<code class="code_inline-code__Bq7ot">&quot;parser&quot;</code> in the pipeline. There are different ways you might want to treat
your components during training, and the most common scenarios are:</p><ol class="list_ol__aclSa">
<li class="list_li__sfx_z">Train a <strong>new component</strong> from scratch on your data.</li>
<li class="list_li__sfx_z">Update an existing <strong>trained component</strong> with more examples.</li>
<li class="list_li__sfx_z">Include an existing trained component without updating it.</li>
<li class="list_li__sfx_z">Include a non-trainable component, like a rule-based
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/entityruler"><code class="code_inline-code__Bq7ot">EntityRuler</code></a> or <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/sentencizer"><code class="code_inline-code__Bq7ot">Sentencizer</code></a>, or a
fully <a class="link_root__1Me7D" href="/usage/processing-pipelines#custom-components">custom component</a>.</li>
</ol><p>If a component block defines a <code class="code_inline-code__Bq7ot">factory</code>, spaCy will look it up in the
<a class="link_root__1Me7D" href="/usage/processing-pipelines#built-in">built-in</a> or
<a class="link_root__1Me7D" href="/usage/processing-pipelines#custom-components">custom</a> components and create a
new component from scratch. All settings defined in the config block will be
passed to the component factory as arguments. This lets you configure the model
settings and hyperparameters. If a component block defines a <code class="code_inline-code__Bq7ot">source</code>, the
component will be copied over from an existing trained pipeline, with its
existing weights. This lets you include an already trained component in your
pipeline, or update a trained component with more data specific to your use
case.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">config.cfg (excerpt)</h4><code class="code_code__CILJL language-ini language-ini"></code></pre><p>The <code class="code_inline-code__Bq7ot">pipeline</code> setting in the <code class="code_inline-code__Bq7ot">[nlp]</code> block defines the pipeline components
added to the pipeline, in order. For example, <code class="code_inline-code__Bq7ot">&quot;parser&quot;</code> here references
<code class="code_inline-code__Bq7ot">[components.parser]</code>. By default, spaCy will <strong>update all components that can
be updated</strong>. Trainable components that are created from scratch are initialized
with random weights. For sourced components, spaCy will keep the existing
weights and <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/language#resume_training"><span class="link_source-text__VDP74">resume training</span></a>.</p><p>If you don’t want a component to be updated, you can <strong>freeze</strong> it by adding it
to the <code class="code_inline-code__Bq7ot">frozen_components</code> list in the <code class="code_inline-code__Bq7ot">[training]</code> block. Frozen components are
<strong>not updated</strong> during training and are included in the final trained pipeline
as-is. They are also excluded when calling
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#initialize"><code class="code_inline-code__Bq7ot">nlp.initialize</code></a>.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Note on frozen components<!-- --> </span></h4>
<p>Even though frozen components are not <strong>updated</strong> during training, they will
still <strong>run</strong> during evaluation. This is very important, because they may
still impact your model’s performance – for instance, a sentence boundary
detector can impact what the parser or entity recognizer considers a valid
parse. So the evaluation results should always reflect what your pipeline will
produce at runtime. If you want a frozen component to run (without updating)
during training as well, so that downstream components can use its
<strong>predictions</strong>, you should add it to the list of
<a class="link_root__1Me7D" href="/usage/training#annotating-components"><code class="code_inline-code__Bq7ot">annotating_components</code></a>.</p>
</div></div></aside><pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre><div><a id="config-components-listeners"></a><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Shared Tok2Vec listener layer</span></h4><p>When the components in your pipeline
<a class="link_root__1Me7D" href="/usage/embeddings-transformers#embedding-layers">share an embedding layer</a>, the
<strong>performance</strong> of your frozen component will be <strong>degraded</strong> if you continue
training other layers with the same underlying <code class="code_inline-code__Bq7ot">Tok2Vec</code> instance. As a rule of
thumb, ensure that your frozen components are truly <strong>independent</strong> in the
pipeline.</p><p>To automatically replace a shared token-to-vector listener with an independent
copy of the token-to-vector layer, you can use the <code class="code_inline-code__Bq7ot">replace_listeners</code> setting
of a sourced component, pointing to the listener layer(s) in the config. For
more details on how this works under the hood, see
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#replace_listeners"><code class="code_inline-code__Bq7ot">Language.replace_listeners</code></a>.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre></aside></div><h3 id="annotating-components" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#annotating-components" class="heading-text typography_permalink__UiIRy">Using predictions from preceding components <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="This feature is new and was introduced in spaCy v3.1">v<!-- -->3.1</span></h3><p>By default, components are updated in isolation during training, which means
that they don’t see the predictions of any earlier components in the pipeline. A
component receives <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example"><code class="code_inline-code__Bq7ot">Example.predicted</code></a> as input and compares its
predictions to <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example"><code class="code_inline-code__Bq7ot">Example.reference</code></a> without saving its
annotations in the <code class="code_inline-code__Bq7ot">predicted</code> doc.</p><p>Instead, if certain components should <strong>set their annotations</strong> during training,
use the setting <code class="code_inline-code__Bq7ot">annotating_components</code> in the <code class="code_inline-code__Bq7ot">[training]</code> block to specify a
list of components. For example, the feature <code class="code_inline-code__Bq7ot">DEP</code> from the parser could be used
as a tagger feature by including <code class="code_inline-code__Bq7ot">DEP</code> in the tok2vec <code class="code_inline-code__Bq7ot">attrs</code> and including
<code class="code_inline-code__Bq7ot">parser</code> in <code class="code_inline-code__Bq7ot">annotating_components</code>:</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">config.cfg (excerpt)</h4><code class="code_code__CILJL language-ini language-ini code_wrap__b41os"></code></pre><p>Any component in the pipeline can be included as an annotating component,
including frozen components. Frozen components can set annotations during
training just as they would set annotations during evaluation or when the final
pipeline is run. The config excerpt below shows how a frozen <code class="code_inline-code__Bq7ot">ner</code> component and
a <code class="code_inline-code__Bq7ot">sentencizer</code> can provide the required <code class="code_inline-code__Bq7ot">doc.sents</code> and <code class="code_inline-code__Bq7ot">doc.ents</code> for the
entity linker during training:</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">config.cfg (excerpt)</h4><code class="code_code__CILJL language-ini language-ini"></code></pre><p>Similarly, a pretrained <code class="code_inline-code__Bq7ot">tok2vec</code> layer can be frozen and specified in the list
of <code class="code_inline-code__Bq7ot">annotating_components</code> to ensure that a downstream component can use the
embedding layer without updating it.</p><div><a id="annotating-components-speed"></a><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Training speed with annotating components</span></h4><p>Be aware that non-frozen annotating components with statistical models will
<strong>run twice</strong> on each batch, once to update the model and once to apply the
now-updated model to the predicted docs.</p></aside></div><h3 id="config-functions" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#config-functions" class="heading-text typography_permalink__UiIRy">Using registered functions <!-- --> </a></h3><p>The training configuration defined in the config file doesn’t have to only
consist of static values. Some settings can also be <strong>functions</strong>. For instance,
the <code class="code_inline-code__Bq7ot">batch_size</code> can be a number that doesn’t change, or a schedule, like a
sequence of compounding values, which has shown to be an effective trick (see
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://arxiv.org/abs/1711.00489">Smith et al., 2017</a>).</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">With static value</h4><code class="code_code__CILJL language-ini language-ini"></code></pre><p>To refer to a function instead, you can make <code class="code_inline-code__Bq7ot">[training.batch_size]</code> its own
section and use the <code class="code_inline-code__Bq7ot">@</code> syntax to specify the function and its arguments – in
this case <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-schedules#compounding"><code class="code_inline-code__Bq7ot">compounding.v1</code></a>
defined in the <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/top-level#registry"><span class="link_source-text__VDP74">function registry</span></a>. All other values
defined in the block are passed to the function as keyword arguments when it’s
initialized. You can also use this mechanism to register
<a class="link_root__1Me7D" href="/usage/training#custom-functions">custom implementations and architectures</a> and reference them
from your configs.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">How the config is resolved<!-- --> </span></h4>
<p>The config file is parsed into a regular dictionary and is resolved and
validated <strong>bottom-up</strong>. Arguments provided for registered functions are
checked against the function’s signature and type annotations. The return
value of a registered function can also be passed into another function – for
instance, a learning rate schedule can be provided as the an argument of an
optimizer.</p>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">With registered function</h4><code class="code_code__CILJL language-ini language-ini"></code></pre><h3 id="model-architectures" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#model-architectures" class="heading-text typography_permalink__UiIRy">Model architectures <!-- --> </a></h3><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">💡 Model type annotations<!-- --> </span></h4>
<p>In the documentation and code base, you may come across type annotations and
descriptions of <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai">Thinc</a> model types, like <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-model">Model</a><span class="code_cli-arg-subtle__IgB5m">[</span>List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" href="/api/doc">Doc</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span>
List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-types#types">Floats2d</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span>. This so-called generic type describes the layer and its
input and output type – in this case, it takes a list of <code class="code_inline-code__Bq7ot">Doc</code> objects as the
input and list of 2-dimensional arrays of floats as the output. You can read
more about defining Thinc models <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/usage-models">here</a>.
Also see the <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/usage-type-checking">type checking</a> for
how to enable linting in your editor to see live feedback if your inputs and
outputs don’t match.</p>
</div></div></aside><p>A <strong>model architecture</strong> is a function that wires up a Thinc
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-model"><code class="code_inline-code__Bq7ot">Model</code></a> instance, which you can then use in a
component or as a layer of a larger network. You can use Thinc as a thin
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/usage-frameworks">wrapper around frameworks</a> such as
PyTorch, TensorFlow or MXNet, or you can implement your logic in Thinc
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/usage-models">directly</a>. For more details and examples,
see the usage guide on <a class="link_root__1Me7D" href="/usage/layers-architectures">layers and architectures</a>.</p><p>spaCy’s built-in components will never construct their <code class="code_inline-code__Bq7ot">Model</code> instances
themselves, so you won’t have to subclass the component to change its model
architecture. You can just <strong>update the config</strong> so that it refers to a
different registered function. Once the component has been created, its <code class="code_inline-code__Bq7ot">Model</code>
instance has already been assigned, so you cannot change its model architecture.
The architecture is like a recipe for the network, and you can’t change the
recipe once the dish has already been prepared. You have to make a new one.
spaCy includes a variety of built-in <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/architectures"><span class="link_source-text__VDP74">architectures</span></a> for
different tasks. For example:</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Architecture</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#HashEmbedCNN"><span class="link_source-text__VDP74">HashEmbedCNN</span></a></td><td class="table_td__rmpJx">Build spaCy’s “standard” embedding layer, which uses hash embedding with subword features and a CNN with layer-normalized maxout. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-model">Model</a><span class="code_cli-arg-subtle__IgB5m">[</span>List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" href="/api/doc">Doc</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-types#types">Floats2d</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#TransitionBasedParser"><span class="link_source-text__VDP74">TransitionBasedParser</span></a></td><td class="table_td__rmpJx">Build a <a class="link_root__1Me7D" href="https://explosion.ai/blog/parsing-english-in-python">transition-based parser</a> model used in the default <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/entityrecognizer"><code class="code_inline-code__Bq7ot">EntityRecognizer</code></a> and <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/dependencyparser"><code class="code_inline-code__Bq7ot">DependencyParser</code></a>. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-model">Model</a><span class="code_cli-arg-subtle__IgB5m">[</span>List<span class="code_cli-arg-subtle__IgB5m">[</span>Docs<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> List<span class="code_cli-arg-subtle__IgB5m">[</span>List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-types#types">Floats2d</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/architectures#TextCatEnsemble"><span class="link_source-text__VDP74">TextCatEnsemble</span></a></td><td class="table_td__rmpJx">Stacked ensemble of a bag-of-words model and a neural network model with an internal CNN embedding layer. Used in the default <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/textcategorizer"><code class="code_inline-code__Bq7ot">TextCategorizer</code></a>. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-model">Model</a><span class="code_cli-arg-subtle__IgB5m">[</span>List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" href="/api/doc">Doc</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-types#types">Floats2d</a><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr></tbody></table><h3 id="metrics" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#metrics" class="heading-text typography_permalink__UiIRy">Metrics, training output and weighted scores <!-- --> </a></h3><p>When you train a pipeline using the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a> command,
you’ll see a table showing the metrics after each pass over the data. The
available metrics <strong>depend on the pipeline components</strong>. Pipeline components
also define which scores are shown and how they should be <strong>weighted in the
final score</strong> that decides about the best model.</p><p>The <code class="code_inline-code__Bq7ot">training.score_weights</code> setting in your <code class="code_inline-code__Bq7ot">config.cfg</code> lets you customize the
scores shown in the table and how they should be weighted. In this example, the
labeled dependency accuracy and NER F-score count towards the final score with
40% each and the tagging accuracy makes up the remaining 20%. The tokenization
accuracy and speed are both shown in the table, but not counted towards the
score.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Why do I need score weights?<!-- --> </span></h4>
<p>At the end of your training process, you typically want to select the <strong>best
model</strong> – but what “best” means depends on the available components and your
specific use case. For instance, you may prefer a pipeline with higher NER and
lower POS tagging accuracy over a pipeline with lower NER and higher POS
accuracy. You can express this preference in the score weights, e.g. by
assigning <code class="code_inline-code__Bq7ot">ents_f</code> (NER F-score) a higher weight.</p>
</div></div></aside><pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre><p>The <code class="code_inline-code__Bq7ot">score_weights</code> don’t <em>have to</em> sum to <code class="code_inline-code__Bq7ot">1.0</code> – but it’s recommended. When
you generate a config for a given pipeline, the score weights are generated by
combining and normalizing the default score weights of the pipeline components.
The default score weights are defined by each pipeline component via the
<code class="code_inline-code__Bq7ot">default_score_weights</code> setting on the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#factory"><code class="code_inline-code__Bq7ot">@Language.factory</code></a> decorator. By default, all pipeline
components are weighted equally. If a score weight is set to <code class="code_inline-code__Bq7ot">null</code>, it will be
excluded from the logs and the score won’t be weighted.</p><section class="accordion" id="score-types"><div class="accordion_root__pPltq accordion_spaced__Ebyjn"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">Understanding the training output and score types</span><a class="link_root__1Me7D accordion_anchor__kidBh link_no-link-layout__RPvod" href="/usage/training#score-types">¶</a></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>Loss</strong></td><td class="table_td__rmpJx">The training loss representing the amount of work left for the optimizer. Should decrease, but usually not to <code class="code_inline-code__Bq7ot">0</code>.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>Precision</strong> (P)</td><td class="table_td__rmpJx">Percentage of predicted annotations that were correct. Should increase.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>Recall</strong> (R)</td><td class="table_td__rmpJx">Percentage of reference annotations recovered. Should increase.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>F-Score</strong> (F)</td><td class="table_td__rmpJx">Harmonic mean of precision and recall. Should increase.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>UAS</strong> / <strong>LAS</strong></td><td class="table_td__rmpJx">Unlabeled and labeled attachment score for the dependency parser, i.e. the percentage of correct arcs. Should increase.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><strong>Speed</strong></td><td class="table_td__rmpJx">Prediction speed in words per second (WPS). Should stay stable.</td></tr></tbody></table><p>Note that if the development data has raw text, some of the gold-standard
entities might not align to the predicted tokenization. These tokenization
errors are <strong>excluded from the NER evaluation</strong>. If your tokenization makes it
impossible for the model to predict 50% of your entities, your NER F-score might
still look good.</p></div></div></section></section>
<section id="section-custom-functions" class="section_root__k1hUl"><h2 id="custom-functions" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#custom-functions" class="heading-text typography_permalink__UiIRy">Custom functions <!-- --> </a></h2><p>Registered functions in the training config files can refer to built-in
implementations, but you can also plug in fully <strong>custom implementations</strong>. All
you need to do is register your function using the <code class="code_inline-code__Bq7ot">@spacy.registry</code> decorator
with the name of the respective <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/top-level#registry"><span class="link_source-text__VDP74">registry</span></a>, e.g.
<code class="code_inline-code__Bq7ot">@spacy.registry.architectures</code>, and a string name to assign to your function.
Registering custom functions allows you to <strong>plug in models</strong> defined in PyTorch
or TensorFlow, make <strong>custom modifications</strong> to the <code class="code_inline-code__Bq7ot">nlp</code> object, create custom
optimizers or schedules, or <strong>stream in data</strong> and preprocess it on the fly
while training.</p><p>Each custom function can have any number of arguments that are passed in via the
<a class="link_root__1Me7D" href="/usage/training#config">config</a>, just the built-in functions. If your function defines
<strong>default argument values</strong>, spaCy is able to auto-fill your config when you run
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-fill-config"><code class="code_inline-code__Bq7ot">init fill-config</code></a>. If you want to make sure that a
given parameter is always explicitly set in the config, avoid setting a default
value for it.</p><h3 id="custom-code" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#custom-code" class="heading-text typography_permalink__UiIRy">Training with custom code <!-- --> </a></h3><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Training</h4><code class="code_code__CILJL language-bash language-bash"></code></pre>
<pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Packaging</h4><code class="code_code__CILJL language-bash language-bash"></code></pre>
</div></div></aside><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a> recipe lets you specify an optional argument
<code class="code_inline-code__Bq7ot">--code</code> that points to a Python file. The file is imported before training and
allows you to add custom functions and architectures to the function registry
that can then be referenced from your <code class="code_inline-code__Bq7ot">config.cfg</code>. This lets you train spaCy
pipelines with custom components, without having to re-implement the whole
training workflow. When you package your trained pipeline later using
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#package"><code class="code_inline-code__Bq7ot">spacy package</code></a>, you can provide one or more Python files to
be included in the package and imported in its <code class="code_inline-code__Bq7ot">__init__.py</code>. This means that
any custom architectures, functions or
<a class="link_root__1Me7D" href="/usage/processing-pipelines#custom-components">components</a> will be shipped with
your pipeline and registered when it’s loaded. See the documentation on
<a class="link_root__1Me7D" href="/usage/saving-loading#models-custom">saving and loading pipelines</a> for details.</p><h4 id="custom-code-nlp-callbacks" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#custom-code-nlp-callbacks" class="heading-text typography_permalink__UiIRy">Example: Modifying the nlp object <!-- --> </a></h4><p>For many use cases, you don’t necessarily want to implement the whole <code class="code_inline-code__Bq7ot">Language</code>
subclass and language data from scratch – it’s often enough to make a few small
modifications, like adjusting the
<a class="link_root__1Me7D" href="/usage/linguistic-features#native-tokenizer-additions">tokenization rules</a> or
<a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/language#defaults"><span class="link_source-text__VDP74">language defaults</span></a> like stop words. The config lets you
provide five optional <strong>callback functions</strong> that give you access to the
language class and <code class="code_inline-code__Bq7ot">nlp</code> object at different points of the lifecycle:</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Callback</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">nlp.before_creation</code></td><td class="table_td__rmpJx">Called before the <code class="code_inline-code__Bq7ot">nlp</code> object is created and receives the language subclass like <code class="code_inline-code__Bq7ot">English</code> (not the instance). Useful for writing to the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#defaults"><code class="code_inline-code__Bq7ot">Language.Defaults</code></a> aside from the tokenizer settings.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">nlp.after_creation</code></td><td class="table_td__rmpJx">Called right after the <code class="code_inline-code__Bq7ot">nlp</code> object is created, but before the pipeline components are added to the pipeline and receives the <code class="code_inline-code__Bq7ot">nlp</code> object.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">nlp.after_pipeline_creation</code></td><td class="table_td__rmpJx">Called right after the pipeline components are created and added and receives the <code class="code_inline-code__Bq7ot">nlp</code> object. Useful for modifying pipeline components.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">initialize.before_init</code></td><td class="table_td__rmpJx">Called before the pipeline components are initialized and receives the <code class="code_inline-code__Bq7ot">nlp</code> object for in-place modification. Useful for modifying the tokenizer settings, similar to the v2 base model option.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">initialize.after_init</code></td><td class="table_td__rmpJx">Called after the pipeline components are initialized and receives the <code class="code_inline-code__Bq7ot">nlp</code> object for in-place modification.</td></tr></tbody></table><p>The <code class="code_inline-code__Bq7ot">@spacy.registry.callbacks</code> decorator lets you register your custom function
in the <code class="code_inline-code__Bq7ot">callbacks</code> <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/top-level#registry"><span class="link_source-text__VDP74">registry</span></a> under a given name. You
can then reference the function in a config block using the <code class="code_inline-code__Bq7ot">@callbacks</code> key. If
a block contains a key starting with an <code class="code_inline-code__Bq7ot">@</code>, it’s interpreted as a reference to
a function. Because you’ve registered the function, spaCy knows how to create it
when you reference <code class="code_inline-code__Bq7ot">&quot;customize_language_data&quot;</code> in your config. Here’s an example
of a callback that runs before the <code class="code_inline-code__Bq7ot">nlp</code> object is created and adds a custom
stop word to the defaults:</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">config.cfg<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">functions.py</h4><code class="code_code__CILJL language-python language-python code_wrap__b41os"></code></pre><aside class="infobox_root__yNIMg infobox_warning__SKl67"><p>Remember that a registered function should always be a function that spaCy
<strong>calls to create something</strong>. In this case, it <strong>creates a callback</strong> – it’s
not the callback itself.</p></aside><p>Any registered function – in this case <code class="code_inline-code__Bq7ot">create_callback</code> – can also take
<strong>arguments</strong> that can be <strong>set by the config</strong>. This lets you implement and
keep track of different configurations, without having to hack at your code. You
can choose any arguments that make sense for your use case. In this example,
we’re adding the arguments <code class="code_inline-code__Bq7ot">extra_stop_words</code> (a list of strings) and <code class="code_inline-code__Bq7ot">debug</code>
(boolean) for printing additional info when the function runs.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">config.cfg<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">functions.py</h4><code class="code_code__CILJL language-python language-python code_wrap__b41os"></code></pre><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">💡</span>Tip: Use Python type hints</span></h4><p>spaCy’s configs are powered by our machine learning library Thinc’s
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/usage-config">configuration system</a>, which supports
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/typing.html">type hints</a> and even
<a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/usage-config#advanced-types">advanced type annotations</a>
using <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/samuelcolvin/pydantic"><code class="code_inline-code__Bq7ot">pydantic</code></a>. If your registered
function provides type hints, the values that are passed in will be checked
against the expected types. For example, <code class="code_inline-code__Bq7ot">debug: bool</code> in the example above will
ensure that the value received as the argument <code class="code_inline-code__Bq7ot">debug</code> is a boolean. If the
value can’t be coerced into a boolean, spaCy will raise an error.
<code class="code_inline-code__Bq7ot">debug: pydantic.StrictBool</code> will force the value to be a boolean and raise an
error if it’s not – for instance, if your config defines <code class="code_inline-code__Bq7ot">1</code> instead of <code class="code_inline-code__Bq7ot">true</code>.</p></aside><p>With your <code class="code_inline-code__Bq7ot">functions.py</code> defining additional code and the updated <code class="code_inline-code__Bq7ot">config.cfg</code>,
you can now run <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a> and point the argument <code class="code_inline-code__Bq7ot">--code</code>
to your Python file. Before loading the config, spaCy will import the
<code class="code_inline-code__Bq7ot">functions.py</code> module and your custom functions will be registered.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><h4 id="custom-tokenizer" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#custom-tokenizer" class="heading-text typography_permalink__UiIRy">Example: Modifying tokenizer settings <!-- --> </a></h4><p>Use the <code class="code_inline-code__Bq7ot">initialize.before_init</code> callback to modify the tokenizer settings when
training a new pipeline. Write a registered callback that modifies the tokenizer
settings and specify this callback in your config:</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">config.cfg<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">functions.py</h4><code class="code_code__CILJL language-python language-python"></code></pre><p>When training, provide the function above with the <code class="code_inline-code__Bq7ot">--code</code> option:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><p>Because this callback is only called in the one-time initialization step before
training, the callback code does not need to be packaged with the final pipeline
package. However, to make it easier for others to replicate your training setup,
you can choose to package the initialization callbacks with the pipeline package
or to publish them separately.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>nlp.before_creation vs. initialize.before_init</span></h4><ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">nlp.before_creation</code> is the best place to modify language defaults other than
the tokenizer settings.</li>
<li class="list_li__sfx_z"><code class="code_inline-code__Bq7ot">initialize.before_init</code> is the best place to modify tokenizer settings when
training a new pipeline.</li>
</ul><p>Unlike the other language defaults, the tokenizer settings are saved with the
pipeline with <code class="code_inline-code__Bq7ot">nlp.to_disk()</code>, so modifications made in <code class="code_inline-code__Bq7ot">nlp.before_creation</code>
will be clobbered by the saved settings when the trained pipeline is loaded from
disk.</p></aside><h4 id="custom-logging" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#custom-logging" class="heading-text typography_permalink__UiIRy">Example: Custom logging function <!-- --> </a></h4><p>During training, the results of each step are passed to a logger function. By
default, these results are written to the console with the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#ConsoleLogger"><code class="code_inline-code__Bq7ot">ConsoleLogger</code></a>. There is also built-in support
for writing the log files to <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://www.wandb.com/">Weights &amp; Biases</a> with the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spacy-loggers#wandblogger"><code class="code_inline-code__Bq7ot">WandbLogger</code></a>. On each
step, the logger function receives a <strong>dictionary</strong> with the following keys:</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Key</th><th class="table_th__QJ9F8">Value</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">epoch</code></td><td class="table_td__rmpJx">How many passes over the data have been completed. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">int</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">step</code></td><td class="table_td__rmpJx">How many steps have been completed. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">int</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">score</code></td><td class="table_td__rmpJx">The main score from the last evaluation, measured on the dev set. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">float</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">other_scores</code></td><td class="table_td__rmpJx">The other scores from the last evaluation, measured on the dev set. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Dict<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">,</span> Any<span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">losses</code></td><td class="table_td__rmpJx">The accumulated training losses, keyed by component name. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Dict<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">,</span> float<span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">checkpoints</code></td><td class="table_td__rmpJx">A list of previous results, where each result is a <code class="code_inline-code__Bq7ot">(score, step)</code> tuple. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">List<span class="code_cli-arg-subtle__IgB5m">[</span>Tuple<span class="code_cli-arg-subtle__IgB5m">[</span>float<span class="code_cli-arg-subtle__IgB5m">,</span> int<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr></tbody></table><p>You can easily implement and plug in your own logger that records the training
results in a custom way, or sends them to an experiment management tracker of
your choice. In this example, the function <code class="code_inline-code__Bq7ot">my_custom_logger.v1</code> writes the
tabular results to a file:</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">config.cfg (excerpt)</h4><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">functions.py</h4><code class="code_code__CILJL language-python language-python"></code></pre><h4 id="custom-code-schedule" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#custom-code-schedule" class="heading-text typography_permalink__UiIRy">Example: Custom batch size schedule <!-- --> </a></h4><p>You can also implement your own batch size schedule to use during training. The
<code class="code_inline-code__Bq7ot">@spacy.registry.schedules</code> decorator lets you register that function in the
<code class="code_inline-code__Bq7ot">schedules</code> <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/top-level#registry"><span class="link_source-text__VDP74">registry</span></a> and assign it a string name:</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Why the version in the name?<!-- --> </span></h4>
<p>A big benefit of the config system is that it makes your experiments
reproducible. We recommend versioning the functions you register, especially
if you expect them to change (like a new model architecture). This way, you
know that a config referencing <code class="code_inline-code__Bq7ot">v1</code> means a different function than a config
referencing <code class="code_inline-code__Bq7ot">v2</code>.</p>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">functions.py</h4><code class="code_code__CILJL language-python language-python"></code></pre><p>In your config, you can now reference the schedule in the
<code class="code_inline-code__Bq7ot">[training.batch_size]</code> block via <code class="code_inline-code__Bq7ot">@schedules</code>. If a block contains a key
starting with an <code class="code_inline-code__Bq7ot">@</code>, it’s interpreted as a reference to a function. All other
settings in the block will be passed to the function as keyword arguments. Keep
in mind that the config shouldn’t have any hidden defaults and all arguments on
the functions need to be represented in the config.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">config.cfg (excerpt)</h4><code class="code_code__CILJL language-ini language-ini"></code></pre><h3 id="custom-architectures" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#custom-architectures" class="heading-text typography_permalink__UiIRy">Defining custom architectures <!-- --> </a></h3><p>Built-in pipeline components such as the tagger or named entity recognizer are
constructed with default neural network <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/architectures"><span class="link_source-text__VDP74">models</span></a>. You can
change the model architecture entirely by implementing your own custom models
and providing those in the config when creating the pipeline component. See the
documentation on <a class="link_root__1Me7D" href="/usage/layers-architectures">layers and model architectures</a>
for more details.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">config.cfg</h4><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">functions.py</h4><code class="code_code__CILJL language-python language-python"></code></pre></section>
<section id="section-initialization" class="section_root__k1hUl"><h2 id="initialization" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#initialization" class="heading-text typography_permalink__UiIRy">Customizing the initialization <!-- --> </a></h2><p>When you start training a new model from scratch,
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a> will call
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#initialize"><code class="code_inline-code__Bq7ot">nlp.initialize</code></a> to initialize the pipeline and load
the required data. All settings for this are defined in the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/data-formats#config-initialize"><code class="code_inline-code__Bq7ot">[initialize]</code></a> block of the config, so
you can keep track of how the initial <code class="code_inline-code__Bq7ot">nlp</code> object was created. The
initialization process typically includes the following:</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">config.cfg (excerpt)<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><ol class="list_ol__aclSa">
<li class="list_li__sfx_z">Load in <strong>data resources</strong> defined in the <code class="code_inline-code__Bq7ot">[initialize]</code> config, including
<strong>word vectors</strong> and
<a class="link_root__1Me7D" href="/usage/embeddings-transformers#pretraining">pretrained</a> <strong>tok2vec
weights</strong>.</li>
<li class="list_li__sfx_z">Call the <code class="code_inline-code__Bq7ot">initialize</code> methods of the tokenizer (if implemented, e.g. for
<a class="link_root__1Me7D" href="/usage/models#chinese">Chinese</a>) and pipeline components with a callback to
access the training data, the current <code class="code_inline-code__Bq7ot">nlp</code> object and any <strong>custom
arguments</strong> defined in the <code class="code_inline-code__Bq7ot">[initialize]</code> config.</li>
<li class="list_li__sfx_z">In <strong>pipeline components</strong>: if needed, use the data to
<a class="link_root__1Me7D" href="/usage/layers-architectures#thinc-shape-inference">infer missing shapes</a> and
set up the label scheme if no labels are provided. Components may also load
other data like lookup tables or dictionaries.</li>
</ol><p>The initialization step allows the config to define <strong>all settings</strong> required
for the pipeline, while keeping a separation between settings and functions that
should only be used <strong>before training</strong> to set up the initial pipeline, and
logic and configuration that needs to be available <strong>at runtime</strong>. Without that
separation, it would be very difficult to use the same, reproducible config file
because the component settings required for training (load data from an external
file) wouldn’t match the component settings required at runtime (load what’s
included with the saved <code class="code_inline-code__Bq7ot">nlp</code> object and don’t depend on external file).</p><figure class="gatsby-resp-image-figure"><img class="embed_image__mSQUH" src="/images/lifecycle.svg" alt="Illustration of pipeline lifecycle" width="650" height="auto"/></figure><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">📖</span>How components save and load data</span></h4><p>For details and examples of how pipeline components can <strong>save and load data
assets</strong> like model weights or lookup tables, and how the component
initialization is implemented under the hood, see the usage guide on
<a class="link_root__1Me7D" href="/usage/processing-pipelines#component-data-initialization">serializing and initializing component data</a>.</p></aside><h4 id="initialization-labels" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#initialization-labels" class="heading-text typography_permalink__UiIRy">Initializing labels <!-- --> </a></h4><p>Built-in pipeline components like the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/entityrecognizer"><code class="code_inline-code__Bq7ot">EntityRecognizer</code></a> or
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/dependencyparser"><code class="code_inline-code__Bq7ot">DependencyParser</code></a> need to know their available labels
and associated internal meta information to initialize their model weights.
Using the <code class="code_inline-code__Bq7ot">get_examples</code> callback provided on initialization, they’re able to
<strong>read the labels off the training data</strong> automatically, which is very
convenient – but it can also slow down the training process to compute this
information on every run.</p><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-labels"><code class="code_inline-code__Bq7ot">init labels</code></a> command lets you auto-generate JSON
files containing the label data for all supported components. You can then pass
in the labels in the <code class="code_inline-code__Bq7ot">[initialize]</code> settings for the respective components to
allow them to initialize faster.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">config.cfg<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><p>Under the hood, the command delegates to the <code class="code_inline-code__Bq7ot">label_data</code> property of the
pipeline components, for instance
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/entityrecognizer#label_data"><code class="code_inline-code__Bq7ot">EntityRecognizer.label_data</code></a>.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>The JSON format differs for each component and some components need additional
meta information about their labels. The format exported by
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-labels"><code class="code_inline-code__Bq7ot">init labels</code></a> matches what the components need, so you
should always let spaCy <strong>auto-generate the labels</strong> for you.</p></aside></section>
<section id="section-data" class="section_root__k1hUl"><h2 id="data" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#data" class="heading-text typography_permalink__UiIRy">Data utilities <!-- --> </a></h2><p>spaCy includes various features and utilities to make it easy to train models
using your own data, manage training and evaluation corpora, convert existing
annotations and configure data augmentation strategies for more robust models.</p><h3 id="data-convert" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#data-convert" class="heading-text typography_permalink__UiIRy">Converting existing corpora and annotations <!-- --> </a></h3><p>If you have training data in a standard format like <code class="code_inline-code__Bq7ot">.conll</code> or <code class="code_inline-code__Bq7ot">.conllu</code>, the
easiest way to convert it for use with spaCy is to run
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#convert"><code class="code_inline-code__Bq7ot">spacy convert</code></a> and pass it a file and an output directory.
By default, the command will pick the converter based on the file extension.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">💡 Tip: Converting from Prodigy<!-- --> </span></h4>
<p>If you’re using the <a class="link_root__1Me7D" href="https://prodi.gy">Prodigy</a> annotation tool to create
training data, you can run the
<a class="link_root__1Me7D" href="https://prodi.gy/docs/recipes#data-to-spacy"><code class="code_inline-code__Bq7ot">data-to-spacy</code> command</a> to
merge and export multiple datasets for use with
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a>. Different types of annotations on the same
text will be combined, giving you one corpus to train multiple components.</p>
</div></div></aside><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">💡</span>Tip: Manage multi-step workflows with projects</span></h4><p>Training workflows often consist of multiple steps, from preprocessing the data
all the way to packaging and deploying the trained model.
<a class="link_root__1Me7D" href="/usage/projects">spaCy projects</a> let you define all steps in one file, manage
data assets, track changes and share your end-to-end processes with your team.</p></aside><p>The binary <code class="code_inline-code__Bq7ot">.spacy</code> format is a serialized <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/docbin"><code class="code_inline-code__Bq7ot">DocBin</code></a> containing
one or more <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> objects. It’s extremely <strong>efficient in storage</strong>,
especially when packing multiple documents together. You can also create <code class="code_inline-code__Bq7ot">Doc</code>
objects manually, so you can write your own custom logic to convert and store
existing annotations for use in spaCy.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Training data from Doc objects</h4><code class="code_code__CILJL language-python language-python code_wrap__b41os"></code></pre><h3 id="data-corpora" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#data-corpora" class="heading-text typography_permalink__UiIRy">Working with corpora <!-- --> </a></h3><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/data-formats#config-corpora"><code class="code_inline-code__Bq7ot">[corpora]</code></a> block in your config lets
you define <strong>data resources</strong> to use for training, evaluation, pretraining or
any other custom workflows. <code class="code_inline-code__Bq7ot">corpora.train</code> and <code class="code_inline-code__Bq7ot">corpora.dev</code> are used as
conventions within spaCy’s default configs, but you can also define any other
custom blocks. Each section in the corpora config should resolve to a
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/corpus"><code class="code_inline-code__Bq7ot">Corpus</code></a> – for example, using spaCy’s built-in
<a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/top-level#corpus-readers"><span class="link_source-text__VDP74">corpus reader</span></a> that takes a path to a binary
<code class="code_inline-code__Bq7ot">.spacy</code> file. The <code class="code_inline-code__Bq7ot">train_corpus</code> and <code class="code_inline-code__Bq7ot">dev_corpus</code> fields in the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/data-formats#config-training"><code class="code_inline-code__Bq7ot">[training]</code></a> block specify where to find
the corpus in your config. This makes it easy to <strong>swap out</strong> different corpora
by only changing a single config setting.</p><p>Instead of making <code class="code_inline-code__Bq7ot">[corpora]</code> a block with multiple subsections for each portion
of the data, you can also use a single function that returns a dictionary of
corpora, keyed by corpus name, e.g. <code class="code_inline-code__Bq7ot">&quot;train&quot;</code> and <code class="code_inline-code__Bq7ot">&quot;dev&quot;</code>. This can be
especially useful if you need to split a single file into corpora for training
and evaluation, without loading the same file twice.</p><p>By default, the training data is loaded into memory and shuffled before each
epoch. If the corpus is <strong>too large to fit into memory</strong> during training, stream
the corpus using a custom reader as described in the next section.</p><h3 id="custom-code-readers-batchers" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#custom-code-readers-batchers" class="heading-text typography_permalink__UiIRy">Custom data reading and batching <!-- --> </a></h3><p>Some use-cases require <strong>streaming in data</strong> or manipulating datasets on the
fly, rather than generating all data beforehand and storing it to disk. Instead
of using the built-in <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/corpus"><code class="code_inline-code__Bq7ot">Corpus</code></a> reader, which uses static file
paths, you can create and register a custom function that generates
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example"><code class="code_inline-code__Bq7ot">Example</code></a> objects.</p><p>In the following example we assume a custom function <code class="code_inline-code__Bq7ot">read_custom_data</code> which
loads or generates texts with relevant text classification annotations. Then,
small lexical variations of the input text are created before generating the
final <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example"><code class="code_inline-code__Bq7ot">Example</code></a> objects. The <code class="code_inline-code__Bq7ot">@spacy.registry.readers</code> decorator
lets you register the function creating the custom reader in the <code class="code_inline-code__Bq7ot">readers</code>
<a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/top-level#registry"><span class="link_source-text__VDP74">registry</span></a> and assign it a string name, so it can be
used in your config. All arguments on the registered function become available
as <strong>config settings</strong> – in this case, <code class="code_inline-code__Bq7ot">source</code>.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">config.cfg<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">functions.py</h4><code class="code_code__CILJL language-python language-python code_wrap__b41os"></code></pre><aside class="infobox_root__yNIMg infobox_warning__SKl67"><p>Remember that a registered function should always be a function that spaCy
<strong>calls to create something</strong>. In this case, it <strong>creates the reader function</strong>
– it’s not the reader itself.</p></aside><p>If the corpus is <strong>too large to load into memory</strong> or the corpus reader is an
<strong>infinite generator</strong>, use the setting <code class="code_inline-code__Bq7ot">max_epochs = -1</code> to indicate that the
train corpus should be streamed. With this setting the train corpus is merely
streamed and batched, not shuffled, so any shuffling needs to be implemented in
the corpus reader itself. In the example below, a corpus reader that generates
sentences containing even or odd numbers is used with an unlimited number of
examples for the train corpus and a limited number of examples for the dev
corpus. The dev corpus should always be finite and fit in memory during the
evaluation step. <code class="code_inline-code__Bq7ot">max_steps</code> and/or <code class="code_inline-code__Bq7ot">patience</code> are used to determine when the
training should stop.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">config.cfg<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">functions.py</h4><code class="code_code__CILJL language-python language-python"></code></pre><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">config.cfg<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><p>If the train corpus is streamed, the initialize step peeks at the first 100
examples in the corpus to find the labels for each component. If this isn’t
sufficient, you’ll need to <a class="link_root__1Me7D" href="/usage/training#initialization-labels">provide the labels</a> for each
component in the <code class="code_inline-code__Bq7ot">[initialize]</code> block. <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#init-labels"><code class="code_inline-code__Bq7ot">init labels</code></a> can
be used to generate JSON files in the correct format, which you can extend with
the full label set.</p><p>We can also customize the <strong>batching strategy</strong> by registering a new batcher
function in the <code class="code_inline-code__Bq7ot">batchers</code> <a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/top-level#registry"><span class="link_source-text__VDP74">registry</span></a>. A batcher turns
a stream of items into a stream of batches. spaCy has several useful built-in
<a class="link_root__1Me7D link_with-icon__NAVDA" href="/api/top-level#batchers"><span class="link_source-text__VDP74">batching strategies</span></a> with customizable sizes, but it’s
also easy to implement your own. For instance, the following function takes the
stream of generated <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example"><code class="code_inline-code__Bq7ot">Example</code></a> objects, and removes those which
have the same underlying raw text, to avoid duplicates within each batch. Note
that in a more realistic implementation, you’d also want to check whether the
annotations are the same.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">config.cfg<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">functions.py</h4><code class="code_code__CILJL language-python language-python"></code></pre><h3 id="data-augmentation" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#data-augmentation" class="heading-text typography_permalink__UiIRy">Data augmentation <!-- --> </a></h3><p>Data augmentation is the process of applying small <strong>modifications</strong> to the
training data. It can be especially useful for punctuation and case replacement
– for example, if your corpus only uses smart quotes and you want to include
variations using regular quotes, or to make the model less sensitive to
capitalization by including a mix of capitalized and lowercase examples.</p><p>The easiest way to use data augmentation during training is to provide an
<code class="code_inline-code__Bq7ot">augmenter</code> to the training corpus, e.g. in the <code class="code_inline-code__Bq7ot">[corpora.train]</code> section of
your config. The built-in <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#orth_variants"><code class="code_inline-code__Bq7ot">orth_variants</code></a>
augmenter creates a data augmentation callback that uses orth-variant
replacement.</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">config.cfg (excerpt)</h4><code class="code_code__CILJL language-ini language-ini code_wrap__b41os"></code></pre><p>The <code class="code_inline-code__Bq7ot">orth_variants</code> argument lets you pass in a dictionary of replacement rules,
typically loaded from a JSON file. There are two types of orth variant rules:
<code class="code_inline-code__Bq7ot">&quot;single&quot;</code> for single tokens that should be replaced (e.g. hyphens) and
<code class="code_inline-code__Bq7ot">&quot;paired&quot;</code> for pairs of tokens (e.g. quotes).</p><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">orth_variants.json</h4><code class="code_code__CILJL language-json language-json"></code></pre><section class="accordion"><div class="accordion_root__pPltq accordion_spaced__Ebyjn"><h4><button class="accordion_button__IPO0E" aria-expanded="true"><span><span class="heading-text">Full examples for English and German</span></span><svg class="accordion_icon__fpBl7" width="20" height="20" viewBox="0 0 10 10" aria-hidden="true" focusable="false"><rect class="accordion_hidden__tgILw" height="8" width="2" x="4" y="1"></rect><rect height="2" width="8" x="1" y="4"></rect></svg></button></h4><div class="accordion_content__divKS"><pre class="code_pre__kzg60"><header class="code_header__Jc0Q1"><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spacy-lookups-data/blob/master/spacy_lookups_data/data/en_orth_variants.json"><code class="code_inline-code__Bq7ot code_inline-code-dark__ZEbch">explosion/spacy-lookups-data/master/spacy_lookups_data/data/en_orth_variants.json</code></a></header><code class="code_code__CILJL code_code__CILJL code_max-height__093k6 code_code__CILJL language-json language-json code_max-height__093k6 language-json"></code></pre><pre class="code_pre__kzg60"><header class="code_header__Jc0Q1"><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spacy-lookups-data/blob/master/spacy_lookups_data/data/de_orth_variants.json"><code class="code_inline-code__Bq7ot code_inline-code-dark__ZEbch">explosion/spacy-lookups-data/master/spacy_lookups_data/data/de_orth_variants.json</code></a></header><code class="code_code__CILJL code_code__CILJL code_max-height__093k6 code_code__CILJL language-json language-json code_max-height__093k6 language-json"></code></pre></div></div></section><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Important note</span></h4><p>When adding data augmentation, keep in mind that it typically only makes sense
to apply it to the <strong>training corpus</strong>, not the development data.</p></aside><h4 id="data-augmentation-custom" class="typography_heading__D82WZ typography_h4__CDRaM"><a href="#data-augmentation-custom" class="heading-text typography_permalink__UiIRy">Writing custom data augmenters <!-- --> </a></h4><p>Using the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/top-level#registry"><code class="code_inline-code__Bq7ot">@spacy.augmenters</code></a> registry, you can also
register your own data augmentation callbacks. The callback should be a function
that takes the current <code class="code_inline-code__Bq7ot">nlp</code> object and a training <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example"><code class="code_inline-code__Bq7ot">Example</code></a> and
yields <code class="code_inline-code__Bq7ot">Example</code> objects. Keep in mind that the augmenter should yield <strong>all
examples</strong> you want to use in your corpus, not only the augmented examples
(unless you want to augment all examples).</p><p>Here’a an example of a custom augmentation callback that produces text variants
in <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://knowyourmeme.com/memes/mocking-spongebob">“SpOnGeBoB cAsE”</a>. The
registered function takes one argument <code class="code_inline-code__Bq7ot">randomize</code> that can be set via the
config and decides whether the uppercase/lowercase transformation is applied
randomly or not. The augmenter yields two <code class="code_inline-code__Bq7ot">Example</code> objects: the original
example and the augmented example.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">config.cfg<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>An easy way to create modified <code class="code_inline-code__Bq7ot">Example</code> objects is to use the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example#from_dict"><code class="code_inline-code__Bq7ot">Example.from_dict</code></a> method with a new reference
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> created from the modified text. In this case, only the
capitalization changes, so only the <code class="code_inline-code__Bq7ot">ORTH</code> values of the tokens will be
different between the original and augmented examples.</p><p>Note that if your data augmentation strategy involves changing the tokenization
(for instance, removing or adding tokens) and your training examples include
token-based annotations like the dependency parse or entity labels, you’ll need
to take care to adjust the <code class="code_inline-code__Bq7ot">Example</code> object so its annotations match and remain
valid.</p></section>
<section id="section-parallel-training" class="section_root__k1hUl"><h2 id="parallel-training" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#parallel-training" class="heading-text typography_permalink__UiIRy">Parallel &amp; distributed training with Ray <!-- --> </a></h2><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Installation<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre>
</div></div></aside><p><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://ray.io/">Ray</a> is a fast and simple framework for building and running
<strong>distributed applications</strong>. You can use Ray to train spaCy on one or more
remote machines, potentially speeding up your training process. Parallel
training won’t always be faster though – it depends on your batch size, models,
and hardware.</p><aside class="infobox_root__yNIMg infobox_warning__SKl67"><p>To use Ray with spaCy, you need the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spacy-ray"><code class="code_inline-code__Bq7ot">spacy-ray</code></a> package installed.
Installing the package will automatically add the <code class="code_inline-code__Bq7ot">ray</code> command to the spaCy
CLI.</p></aside><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#ray-train"><code class="code_inline-code__Bq7ot">spacy ray train</code></a> command follows the same API as
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a>, with a few extra options to configure the Ray
setup. You can optionally set the <code class="code_inline-code__Bq7ot">--address</code> option to point to your Ray
cluster. If it’s not set, Ray will run locally.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-bash language-bash"></code></pre><aside class="infobox_root__yNIMg"><h4 class="infobox_title__uDT7C"><span><span class="infobox_emoji__6_YUY" aria-hidden="true">🪐</span>Get started with a project template<!-- -->:<!-- --> <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/projects/tree/v3/integrations/ray"><code class="code_inline-code__Bq7ot">integrations/ray</code></a></span></h4><p>Get started with parallel training using our project template. It trains a
simple model on a Universal Dependencies Treebank and lets you parallelize the
training with Ray.</p><div class="copy_root__9E6qI"><span class="copy_prefix__p_JKI">$</span><textarea readonly="" class="copy_textarea__ATeHi" rows="1" aria-label="Example bash command to start with an end-to-end template">python -m spacy project clone integrations/ray</textarea></div></aside><h3 id="parallel-training-details" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#parallel-training-details" class="heading-text typography_permalink__UiIRy">How parallel training works <!-- --> </a></h3><p>Each worker receives a shard of the <strong>data</strong> and builds a copy of the <strong>model
and optimizer</strong> from the <a class="link_root__1Me7D" href="/usage/training#config"><code class="code_inline-code__Bq7ot">config.cfg</code></a>. It also has a communication
channel to <strong>pass gradients and parameters</strong> to the other workers. Additionally,
each worker is given ownership of a subset of the parameter arrays. Every
parameter array is owned by exactly one worker, and the workers are given a
mapping so they know which worker owns which parameter.</p><figure class="gatsby-resp-image-figure"><img class="embed_image__mSQUH" src="/images/spacy-ray.svg" alt="Illustration of setup" width="650" height="auto"/></figure><p>As training proceeds, every worker will be computing gradients for <strong>all</strong> of
the model parameters. When they compute gradients for parameters they don’t own,
they’ll <strong>send them to the worker</strong> that does own that parameter, along with a
version identifier so that the owner can decide whether to discard the gradient.
Workers use the gradients they receive and the ones they compute locally to
update the parameters they own, and then broadcast the updated array and a new
version ID to the other workers.</p><p>This training procedure is <strong>asynchronous</strong> and <strong>non-blocking</strong>. Workers always
push their gradient increments and parameter updates, they do not have to pull
them and block on the result, so the transfers can happen in the background,
overlapped with the actual training work. The workers also do not have to stop
and wait for each other (“synchronize”) at the start of each batch. This is very
useful for spaCy, because spaCy is often trained on long documents, which means
<strong>batches can vary in size</strong> significantly. Uneven workloads make synchronous
gradient descent inefficient, because if one batch is slow, all of the other
workers are stuck waiting for it to complete before they can continue.</p></section>
<section id="section-api" class="section_root__k1hUl"><h2 id="api" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#api" class="heading-text typography_permalink__UiIRy">Internal training API <!-- --> </a></h2><aside class="infobox_root__yNIMg infobox_danger__a_J18"><p>spaCy gives you full control over the training loop. However, for most use
cases, it’s recommended to train your pipelines via the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a> command with a <a class="link_root__1Me7D" href="/usage/training#config"><code class="code_inline-code__Bq7ot">config.cfg</code></a> to keep
track of your settings and hyperparameters, instead of writing your own training
scripts from scratch. <a class="link_root__1Me7D" href="/usage/training#custom-code">Custom registered functions</a> should
typically give you everything you need to train fully custom pipelines with
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a>.</p></aside><h3 id="api-train" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#api-train" class="heading-text typography_permalink__UiIRy">Training from a Python script <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH" data-tooltip="This feature is new and was introduced in spaCy v3.2">v<!-- -->3.2</span></h3><p>If you want to run the training from a Python script instead of using the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a> CLI command, you can call into the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train-function"><code class="code_inline-code__Bq7ot">train</code></a> helper function directly. It takes the path
to the config file, an optional output directory and an optional dictionary of
<a class="link_root__1Me7D" href="/usage/training#config-overrides">config overrides</a>.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><h3 id="api-loop" class="typography_heading__D82WZ typography_h3__mPKmB"><a href="#api-loop" class="heading-text typography_permalink__UiIRy">Internal training loop API <!-- --> </a></h3><aside class="infobox_root__yNIMg infobox_warning__SKl67"><p>This section documents how the training loop and updates to the <code class="code_inline-code__Bq7ot">nlp</code> object
work internally. You typically shouldn’t have to implement this in Python unless
you’re writing your own trainable components. To train a pipeline, use
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train"><code class="code_inline-code__Bq7ot">spacy train</code></a> or the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/cli#train-function"><code class="code_inline-code__Bq7ot">train</code></a> helper
function instead.</p></aside><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example"><code class="code_inline-code__Bq7ot">Example</code></a> object contains annotated training data, also
called the <strong>gold standard</strong>. It’s initialized with a <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> object
that will hold the predictions, and another <code class="code_inline-code__Bq7ot">Doc</code> object that holds the
gold-standard annotations. It also includes the <strong>alignment</strong> between those two
documents if they differ in tokenization. The <code class="code_inline-code__Bq7ot">Example</code> class ensures that spaCy
can rely on one <strong>standardized format</strong> that’s passed through the pipeline. For
instance, let’s say we want to define gold-standard part-of-speech tags:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>As this is quite verbose, there’s an alternative way to create the reference
<code class="code_inline-code__Bq7ot">Doc</code> with the gold-standard annotations. The function <code class="code_inline-code__Bq7ot">Example.from_dict</code> takes
a dictionary with keyword arguments specifying the annotations, like <code class="code_inline-code__Bq7ot">tags</code> or
<code class="code_inline-code__Bq7ot">entities</code>. Using the resulting <code class="code_inline-code__Bq7ot">Example</code> object and its gold-standard
annotations, the model can be updated to learn a sentence of three words with
their assigned part-of-speech tags.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><p>Here’s another example that shows how to define gold-standard named entities.
The letters added before the labels refer to the tags of the
<a class="link_root__1Me7D" href="/usage/linguistic-features#updating-biluo">BILUO scheme</a> – <code class="code_inline-code__Bq7ot">O</code> is a token
outside an entity, <code class="code_inline-code__Bq7ot">U</code> a single entity unit, <code class="code_inline-code__Bq7ot">B</code> the beginning of an entity, <code class="code_inline-code__Bq7ot">I</code>
a token inside an entity and <code class="code_inline-code__Bq7ot">L</code> the last token of an entity.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Migrating from v2.x</span></h4><p>As of v3.0, the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example"><code class="code_inline-code__Bq7ot">Example</code></a> object replaces the <code class="code_inline-code__Bq7ot">GoldParse</code> class.
It can be constructed in a very similar way – from a <code class="code_inline-code__Bq7ot">Doc</code> and a dictionary of
annotations. For more details, see the
<a class="link_root__1Me7D" href="/usage/v3#migrating-training">migration guide</a>.</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-diff language-diff"></code></pre></aside><p>Of course, it’s not enough to only show a model a single example once.
Especially if you only have few examples, you’ll want to train for a <strong>number of
iterations</strong>. At each iteration, the training data is <strong>shuffled</strong> to ensure the
model doesn’t make any generalizations based on the order of examples. Another
technique to improve the learning results is to set a <strong>dropout rate</strong>, a rate
at which to randomly “drop” individual features and representations. This makes
it harder for the model to memorize the training data. For example, a <code class="code_inline-code__Bq7ot">0.25</code>
dropout means that each feature or internal representation has a 1/4 likelihood
of being dropped.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<ul class="list_ul__fe_HF">
<li class="list_li__sfx_z"><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language"><code class="code_inline-code__Bq7ot">nlp</code></a>: The <code class="code_inline-code__Bq7ot">nlp</code> object with the pipeline components and
their models.</li>
<li class="list_li__sfx_z"><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#initialize"><code class="code_inline-code__Bq7ot">nlp.initialize</code></a>: Initialize the pipeline and
return an optimizer to update the component model weights.</li>
<li class="list_li__sfx_z"><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-optimizers"><code class="code_inline-code__Bq7ot">Optimizer</code></a>: Function that holds
state between updates.</li>
<li class="list_li__sfx_z"><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#update"><code class="code_inline-code__Bq7ot">nlp.update</code></a>: Update component models with examples.</li>
<li class="list_li__sfx_z"><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example"><code class="code_inline-code__Bq7ot">Example</code></a>: object holding predictions and gold-standard
annotations.</li>
<li class="list_li__sfx_z"><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#to_disk"><code class="code_inline-code__Bq7ot">nlp.to_disk</code></a>: Save the updated pipeline to a
directory.</li>
</ul>
</div></div></aside><pre class="code_pre__kzg60"><h4 class="code_title__Zz9rs">Example training loop</h4><code class="code_code__CILJL language-python language-python"></code></pre><p>The <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#update"><code class="code_inline-code__Bq7ot">nlp.update</code></a> method takes the following arguments:</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">examples</code></td><td class="table_td__rmpJx"><a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example"><code class="code_inline-code__Bq7ot">Example</code></a> objects. The <code class="code_inline-code__Bq7ot">update</code> method takes a sequence of them, so you can batch up your training examples.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">drop</code></td><td class="table_td__rmpJx">Dropout rate. Makes it harder for the model to just memorize the data.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">sgd</code></td><td class="table_td__rmpJx">An <a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://thinc.ai/docs/api-optimizers"><code class="code_inline-code__Bq7ot">Optimizer</code></a> object, which updates the model’s weights. If not set, spaCy will create a new one and save it for further use.</td></tr></tbody></table><aside class="infobox_root__yNIMg infobox_warning__SKl67"><h4 class="infobox_title__uDT7C"><span>Migrating from v2.x</span></h4><p>As of v3.0, the <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example"><code class="code_inline-code__Bq7ot">Example</code></a> object replaces the <code class="code_inline-code__Bq7ot">GoldParse</code> class
and the “simple training style” of calling <code class="code_inline-code__Bq7ot">nlp.update</code> with a text and a
dictionary of annotations. Updating your code to use the <code class="code_inline-code__Bq7ot">Example</code> object should
be very straightforward: you can call
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/example#from_dict"><code class="code_inline-code__Bq7ot">Example.from_dict</code></a> with a <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/doc"><code class="code_inline-code__Bq7ot">Doc</code></a> and the
dictionary of annotations:</p><pre class="code_pre__kzg60"><code class="code_code__CILJL language-diff language-diff"></code></pre></aside></section><div class="grid_root__EfDZl grid_spacing__fhBCv grid_half__xoJZs"><div style="margin-top:var(--spacing-lg)"><a class="link_root__1Me7D button_root__jwipc button_secondary__ukZAk" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/website/docs/usage/training.mdx">Suggest edits</a></div><a class="link_root__1Me7D readnext_root__JNzwZ link_no-link-layout__RPvod" href="/usage/layers-architectures"><span><span class="typography_label__l_oVJ">Read next</span>Layers &amp; Model Architectures</span><span class="readnext_icon__jfRnJ"></span></a></div></article><div class="main_asides__RITE5" style="background-image:url(/_next/static/media/pattern_blue.d167bed5.png"></div><footer class="footer_root__zlkjP"><div class="grid_root__EfDZl footer_content__LaE1F grid_narrow__x_6xS grid_spacing__fhBCv grid_third__edHuB"><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">spaCy</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/usage">Usage</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/models">Models</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/api">API Reference</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://course.spacy.io">Online Course</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/custom-solutions">Custom Solutions</a></li></ul></section><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Community</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/universe">Universe</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/discussions">GitHub Discussions</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/issues">Issue Tracker</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="http://stackoverflow.com/questions/tagged/spacy">Stack Overflow</a></li></ul></section><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Connect</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://twitter.com/spacy_io">Twitter</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy">GitHub</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://youtube.com/c/ExplosionAI">YouTube</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/blog">Blog</a></li></ul></section><section class="footer_full___icln"><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Stay in the loop!</li><li>Receive updates about new releases, tutorials and more.</li><li><form id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" action="//spacy.us12.list-manage.com/subscribe/post?u=83b0498b1e7fa3c91ce68c3f1&amp;amp;id=ecc82e0493" method="post" target="_blank" novalidate=""><div style="position:absolute;left:-5000px" aria-hidden="true"><input type="text" name="b_83b0498b1e7fa3c91ce68c3f1_ecc82e0493" tabindex="-1" value=""/></div><div class="newsletter_root__uh6MU"><input class="newsletter_input___SMSB" id="mce-EMAIL" type="email" name="EMAIL" placeholder="Your email" aria-label="Your email"/><button class="newsletter_button__gKW8E" id="mc-embedded-subscribe" type="submit" name="subscribe">Sign up</button></div></form></li></ul></section></div><div class="footer_content__LaE1F footer_copy__rbjvc"><span>© 2016-<!-- -->2023<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai">Explosion</a></span><a class="link_root__1Me7D footer_logo__BthsJ link_no-link-layout__RPvod" aria-label="Explosion" href="https://explosion.ai"></a><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/legal">Legal / Imprint</a></div></footer></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"title":"Training Pipelines \u0026 Models","teaser":"Train and update components on your own data and integrate custom models","next":{"slug":"/usage/layers-architectures","title":"Layers \u0026 Model Architectures"},"menu":[["Introduction","basics"],["Quickstart","quickstart"],["Config System","config"],["Training Data","training-data"],["Custom Training","config-custom"],["Custom Functions","custom-functions"],["Initialization","initialization"],["Data Utilities","data"],["Parallel Training","parallel-training"],["Internal API","api"]],"slug":"/usage/training","mdx":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\n/*\u003cQuickstartTraining /\u003e*/\n/*TODO: Custom corpus class, Minibatching*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    section: \"section\",\n    h2: \"h2\",\n    p: \"p\",\n    a: \"a\",\n    strong: \"strong\",\n    blockquote: \"blockquote\",\n    h4: \"h4\",\n    ol: \"ol\",\n    li: \"li\",\n    pre: \"pre\",\n    code: \"code\",\n    ul: \"ul\",\n    table: \"table\",\n    thead: \"thead\",\n    tr: \"tr\",\n    th: \"th\",\n    tbody: \"tbody\",\n    td: \"td\",\n    h3: \"h3\",\n    img: \"img\",\n    del: \"del\",\n    em: \"em\"\n  }, _provideComponents(), props.components), {Training101, Infobox, Image, InlineCode, Accordion, Project, YouTube} = _components;\n  if (!Accordion) _missingMdxReference(\"Accordion\", true);\n  if (!Image) _missingMdxReference(\"Image\", true);\n  if (!Infobox) _missingMdxReference(\"Infobox\", true);\n  if (!InlineCode) _missingMdxReference(\"InlineCode\", true);\n  if (!Project) _missingMdxReference(\"Project\", true);\n  if (!Training101) _missingMdxReference(\"Training101\", true);\n  if (!YouTube) _missingMdxReference(\"YouTube\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.section, {\n      id: \"section-basics\",\n      children: [_jsx(_components.h2, {\n        id: \"basics\",\n        hidden: \"true\",\n        children: \"Introduction to training \"\n      }), _jsx(Training101, {}), _jsxs(Infobox, {\n        title: \"Tip: Try the Prodigy annotation tool\",\n        children: [_jsx(Image, {\n          src: \"/images/prodigy.jpg\",\n          href: \"https://prodi.gy\",\n          alt: \"Prodigy: Radically efficient machine teaching\"\n        }), _jsxs(_components.p, {\n          children: [\"If you need to label a lot of data, check out \", _jsx(_components.a, {\n            href: \"https://prodi.gy\",\n            children: \"Prodigy\"\n          }), \", a\\nnew, active learning-powered annotation tool we’ve developed. Prodigy is fast\\nand extensible, and comes with a modern \", _jsx(_components.strong, {\n            children: \"web application\"\n          }), \" that helps you\\ncollect training data faster. It integrates seamlessly with spaCy, pre-selects\\nthe \", _jsx(_components.strong, {\n            children: \"most relevant examples\"\n          }), \" for annotation, and lets you train and evaluate\\nready-to-use spaCy pipelines.\"]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-quickstart\",\n      children: [_jsx(_components.h2, {\n        id: \"quickstart\",\n        tag: \"new\",\n        children: \"Quickstart \"\n      }), _jsxs(_components.p, {\n        children: [\"The recommended way to train your spaCy pipelines is via the\\n\", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \" command on the command line. It only needs a\\nsingle \", _jsx(_components.a, {\n          href: \"#config\",\n          children: _jsx(InlineCode, {\n            children: \"config.cfg\"\n          })\n        }), \" \", _jsx(_components.strong, {\n          children: \"configuration file\"\n        }), \" that includes all settings\\nand hyperparameters. You can optionally \", _jsx(_components.a, {\n          href: \"#config-overrides\",\n          children: \"overwrite\"\n        }), \" settings\\non the command line, and load in a Python file to register\\n\", _jsx(_components.a, {\n          href: \"#custom-code\",\n          children: \"custom functions\"\n        }), \" and architectures. This quickstart widget helps\\nyou generate a starter config with the \", _jsx(_components.strong, {\n          children: \"recommended settings\"\n        }), \" for your\\nspecific use case. It’s also available in spaCy as the\\n\", _jsx(_components.a, {\n          href: \"/api/cli#init-config\",\n          children: _jsx(InlineCode, {\n            children: \"init config\"\n          })\n        }), \" command.\"]\n      }), _jsx(Infobox, {\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Upgrade to the \", _jsx(_components.a, {\n            href: \"/usage\",\n            children: \"latest version of spaCy\"\n          }), \" to use the quickstart widget.\\nFor earlier releases, follow the CLI instructions to generate a compatible\\nconfig.\"]\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Instructions: widget\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"Select your requirements and settings.\"\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Use the buttons at the bottom to save the result to your clipboard or a\\nfile \", _jsx(InlineCode, {\n              children: \"base_config.cfg\"\n            }), \".\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Run \", _jsx(_components.a, {\n              href: \"/api/cli#init-fill-config\",\n              children: _jsx(InlineCode, {\n                children: \"init fill-config\"\n              })\n            }), \" to create a full\\nconfig.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Run \", _jsx(_components.a, {\n              href: \"/api/cli#train\",\n              children: _jsx(InlineCode, {\n                children: \"train\"\n              })\n            }), \" with your config and data.\"]\n          }), \"\\n\"]\n        }), \"\\n\", _jsx(_components.h4, {\n          children: \"Instructions: CLI\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"Run the \", _jsx(_components.a, {\n              href: \"/api/cli#init-config\",\n              children: _jsx(InlineCode, {\n                children: \"init config\"\n              })\n            }), \" command and specify your\\nrequirements and settings as CLI arguments.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Run \", _jsx(_components.a, {\n              href: \"/api/cli#train\",\n              children: _jsx(InlineCode, {\n                children: \"train\"\n              })\n            }), \" with the exported config and data.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"After you’ve saved the starter config to a file \", _jsx(InlineCode, {\n          children: \"base_config.cfg\"\n        }), \", you can use\\nthe \", _jsx(_components.a, {\n          href: \"/api/cli#init-fill-config\",\n          children: _jsx(InlineCode, {\n            children: \"init fill-config\"\n          })\n        }), \" command to fill in the\\nremaining defaults. Training configs should always be \", _jsx(_components.strong, {\n          children: \"complete and without\\nhidden defaults\"\n        }), \", to keep your experiments reproducible.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ python -m spacy init fill-config base_config.cfg config.cfg\\n\"\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Tip: Debug your data\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"The \", _jsxs(_components.a, {\n            href: \"/api/cli#debug-data\",\n            children: [_jsx(InlineCode, {\n              children: \"debug data\"\n            }), \" command\"]\n          }), \" lets you analyze and validate\\nyour training and development data, get useful stats, and find problems like\\ninvalid entity annotations, cyclic dependencies, low data labels and more.\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-bash\",\n            lang: \"bash\",\n            children: \"$ python -m spacy debug data config.cfg\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"Instead of exporting your starter config from the quickstart widget and\\nauto-filling it, you can also use the \", _jsx(_components.a, {\n          href: \"/api/cli#init-config\",\n          children: _jsx(InlineCode, {\n            children: \"init config\"\n          })\n        }), \"\\ncommand and specify your requirement and settings as CLI arguments. You can now\\nadd your data and run \", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"train\"\n          })\n        }), \" with your config. See the\\n\", _jsx(_components.a, {\n          href: \"/api/cli#convert\",\n          children: _jsx(InlineCode, {\n            children: \"convert\"\n          })\n        }), \" command for details on how to convert your data to\\nspaCy’s binary \", _jsx(InlineCode, {\n          children: \".spacy\"\n        }), \" format. You can either include the data paths in the\\n\", _jsx(InlineCode, {\n          children: \"[paths]\"\n        }), \" section of your config, or pass them in via the command line.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy\\n\"\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Tip: Enable your GPU\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"Use the \", _jsx(InlineCode, {\n            children: \"--gpu-id\"\n          }), \" option to select the GPU:\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-bash\",\n            lang: \"bash\",\n            children: \"$ python -m spacy train config.cfg --gpu-id 0\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(Accordion, {\n        title: \"How are the config recommendations generated?\",\n        id: \"quickstart-source\",\n        spaced: true,\n        children: [_jsxs(_components.p, {\n          children: [\"The recommended config settings generated by the quickstart widget and the\\n\", _jsx(_components.a, {\n            href: \"/api/cli#init-config\",\n            children: _jsx(InlineCode, {\n              children: \"init config\"\n            })\n          }), \" command are based on some general \", _jsx(_components.strong, {\n            children: \"best\\npractices\"\n          }), \" and things we’ve found to work well in our experiments. The goal is\\nto provide you with the most \", _jsx(_components.strong, {\n            children: \"useful defaults\"\n          }), \".\"]\n        }), _jsxs(_components.p, {\n          children: [\"Under the hood, the\\n\", _jsx(_components.a, {\n            href: \"https://github.com/explosion/spaCy/tree/master/spacy/cli/templates/quickstart_training.jinja\",\n            children: _jsx(InlineCode, {\n              children: \"quickstart_training.jinja\"\n            })\n          }), \"\\ntemplate defines the different combinations – for example, which parameters to\\nchange if the pipeline should optimize for efficiency vs. accuracy. The file\\n\", _jsx(_components.a, {\n            href: \"https://github.com/explosion/spaCy/tree/master/spacy/cli/templates/quickstart_training_recommendations.yml\",\n            children: _jsx(InlineCode, {\n              children: \"quickstart_training_recommendations.yml\"\n            })\n          }), \"\\ncollects the recommended settings and available resources for each language\\nincluding the different transformer weights. For some languages, we include\\ndifferent transformer recommendations, depending on whether you want the model\\nto be more efficient or more accurate. The recommendations will be \", _jsx(_components.strong, {\n            children: \"evolving\"\n          }), \"\\nas we run more experiments.\"]\n        })]\n      }), _jsx(Project, {\n        id: \"pipelines/tagger_parser_ud\",\n        children: _jsxs(_components.p, {\n          children: [\"The easiest way to get started is to clone a \", _jsx(_components.a, {\n            href: \"/usage/projects\",\n            children: \"project template\"\n          }), \"\\nand run it – for example, this end-to-end template that lets you train a\\n\", _jsx(_components.strong, {\n            children: \"part-of-speech tagger\"\n          }), \" and \", _jsx(_components.strong, {\n            children: \"dependency parser\"\n          }), \" on a Universal Dependencies\\ntreebank.\"]\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-config\",\n      children: [_jsx(_components.h2, {\n        id: \"config\",\n        children: \"Training config system \"\n      }), _jsxs(_components.p, {\n        children: [\"Training config files include all \", _jsx(_components.strong, {\n          children: \"settings and hyperparameters\"\n        }), \" for training\\nyour pipeline. Instead of providing lots of arguments on the command line, you\\nonly need to pass your \", _jsx(InlineCode, {\n          children: \"config.cfg\"\n        }), \" file to \", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \".\\nUnder the hood, the training config uses the\\n\", _jsx(_components.a, {\n          href: \"https://thinc.ai/docs/usage-config\",\n          children: \"configuration system\"\n        }), \" provided by our\\nmachine learning library \", _jsx(_components.a, {\n          href: \"https://thinc.ai\",\n          children: \"Thinc\"\n        }), \". This also makes it easy to\\nintegrate custom models and architectures, written in your framework of choice.\\nSome of the main advantages and features of spaCy’s training config are:\"]\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Structured sections.\"\n          }), \" The config is grouped into sections, and nested\\nsections are defined using the \", _jsx(InlineCode, {\n            children: \".\"\n          }), \" notation. For example, \", _jsx(InlineCode, {\n            children: \"[components.ner]\"\n          }), \"\\ndefines the settings for the pipeline’s named entity recognizer. The config\\ncan be loaded as a Python dict.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"References to registered functions.\"\n          }), \" Sections can refer to registered\\nfunctions like \", _jsx(_components.a, {\n            href: \"/api/architectures\",\n            children: \"model architectures\"\n          }), \",\\n\", _jsx(_components.a, {\n            href: \"https://thinc.ai/docs/api-optimizers\",\n            children: \"optimizers\"\n          }), \" or\\n\", _jsx(_components.a, {\n            href: \"https://thinc.ai/docs/api-schedules\",\n            children: \"schedules\"\n          }), \" and define arguments that are\\npassed into them. You can also\\n\", _jsx(_components.a, {\n            href: \"#custom-functions\",\n            children: \"register your own functions\"\n          }), \" to define custom\\narchitectures or methods, reference them in your config and tweak their\\nparameters.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Interpolation.\"\n          }), \" If you have hyperparameters or other settings used by\\nmultiple components, define them once and reference them as\\n\", _jsx(_components.a, {\n            href: \"#config-interpolation\",\n            children: \"variables\"\n          }), \".\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Reproducibility with no hidden defaults.\"\n          }), \" The config file is the “single\\nsource of truth” and includes all settings.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Automated checks and validation.\"\n          }), \" When you load a config, spaCy checks if\\nthe settings are complete and if all values have the correct types. This lets\\nyou catch potential mistakes early. In your custom architectures, you can use\\nPython \", _jsx(_components.a, {\n            href: \"https://docs.python.org/3/library/typing.html\",\n            children: \"type hints\"\n          }), \" to tell the\\nconfig which types of data to expect.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          github: \"https://github.com/explosion/spaCy/tree/master/spacy/default_config.cfg\",\n          children: \"https://github.com/explosion/spaCy/tree/master/spacy/default_config.cfg\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Under the hood, the config is parsed into a dictionary. It’s divided into\\nsections and subsections, indicated by the square brackets and dot notation. For\\nexample, \", _jsx(InlineCode, {\n          children: \"[training]\"\n        }), \" is a section and \", _jsx(InlineCode, {\n          children: \"[training.batch_size]\"\n        }), \" a subsection.\\nSubsections can define values, just like a dictionary, or use the \", _jsx(InlineCode, {\n          children: \"@\"\n        }), \" syntax to\\nrefer to \", _jsx(_components.a, {\n          href: \"#config-functions\",\n          children: \"registered functions\"\n        }), \". This allows the config to\\nnot just define static settings, but also construct objects like architectures,\\nschedules, optimizers or any other custom components. The main top-level\\nsections of a config file are:\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Section\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nlp\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Definition of the \", _jsx(InlineCode, {\n                children: \"nlp\"\n              }), \" object, its tokenizer and \", _jsx(_components.a, {\n                href: \"/usage/processing-pipelines\",\n                children: \"processing pipeline\"\n              }), \" component names.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"components\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Definitions of the \", _jsx(_components.a, {\n                href: \"/usage/processing-pipelines\",\n                children: \"pipeline components\"\n              }), \" and their models.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"paths\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Paths to data and other assets. Re-used across the config as variables, e.g. \", _jsx(InlineCode, {\n                children: \"${paths.train}\"\n              }), \", and can be \", _jsx(_components.a, {\n                href: \"#config-overrides\",\n                children: \"overwritten\"\n              }), \" on the CLI.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"system\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Settings related to system and hardware. Re-used across the config as variables, e.g. \", _jsx(InlineCode, {\n                children: \"${system.seed}\"\n              }), \", and can be \", _jsx(_components.a, {\n                href: \"#config-overrides\",\n                children: \"overwritten\"\n              }), \" on the CLI.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"training\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Settings and controls for the training and evaluation process.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"pretraining\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Optional settings and controls for the \", _jsx(_components.a, {\n                href: \"/usage/embeddings-transformers#pretraining\",\n                children: \"language model pretraining\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"initialize\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Data resources and arguments passed to components when \", _jsx(_components.a, {\n                href: \"/api/language#initialize\",\n                children: _jsx(InlineCode, {\n                  children: \"nlp.initialize\"\n                })\n              }), \" is called before training (but not at runtime).\"]\n            })]\n          })]\n        })]\n      }), _jsx(Infobox, {\n        title: \"Config format and settings\",\n        emoji: \"📖\",\n        children: _jsxs(_components.p, {\n          children: [\"For a full overview of spaCy’s config format and settings, see the\\n\", _jsx(_components.a, {\n            href: \"/api/data-formats#config\",\n            children: \"data format documentation\"\n          }), \" and\\n\", _jsx(_components.a, {\n            href: \"https://thinc.ai/docs/usage-config\",\n            children: \"Thinc’s config system docs\"\n          }), \". The settings\\navailable for the different architectures are documented with the\\n\", _jsx(_components.a, {\n            href: \"/api/architectures\",\n            children: \"model architectures API\"\n          }), \". See the Thinc documentation for\\n\", _jsx(_components.a, {\n            href: \"https://thinc.ai/docs/api-optimizers\",\n            children: \"optimizers\"\n          }), \" and\\n\", _jsx(_components.a, {\n            href: \"https://thinc.ai/docs/api-schedules\",\n            children: \"schedules\"\n          }), \".\"]\n        })\n      }), _jsx(YouTube, {\n        id: \"BWhh3r6W-qE\"\n      }), _jsx(_components.h3, {\n        id: \"config-lifecycle\",\n        children: \"Config lifecycle at runtime and training \"\n      }), _jsxs(_components.p, {\n        children: [\"A pipeline’s \", _jsx(InlineCode, {\n          children: \"config.cfg\"\n        }), \" is considered the “single source of truth”, both at\\n\", _jsx(_components.strong, {\n          children: \"training\"\n        }), \" and \", _jsx(_components.strong, {\n          children: \"runtime\"\n        }), \". Under the hood,\\n\", _jsx(_components.a, {\n          href: \"/api/language#from_config\",\n          children: _jsx(InlineCode, {\n            children: \"Language.from_config\"\n          })\n        }), \" takes care of constructing\\nthe \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object using the settings defined in the config. An \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object’s\\nconfig is available as \", _jsx(_components.a, {\n          href: \"/api/language#config\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.config\"\n          })\n        }), \" and it includes all\\ninformation about the pipeline, as well as the settings used to train and\\ninitialize it.\"]\n      }), _jsx(_components.img, {\n        src: \"/images/lifecycle.svg\",\n        alt: \"Illustration of pipeline lifecycle\"\n      }), _jsxs(_components.p, {\n        children: [\"At runtime spaCy will only use the \", _jsx(InlineCode, {\n          children: \"[nlp]\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"[components]\"\n        }), \" blocks of the\\nconfig and load all data, including tokenization rules, model weights and other\\nresources from the pipeline directory. The \", _jsx(InlineCode, {\n          children: \"[training]\"\n        }), \" block contains the\\nsettings for training the model and is only used during training. Similarly, the\\n\", _jsx(InlineCode, {\n          children: \"[initialize]\"\n        }), \" block defines how the initial \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object should be set up\\nbefore training and whether it should be initialized with vectors or pretrained\\ntok2vec weights, or any other data needed by the components.\"]\n      }), _jsxs(_components.p, {\n        children: [\"The initialization settings are only loaded and used when\\n\", _jsx(_components.a, {\n          href: \"/api/language#initialize\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.initialize\"\n          })\n        }), \" is called (typically right before\\ntraining). This allows you to set up your pipeline using local data resources\\nand custom functions, and preserve the information in your config – but without\\nrequiring it to be available at runtime. You can also use this mechanism to\\nprovide data paths to custom pipeline components and custom tokenizers – see the\\nsection on \", _jsx(_components.a, {\n          href: \"#initialization\",\n          children: \"custom initialization\"\n        }), \" for details.\"]\n      }), _jsx(_components.h3, {\n        id: \"config-overrides\",\n        children: \"Overwriting config settings on the command line \"\n      }), _jsxs(_components.p, {\n        children: [\"The config system means that you can define all settings \", _jsx(_components.strong, {\n          children: \"in one place\"\n        }), \" and in\\na consistent format. There are no command-line arguments that need to be set,\\nand no hidden defaults. However, there can still be scenarios where you may want\\nto override config settings when you run \", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \". This\\nincludes \", _jsx(_components.strong, {\n          children: \"file paths\"\n        }), \" to vectors or other resources that shouldn’t be\\nhard-coded in a config file, or \", _jsx(_components.strong, {\n          children: \"system-dependent settings\"\n        }), \".\"]\n      }), _jsxs(_components.p, {\n        children: [\"For cases like this, you can set additional command-line options starting with\\n\", _jsx(InlineCode, {\n          children: \"--\"\n        }), \" that correspond to the config section and value to override. For example,\\n\", _jsx(InlineCode, {\n          children: \"--paths.train ./corpus/train.spacy\"\n        }), \" sets the \", _jsx(InlineCode, {\n          children: \"train\"\n        }), \" value in the \", _jsx(InlineCode, {\n          children: \"[paths]\"\n        }), \"\\nblock.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ python -m spacy train config.cfg --paths.train ./corpus/train.spacy --paths.dev ./corpus/dev.spacy --training.batch_size 128\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Only existing sections and values in the config can be overwritten. At the end\\nof the training, the final filled \", _jsx(InlineCode, {\n          children: \"config.cfg\"\n        }), \" is exported with your pipeline,\\nso you’ll always have a record of the settings that were used, including your\\noverrides. Overrides are added before \", _jsx(_components.a, {\n          href: \"#config-interpolation\",\n          children: \"variables\"\n        }), \" are\\nresolved, by the way – so if you need to use a value in multiple places,\\nreference it across your config and override it on the CLI once.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"💡 Tip: Verbose logging\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"If you’re using config overrides, you can set the \", _jsx(InlineCode, {\n            children: \"--verbose\"\n          }), \" flag on\\n\", _jsx(_components.a, {\n            href: \"/api/cli#train\",\n            children: _jsx(InlineCode, {\n              children: \"spacy train\"\n            })\n          }), \" to make spaCy log more info, including which\\noverrides were set via the CLI and environment variables.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.h4, {\n        id: \"config-overrides-env\",\n        children: \"Adding overrides via environment variables \"\n      }), _jsxs(_components.p, {\n        children: [\"Instead of defining the overrides as CLI arguments, you can also use the\\n\", _jsx(InlineCode, {\n          children: \"SPACY_CONFIG_OVERRIDES\"\n        }), \" environment variable using the same argument syntax.\\nThis is especially useful if you’re training models as part of an automated\\nprocess. Environment variables \", _jsx(_components.strong, {\n          children: \"take precedence\"\n        }), \" over CLI overrides and values\\ndefined in the config file.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ SPACY_CONFIG_OVERRIDES=\\\"--system.gpu_allocator pytorch --training.batch_size 128\\\" ./your_script.sh\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"config-stdin\",\n        children: \"Reading from standard input \"\n      }), _jsxs(_components.p, {\n        children: [\"Setting the config path to \", _jsx(InlineCode, {\n          children: \"-\"\n        }), \" on the command line lets you read the config from\\nstandard input and pipe it forward from a different process, like\\n\", _jsx(_components.a, {\n          href: \"/api/cli#init-config\",\n          children: _jsx(InlineCode, {\n            children: \"init config\"\n          })\n        }), \" or your own custom script. This is\\nespecially useful for quick experiments, as it lets you generate a config on the\\nfly without having to save to and load from disk.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"💡 Tip: Writing to stdout\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"When you run \", _jsx(InlineCode, {\n            children: \"init config\"\n          }), \", you can set the output path to \", _jsx(InlineCode, {\n            children: \"-\"\n          }), \" to write to\\nstdout. In a custom script, you can print the string config, e.g.\\n\", _jsx(InlineCode, {\n            children: \"print(nlp.config.to_str())\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ python -m spacy init config - --lang en --pipeline ner,textcat --optimize accuracy | python -m spacy train - --paths.train ./corpus/train.spacy --paths.dev ./corpus/dev.spacy\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"config-interpolation\",\n        children: \"Using variable interpolation \"\n      }), _jsxs(_components.p, {\n        children: [\"Another very useful feature of the config system is that it supports variable\\ninterpolation for both \", _jsx(_components.strong, {\n          children: \"values and sections\"\n        }), \". This means that you only need to\\ndefine a setting once and can reference it across your config using the\\n\", _jsx(InlineCode, {\n          children: \"${section.value}\"\n        }), \" syntax. In this example, the value of \", _jsx(InlineCode, {\n          children: \"seed\"\n        }), \" is reused within\\nthe \", _jsx(InlineCode, {\n          children: \"[training]\"\n        }), \" block, and the whole block of \", _jsx(InlineCode, {\n          children: \"[training.optimizer]\"\n        }), \" is reused\\nin \", _jsx(InlineCode, {\n          children: \"[pretraining]\"\n        }), \" and will become \", _jsx(InlineCode, {\n          children: \"pretraining.optimizer\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"config.cfg (excerpt)\",\n          highlight: \"5,18\",\n          children: \"[system]\\nseed = 0\\n\\n[training]\\nseed = ${system.seed}\\n\\n[training.optimizer]\\n@optimizers = \\\"Adam.v1\\\"\\nbeta1 = 0.9\\nbeta2 = 0.999\\nL2_is_weight_decay = true\\nL2 = 0.01\\ngrad_clip = 1.0\\nuse_averages = false\\neps = 1e-8\\n\\n[pretraining]\\noptimizer = ${training.optimizer}\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"You can also use variables inside strings. In that case, it works just like\\nf-strings in Python. If the value of a variable is not a string, it’s converted\\nto a string.\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          children: \"[paths]\\nversion = 5\\nroot = \\\"/Users/you/data\\\"\\ntrain = \\\"${paths.root}/train_${paths.version}.spacy\\\"\\n# Result: /Users/you/data/train_5.spacy\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Tip: Override variables on the CLI\",\n        emoji: \"💡\",\n        children: _jsxs(_components.p, {\n          children: [\"If you need to change certain values between training runs, you can define them\\nonce, reference them as variables and then \", _jsx(_components.a, {\n            href: \"#config-overrides\",\n            children: \"override\"\n          }), \" them on\\nthe CLI. For example, \", _jsx(InlineCode, {\n            children: \"--paths.root /other/root\"\n          }), \" will change the value of \", _jsx(InlineCode, {\n            children: \"root\"\n          }), \"\\nin the block \", _jsx(InlineCode, {\n            children: \"[paths]\"\n          }), \" and the change will be reflected across all other values\\nthat reference this variable.\"]\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-training-data\",\n      children: [_jsx(_components.h2, {\n        id: \"training-data\",\n        children: \"Preparing Training Data \"\n      }), _jsxs(_components.p, {\n        children: [\"Training data for NLP projects comes in many different formats. For some common\\nformats such as CoNLL, spaCy provides \", _jsx(_components.a, {\n          href: \"/api/cli#convert\",\n          children: \"converters\"\n        }), \" you can use\\nfrom the command line. In other cases you’ll have to prepare the training data\\nyourself.\"]\n      }), _jsxs(_components.p, {\n        children: [\"When converting training data for use in spaCy, the main thing is to create\\n\", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" objects just like the results you want as output from the\\npipeline. For example, if you’re creating an NER pipeline, loading your\\nannotations and setting them as the \", _jsx(InlineCode, {\n          children: \".ents\"\n        }), \" property on a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" is all you need\\nto worry about. On disk the annotations will be saved as a\\n\", _jsx(_components.a, {\n          href: \"/api/docbin\",\n          children: _jsx(InlineCode, {\n            children: \"DocBin\"\n          })\n        }), \" in the\\n\", _jsxs(_components.a, {\n          href: \"/api/data-formats#binary-training\",\n          children: [_jsx(InlineCode, {\n            children: \".spacy\"\n          }), \" format\"]\n        }), \", but the details of that\\nare handled automatically.\"]\n      }), _jsxs(_components.p, {\n        children: [\"Here’s an example of creating a \", _jsx(InlineCode, {\n          children: \".spacy\"\n        }), \" file from some NER annotations.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"preprocess.py\",\n          children: \"import spacy\\nfrom spacy.tokens import DocBin\\n\\nnlp = spacy.blank(\\\"en\\\")\\ntraining_data = [\\n  (\\\"Tokyo Tower is 333m tall.\\\", [(0, 11, \\\"BUILDING\\\")]),\\n]\\n# the DocBin will store the example documents\\ndb = DocBin()\\nfor text, annotations in training_data:\\n    doc = nlp(text)\\n    ents = []\\n    for start, end, label in annotations:\\n        span = doc.char_span(start, end, label=label)\\n        ents.append(span)\\n    doc.ents = ents\\n    db.add(doc)\\ndb.to_disk(\\\"./train.spacy\\\")\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"For more examples of how to convert training data from a wide variety of formats\\nfor use with spaCy, look at the preprocessing steps in the\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/projects/tree/v3/tutorials\",\n          children: \"tutorial projects\"\n        }), \".\"]\n      }), _jsx(Accordion, {\n        title: \"What about the spaCy JSON format?\",\n        id: \"json-annotations\",\n        spaced: true,\n        children: _jsxs(_components.p, {\n          children: [\"In spaCy v2, the recommended way to store training data was in\\n\", _jsx(_components.a, {\n            href: \"/api/data-formats#json-input\",\n            children: \"a particular JSON format\"\n          }), \", but in v3 this format\\nis deprecated. It’s fine as a readable storage format, but there’s no need to\\nconvert your data to JSON before creating a \", _jsx(InlineCode, {\n            children: \".spacy\"\n          }), \" file.\"]\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-config-custom\",\n      children: [_jsx(_components.h2, {\n        id: \"config-custom\",\n        children: \"Customizing the pipeline and training \"\n      }), _jsx(_components.h3, {\n        id: \"config-components\",\n        children: \"Defining pipeline components \"\n      }), _jsxs(_components.p, {\n        children: [\"You typically train a \", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines\",\n          children: \"pipeline\"\n        }), \" of \", _jsx(_components.strong, {\n          children: \"one or more\\ncomponents\"\n        }), \". The \", _jsx(InlineCode, {\n          children: \"[components]\"\n        }), \" block in the config defines the available\\npipeline components and how they should be created – either by a built-in or\\ncustom \", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#built-in\",\n          children: \"factory\"\n        }), \", or\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#sourced-components\",\n          children: \"sourced\"\n        }), \" from an existing\\ntrained pipeline. For example, \", _jsx(InlineCode, {\n          children: \"[components.parser]\"\n        }), \" defines the component named\\n\", _jsx(InlineCode, {\n          children: \"\\\"parser\\\"\"\n        }), \" in the pipeline. There are different ways you might want to treat\\nyour components during training, and the most common scenarios are:\"]\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"Train a \", _jsx(_components.strong, {\n            children: \"new component\"\n          }), \" from scratch on your data.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Update an existing \", _jsx(_components.strong, {\n            children: \"trained component\"\n          }), \" with more examples.\"]\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Include an existing trained component without updating it.\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Include a non-trainable component, like a rule-based\\n\", _jsx(_components.a, {\n            href: \"/api/entityruler\",\n            children: _jsx(InlineCode, {\n              children: \"EntityRuler\"\n            })\n          }), \" or \", _jsx(_components.a, {\n            href: \"/api/sentencizer\",\n            children: _jsx(InlineCode, {\n              children: \"Sentencizer\"\n            })\n          }), \", or a\\nfully \", _jsx(_components.a, {\n            href: \"/usage/processing-pipelines#custom-components\",\n            children: \"custom component\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"If a component block defines a \", _jsx(InlineCode, {\n          children: \"factory\"\n        }), \", spaCy will look it up in the\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#built-in\",\n          children: \"built-in\"\n        }), \" or\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components\",\n          children: \"custom\"\n        }), \" components and create a\\nnew component from scratch. All settings defined in the config block will be\\npassed to the component factory as arguments. This lets you configure the model\\nsettings and hyperparameters. If a component block defines a \", _jsx(InlineCode, {\n          children: \"source\"\n        }), \", the\\ncomponent will be copied over from an existing trained pipeline, with its\\nexisting weights. This lets you include an already trained component in your\\npipeline, or update a trained component with more data specific to your use\\ncase.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"config.cfg (excerpt)\",\n          children: \"[components]\\n\\n# \\\"parser\\\" and \\\"ner\\\" are sourced from a trained pipeline\\n[components.parser]\\nsource = \\\"en_core_web_sm\\\"\\n\\n[components.ner]\\nsource = \\\"en_core_web_sm\\\"\\n\\n# \\\"textcat\\\" and \\\"custom\\\" are created blank from a built-in / custom factory\\n[components.textcat]\\nfactory = \\\"textcat\\\"\\n\\n[components.custom]\\nfactory = \\\"your_custom_factory\\\"\\nyour_custom_setting = true\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"pipeline\"\n        }), \" setting in the \", _jsx(InlineCode, {\n          children: \"[nlp]\"\n        }), \" block defines the pipeline components\\nadded to the pipeline, in order. For example, \", _jsx(InlineCode, {\n          children: \"\\\"parser\\\"\"\n        }), \" here references\\n\", _jsx(InlineCode, {\n          children: \"[components.parser]\"\n        }), \". By default, spaCy will \", _jsx(_components.strong, {\n          children: \"update all components that can\\nbe updated\"\n        }), \". Trainable components that are created from scratch are initialized\\nwith random weights. For sourced components, spaCy will keep the existing\\nweights and \", _jsx(_components.a, {\n          href: \"/api/language#resume_training\",\n          children: \"resume training\"\n        }), \".\"]\n      }), _jsxs(_components.p, {\n        children: [\"If you don’t want a component to be updated, you can \", _jsx(_components.strong, {\n          children: \"freeze\"\n        }), \" it by adding it\\nto the \", _jsx(InlineCode, {\n          children: \"frozen_components\"\n        }), \" list in the \", _jsx(InlineCode, {\n          children: \"[training]\"\n        }), \" block. Frozen components are\\n\", _jsx(_components.strong, {\n          children: \"not updated\"\n        }), \" during training and are included in the final trained pipeline\\nas-is. They are also excluded when calling\\n\", _jsx(_components.a, {\n          href: \"/api/language#initialize\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.initialize\"\n          })\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Note on frozen components\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"Even though frozen components are not \", _jsx(_components.strong, {\n            children: \"updated\"\n          }), \" during training, they will\\nstill \", _jsx(_components.strong, {\n            children: \"run\"\n          }), \" during evaluation. This is very important, because they may\\nstill impact your model’s performance – for instance, a sentence boundary\\ndetector can impact what the parser or entity recognizer considers a valid\\nparse. So the evaluation results should always reflect what your pipeline will\\nproduce at runtime. If you want a frozen component to run (without updating)\\nduring training as well, so that downstream components can use its\\n\", _jsx(_components.strong, {\n            children: \"predictions\"\n          }), \", you should add it to the list of\\n\", _jsx(_components.a, {\n            href: \"/usage/training#annotating-components\",\n            children: _jsx(InlineCode, {\n              children: \"annotating_components\"\n            })\n          }), \".\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          children: \"[nlp]\\nlang = \\\"en\\\"\\npipeline = [\\\"parser\\\", \\\"ner\\\", \\\"textcat\\\", \\\"custom\\\"]\\n\\n[training]\\nfrozen_components = [\\\"parser\\\", \\\"custom\\\"]\\n\"\n        })\n      }), _jsxs(Infobox, {\n        variant: \"warning\",\n        title: \"Shared Tok2Vec listener layer\",\n        id: \"config-components-listeners\",\n        children: [_jsxs(_components.p, {\n          children: [\"When the components in your pipeline\\n\", _jsx(_components.a, {\n            href: \"/usage/embeddings-transformers#embedding-layers\",\n            children: \"share an embedding layer\"\n          }), \", the\\n\", _jsx(_components.strong, {\n            children: \"performance\"\n          }), \" of your frozen component will be \", _jsx(_components.strong, {\n            children: \"degraded\"\n          }), \" if you continue\\ntraining other layers with the same underlying \", _jsx(InlineCode, {\n            children: \"Tok2Vec\"\n          }), \" instance. As a rule of\\nthumb, ensure that your frozen components are truly \", _jsx(_components.strong, {\n            children: \"independent\"\n          }), \" in the\\npipeline.\"]\n        }), _jsxs(_components.p, {\n          children: [\"To automatically replace a shared token-to-vector listener with an independent\\ncopy of the token-to-vector layer, you can use the \", _jsx(InlineCode, {\n            children: \"replace_listeners\"\n          }), \" setting\\nof a sourced component, pointing to the listener layer(s) in the config. For\\nmore details on how this works under the hood, see\\n\", _jsx(_components.a, {\n            href: \"/api/language#replace_listeners\",\n            children: _jsx(InlineCode, {\n              children: \"Language.replace_listeners\"\n            })\n          }), \".\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[training]\\nfrozen_components = [\\\"tagger\\\"]\\n\\n[components.tagger]\\nsource = \\\"en_core_web_sm\\\"\\nreplace_listeners = [\\\"model.tok2vec\\\"]\\n\"\n          })\n        })]\n      }), _jsx(_components.h3, {\n        id: \"annotating-components\",\n        version: \"3.1\",\n        children: \"Using predictions from preceding components \"\n      }), _jsxs(_components.p, {\n        children: [\"By default, components are updated in isolation during training, which means\\nthat they don’t see the predictions of any earlier components in the pipeline. A\\ncomponent receives \", _jsx(_components.a, {\n          href: \"/api/example\",\n          children: _jsx(InlineCode, {\n            children: \"Example.predicted\"\n          })\n        }), \" as input and compares its\\npredictions to \", _jsx(_components.a, {\n          href: \"/api/example\",\n          children: _jsx(InlineCode, {\n            children: \"Example.reference\"\n          })\n        }), \" without saving its\\nannotations in the \", _jsx(InlineCode, {\n          children: \"predicted\"\n        }), \" doc.\"]\n      }), _jsxs(_components.p, {\n        children: [\"Instead, if certain components should \", _jsx(_components.strong, {\n          children: \"set their annotations\"\n        }), \" during training,\\nuse the setting \", _jsx(InlineCode, {\n          children: \"annotating_components\"\n        }), \" in the \", _jsx(InlineCode, {\n          children: \"[training]\"\n        }), \" block to specify a\\nlist of components. For example, the feature \", _jsx(InlineCode, {\n          children: \"DEP\"\n        }), \" from the parser could be used\\nas a tagger feature by including \", _jsx(InlineCode, {\n          children: \"DEP\"\n        }), \" in the tok2vec \", _jsx(InlineCode, {\n          children: \"attrs\"\n        }), \" and including\\n\", _jsx(InlineCode, {\n          children: \"parser\"\n        }), \" in \", _jsx(InlineCode, {\n          children: \"annotating_components\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"config.cfg (excerpt)\",\n          highlight: \"7,12\",\n          children: \"[nlp]\\npipeline = [\\\"parser\\\", \\\"tagger\\\"]\\n\\n[components.tagger.model.tok2vec.embed]\\n@architectures = \\\"spacy.MultiHashEmbed.v1\\\"\\nwidth = ${components.tagger.model.tok2vec.encode.width}\\nattrs = [\\\"NORM\\\",\\\"DEP\\\"]\\nrows = [5000,2500]\\ninclude_static_vectors = false\\n\\n[training]\\nannotating_components = [\\\"parser\\\"]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Any component in the pipeline can be included as an annotating component,\\nincluding frozen components. Frozen components can set annotations during\\ntraining just as they would set annotations during evaluation or when the final\\npipeline is run. The config excerpt below shows how a frozen \", _jsx(InlineCode, {\n          children: \"ner\"\n        }), \" component and\\na \", _jsx(InlineCode, {\n          children: \"sentencizer\"\n        }), \" can provide the required \", _jsx(InlineCode, {\n          children: \"doc.sents\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \" for the\\nentity linker during training:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"config.cfg (excerpt)\",\n          children: \"[nlp]\\npipeline = [\\\"sentencizer\\\", \\\"ner\\\", \\\"entity_linker\\\"]\\n\\n[components.ner]\\nsource = \\\"en_core_web_sm\\\"\\n\\n[training]\\nfrozen_components = [\\\"ner\\\"]\\nannotating_components = [\\\"sentencizer\\\", \\\"ner\\\"]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Similarly, a pretrained \", _jsx(InlineCode, {\n          children: \"tok2vec\"\n        }), \" layer can be frozen and specified in the list\\nof \", _jsx(InlineCode, {\n          children: \"annotating_components\"\n        }), \" to ensure that a downstream component can use the\\nembedding layer without updating it.\"]\n      }), _jsx(Infobox, {\n        variant: \"warning\",\n        title: \"Training speed with annotating components\",\n        id: \"annotating-components-speed\",\n        children: _jsxs(_components.p, {\n          children: [\"Be aware that non-frozen annotating components with statistical models will\\n\", _jsx(_components.strong, {\n            children: \"run twice\"\n          }), \" on each batch, once to update the model and once to apply the\\nnow-updated model to the predicted docs.\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"config-functions\",\n        children: \"Using registered functions \"\n      }), _jsxs(_components.p, {\n        children: [\"The training configuration defined in the config file doesn’t have to only\\nconsist of static values. Some settings can also be \", _jsx(_components.strong, {\n          children: \"functions\"\n        }), \". For instance,\\nthe \", _jsx(InlineCode, {\n          children: \"batch_size\"\n        }), \" can be a number that doesn’t change, or a schedule, like a\\nsequence of compounding values, which has shown to be an effective trick (see\\n\", _jsx(_components.a, {\n          href: \"https://arxiv.org/abs/1711.00489\",\n          children: \"Smith et al., 2017\"\n        }), \").\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"With static value\",\n          children: \"[training]\\nbatch_size = 128\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"To refer to a function instead, you can make \", _jsx(InlineCode, {\n          children: \"[training.batch_size]\"\n        }), \" its own\\nsection and use the \", _jsx(InlineCode, {\n          children: \"@\"\n        }), \" syntax to specify the function and its arguments – in\\nthis case \", _jsx(_components.a, {\n          href: \"https://thinc.ai/docs/api-schedules#compounding\",\n          children: _jsx(InlineCode, {\n            children: \"compounding.v1\"\n          })\n        }), \"\\ndefined in the \", _jsx(_components.a, {\n          href: \"/api/top-level#registry\",\n          children: \"function registry\"\n        }), \". All other values\\ndefined in the block are passed to the function as keyword arguments when it’s\\ninitialized. You can also use this mechanism to register\\n\", _jsx(_components.a, {\n          href: \"#custom-functions\",\n          children: \"custom implementations and architectures\"\n        }), \" and reference them\\nfrom your configs.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"How the config is resolved\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"The config file is parsed into a regular dictionary and is resolved and\\nvalidated \", _jsx(_components.strong, {\n            children: \"bottom-up\"\n          }), \". Arguments provided for registered functions are\\nchecked against the function’s signature and type annotations. The return\\nvalue of a registered function can also be passed into another function – for\\ninstance, a learning rate schedule can be provided as the an argument of an\\noptimizer.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"With registered function\",\n          children: \"[training.batch_size]\\n@schedules = \\\"compounding.v1\\\"\\nstart = 100\\nstop = 1000\\ncompound = 1.001\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"model-architectures\",\n        children: \"Model architectures \"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"💡 Model type annotations\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"In the documentation and code base, you may come across type annotations and\\ndescriptions of \", _jsx(_components.a, {\n            href: \"https://thinc.ai\",\n            children: \"Thinc\"\n          }), \" model types, like \", _jsx(_components.del, {\n            children: \"Model[List[Doc],\\nList[Floats2d]]\"\n          }), \". This so-called generic type describes the layer and its\\ninput and output type – in this case, it takes a list of \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \" objects as the\\ninput and list of 2-dimensional arrays of floats as the output. You can read\\nmore about defining Thinc models \", _jsx(_components.a, {\n            href: \"https://thinc.ai/docs/usage-models\",\n            children: \"here\"\n          }), \".\\nAlso see the \", _jsx(_components.a, {\n            href: \"https://thinc.ai/docs/usage-type-checking\",\n            children: \"type checking\"\n          }), \" for\\nhow to enable linting in your editor to see live feedback if your inputs and\\noutputs don’t match.\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"A \", _jsx(_components.strong, {\n          children: \"model architecture\"\n        }), \" is a function that wires up a Thinc\\n\", _jsx(_components.a, {\n          href: \"https://thinc.ai/docs/api-model\",\n          children: _jsx(InlineCode, {\n            children: \"Model\"\n          })\n        }), \" instance, which you can then use in a\\ncomponent or as a layer of a larger network. You can use Thinc as a thin\\n\", _jsx(_components.a, {\n          href: \"https://thinc.ai/docs/usage-frameworks\",\n          children: \"wrapper around frameworks\"\n        }), \" such as\\nPyTorch, TensorFlow or MXNet, or you can implement your logic in Thinc\\n\", _jsx(_components.a, {\n          href: \"https://thinc.ai/docs/usage-models\",\n          children: \"directly\"\n        }), \". For more details and examples,\\nsee the usage guide on \", _jsx(_components.a, {\n          href: \"/usage/layers-architectures\",\n          children: \"layers and architectures\"\n        }), \".\"]\n      }), _jsxs(_components.p, {\n        children: [\"spaCy’s built-in components will never construct their \", _jsx(InlineCode, {\n          children: \"Model\"\n        }), \" instances\\nthemselves, so you won’t have to subclass the component to change its model\\narchitecture. You can just \", _jsx(_components.strong, {\n          children: \"update the config\"\n        }), \" so that it refers to a\\ndifferent registered function. Once the component has been created, its \", _jsx(InlineCode, {\n          children: \"Model\"\n        }), \"\\ninstance has already been assigned, so you cannot change its model architecture.\\nThe architecture is like a recipe for the network, and you can’t change the\\nrecipe once the dish has already been prepared. You have to make a new one.\\nspaCy includes a variety of built-in \", _jsx(_components.a, {\n          href: \"/api/architectures\",\n          children: \"architectures\"\n        }), \" for\\ndifferent tasks. For example:\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Architecture\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"/api/architectures#HashEmbedCNN\",\n                children: \"HashEmbedCNN\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Build spaCy’s “standard” embedding layer, which uses hash embedding with subword features and a CNN with layer-normalized maxout. \", _jsx(_components.del, {\n                children: \"Model[List[Doc], List[Floats2d]]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"/api/architectures#TransitionBasedParser\",\n                children: \"TransitionBasedParser\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Build a \", _jsx(_components.a, {\n                href: \"https://explosion.ai/blog/parsing-english-in-python\",\n                children: \"transition-based parser\"\n              }), \" model used in the default \", _jsx(_components.a, {\n                href: \"/api/entityrecognizer\",\n                children: _jsx(InlineCode, {\n                  children: \"EntityRecognizer\"\n                })\n              }), \" and \", _jsx(_components.a, {\n                href: \"/api/dependencyparser\",\n                children: _jsx(InlineCode, {\n                  children: \"DependencyParser\"\n                })\n              }), \". \", _jsx(_components.del, {\n                children: \"Model[List[Docs], List[List[Floats2d]]]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"/api/architectures#TextCatEnsemble\",\n                children: \"TextCatEnsemble\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Stacked ensemble of a bag-of-words model and a neural network model with an internal CNN embedding layer. Used in the default \", _jsx(_components.a, {\n                href: \"/api/textcategorizer\",\n                children: _jsx(InlineCode, {\n                  children: \"TextCategorizer\"\n                })\n              }), \". \", _jsx(_components.del, {\n                children: \"Model[List[Doc], Floats2d]\"\n              })]\n            })]\n          })]\n        })]\n      }), _jsx(_components.h3, {\n        id: \"metrics\",\n        children: \"Metrics, training output and weighted scores \"\n      }), _jsxs(_components.p, {\n        children: [\"When you train a pipeline using the \", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \" command,\\nyou’ll see a table showing the metrics after each pass over the data. The\\navailable metrics \", _jsx(_components.strong, {\n          children: \"depend on the pipeline components\"\n        }), \". Pipeline components\\nalso define which scores are shown and how they should be \", _jsx(_components.strong, {\n          children: \"weighted in the\\nfinal score\"\n        }), \" that decides about the best model.\"]\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"training.score_weights\"\n        }), \" setting in your \", _jsx(InlineCode, {\n          children: \"config.cfg\"\n        }), \" lets you customize the\\nscores shown in the table and how they should be weighted. In this example, the\\nlabeled dependency accuracy and NER F-score count towards the final score with\\n40% each and the tagging accuracy makes up the remaining 20%. The tokenization\\naccuracy and speed are both shown in the table, but not counted towards the\\nscore.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Why do I need score weights?\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"At the end of your training process, you typically want to select the \", _jsx(_components.strong, {\n            children: \"best\\nmodel\"\n          }), \" – but what “best” means depends on the available components and your\\nspecific use case. For instance, you may prefer a pipeline with higher NER and\\nlower POS tagging accuracy over a pipeline with lower NER and higher POS\\naccuracy. You can express this preference in the score weights, e.g. by\\nassigning \", _jsx(InlineCode, {\n            children: \"ents_f\"\n          }), \" (NER F-score) a higher weight.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          children: \"[training.score_weights]\\ndep_las = 0.4\\ndep_uas = null\\nents_f = 0.4\\ntag_acc = 0.2\\ntoken_acc = 0.0\\nspeed = 0.0\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"score_weights\"\n        }), \" don’t \", _jsx(_components.em, {\n          children: \"have to\"\n        }), \" sum to \", _jsx(InlineCode, {\n          children: \"1.0\"\n        }), \" – but it’s recommended. When\\nyou generate a config for a given pipeline, the score weights are generated by\\ncombining and normalizing the default score weights of the pipeline components.\\nThe default score weights are defined by each pipeline component via the\\n\", _jsx(InlineCode, {\n          children: \"default_score_weights\"\n        }), \" setting on the\\n\", _jsx(_components.a, {\n          href: \"/api/language#factory\",\n          children: _jsx(InlineCode, {\n            children: \"@Language.factory\"\n          })\n        }), \" decorator. By default, all pipeline\\ncomponents are weighted equally. If a score weight is set to \", _jsx(InlineCode, {\n          children: \"null\"\n        }), \", it will be\\nexcluded from the logs and the score won’t be weighted.\"]\n      }), _jsxs(Accordion, {\n        title: \"Understanding the training output and score types\",\n        spaced: true,\n        id: \"score-types\",\n        children: [_jsxs(_components.table, {\n          children: [_jsx(_components.thead, {\n            children: _jsxs(_components.tr, {\n              children: [_jsx(_components.th, {\n                children: \"Name\"\n              }), _jsx(_components.th, {\n                children: \"Description\"\n              })]\n            })\n          }), _jsxs(_components.tbody, {\n            children: [_jsxs(_components.tr, {\n              children: [_jsx(_components.td, {\n                children: _jsx(_components.strong, {\n                  children: \"Loss\"\n                })\n              }), _jsxs(_components.td, {\n                children: [\"The training loss representing the amount of work left for the optimizer. Should decrease, but usually not to \", _jsx(InlineCode, {\n                  children: \"0\"\n                }), \".\"]\n              })]\n            }), _jsxs(_components.tr, {\n              children: [_jsxs(_components.td, {\n                children: [_jsx(_components.strong, {\n                  children: \"Precision\"\n                }), \" (P)\"]\n              }), _jsx(_components.td, {\n                children: \"Percentage of predicted annotations that were correct. Should increase.\"\n              })]\n            }), _jsxs(_components.tr, {\n              children: [_jsxs(_components.td, {\n                children: [_jsx(_components.strong, {\n                  children: \"Recall\"\n                }), \" (R)\"]\n              }), _jsx(_components.td, {\n                children: \"Percentage of reference annotations recovered. Should increase.\"\n              })]\n            }), _jsxs(_components.tr, {\n              children: [_jsxs(_components.td, {\n                children: [_jsx(_components.strong, {\n                  children: \"F-Score\"\n                }), \" (F)\"]\n              }), _jsx(_components.td, {\n                children: \"Harmonic mean of precision and recall. Should increase.\"\n              })]\n            }), _jsxs(_components.tr, {\n              children: [_jsxs(_components.td, {\n                children: [_jsx(_components.strong, {\n                  children: \"UAS\"\n                }), \" / \", _jsx(_components.strong, {\n                  children: \"LAS\"\n                })]\n              }), _jsx(_components.td, {\n                children: \"Unlabeled and labeled attachment score for the dependency parser, i.e. the percentage of correct arcs. Should increase.\"\n              })]\n            }), _jsxs(_components.tr, {\n              children: [_jsx(_components.td, {\n                children: _jsx(_components.strong, {\n                  children: \"Speed\"\n                })\n              }), _jsx(_components.td, {\n                children: \"Prediction speed in words per second (WPS). Should stay stable.\"\n              })]\n            })]\n          })]\n        }), _jsxs(_components.p, {\n          children: [\"Note that if the development data has raw text, some of the gold-standard\\nentities might not align to the predicted tokenization. These tokenization\\nerrors are \", _jsx(_components.strong, {\n            children: \"excluded from the NER evaluation\"\n          }), \". If your tokenization makes it\\nimpossible for the model to predict 50% of your entities, your NER F-score might\\nstill look good.\"]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-custom-functions\",\n      children: [_jsx(_components.h2, {\n        id: \"custom-functions\",\n        children: \"Custom functions \"\n      }), _jsxs(_components.p, {\n        children: [\"Registered functions in the training config files can refer to built-in\\nimplementations, but you can also plug in fully \", _jsx(_components.strong, {\n          children: \"custom implementations\"\n        }), \". All\\nyou need to do is register your function using the \", _jsx(InlineCode, {\n          children: \"@spacy.registry\"\n        }), \" decorator\\nwith the name of the respective \", _jsx(_components.a, {\n          href: \"/api/top-level#registry\",\n          children: \"registry\"\n        }), \", e.g.\\n\", _jsx(InlineCode, {\n          children: \"@spacy.registry.architectures\"\n        }), \", and a string name to assign to your function.\\nRegistering custom functions allows you to \", _jsx(_components.strong, {\n          children: \"plug in models\"\n        }), \" defined in PyTorch\\nor TensorFlow, make \", _jsx(_components.strong, {\n          children: \"custom modifications\"\n        }), \" to the \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object, create custom\\noptimizers or schedules, or \", _jsx(_components.strong, {\n          children: \"stream in data\"\n        }), \" and preprocess it on the fly\\nwhile training.\"]\n      }), _jsxs(_components.p, {\n        children: [\"Each custom function can have any number of arguments that are passed in via the\\n\", _jsx(_components.a, {\n          href: \"#config\",\n          children: \"config\"\n        }), \", just the built-in functions. If your function defines\\n\", _jsx(_components.strong, {\n          children: \"default argument values\"\n        }), \", spaCy is able to auto-fill your config when you run\\n\", _jsx(_components.a, {\n          href: \"/api/cli#init-fill-config\",\n          children: _jsx(InlineCode, {\n            children: \"init fill-config\"\n          })\n        }), \". If you want to make sure that a\\ngiven parameter is always explicitly set in the config, avoid setting a default\\nvalue for it.\"]\n      }), _jsx(_components.h3, {\n        id: \"custom-code\",\n        children: \"Training with custom code \"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-bash\",\n            lang: \"bash\",\n            title: \"Training\",\n            children: \"$ python -m spacy train config.cfg --code functions.py\\n\"\n          })\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-bash\",\n            lang: \"bash\",\n            title: \"Packaging\",\n            children: \"$ python -m spacy package ./model-best ./packages --code functions.py\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \" recipe lets you specify an optional argument\\n\", _jsx(InlineCode, {\n          children: \"--code\"\n        }), \" that points to a Python file. The file is imported before training and\\nallows you to add custom functions and architectures to the function registry\\nthat can then be referenced from your \", _jsx(InlineCode, {\n          children: \"config.cfg\"\n        }), \". This lets you train spaCy\\npipelines with custom components, without having to re-implement the whole\\ntraining workflow. When you package your trained pipeline later using\\n\", _jsx(_components.a, {\n          href: \"/api/cli#package\",\n          children: _jsx(InlineCode, {\n            children: \"spacy package\"\n          })\n        }), \", you can provide one or more Python files to\\nbe included in the package and imported in its \", _jsx(InlineCode, {\n          children: \"__init__.py\"\n        }), \". This means that\\nany custom architectures, functions or\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components\",\n          children: \"components\"\n        }), \" will be shipped with\\nyour pipeline and registered when it’s loaded. See the documentation on\\n\", _jsx(_components.a, {\n          href: \"/usage/saving-loading#models-custom\",\n          children: \"saving and loading pipelines\"\n        }), \" for details.\"]\n      }), _jsx(_components.h4, {\n        id: \"custom-code-nlp-callbacks\",\n        children: \"Example: Modifying the nlp object \"\n      }), _jsxs(_components.p, {\n        children: [\"For many use cases, you don’t necessarily want to implement the whole \", _jsx(InlineCode, {\n          children: \"Language\"\n        }), \"\\nsubclass and language data from scratch – it’s often enough to make a few small\\nmodifications, like adjusting the\\n\", _jsx(_components.a, {\n          href: \"/usage/linguistic-features#native-tokenizer-additions\",\n          children: \"tokenization rules\"\n        }), \" or\\n\", _jsx(_components.a, {\n          href: \"/api/language#defaults\",\n          children: \"language defaults\"\n        }), \" like stop words. The config lets you\\nprovide five optional \", _jsx(_components.strong, {\n          children: \"callback functions\"\n        }), \" that give you access to the\\nlanguage class and \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object at different points of the lifecycle:\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Callback\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nlp.before_creation\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Called before the \", _jsx(InlineCode, {\n                children: \"nlp\"\n              }), \" object is created and receives the language subclass like \", _jsx(InlineCode, {\n                children: \"English\"\n              }), \" (not the instance). Useful for writing to the \", _jsx(_components.a, {\n                href: \"/api/language#defaults\",\n                children: _jsx(InlineCode, {\n                  children: \"Language.Defaults\"\n                })\n              }), \" aside from the tokenizer settings.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nlp.after_creation\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Called right after the \", _jsx(InlineCode, {\n                children: \"nlp\"\n              }), \" object is created, but before the pipeline components are added to the pipeline and receives the \", _jsx(InlineCode, {\n                children: \"nlp\"\n              }), \" object.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nlp.after_pipeline_creation\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Called right after the pipeline components are created and added and receives the \", _jsx(InlineCode, {\n                children: \"nlp\"\n              }), \" object. Useful for modifying pipeline components.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"initialize.before_init\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Called before the pipeline components are initialized and receives the \", _jsx(InlineCode, {\n                children: \"nlp\"\n              }), \" object for in-place modification. Useful for modifying the tokenizer settings, similar to the v2 base model option.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"initialize.after_init\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Called after the pipeline components are initialized and receives the \", _jsx(InlineCode, {\n                children: \"nlp\"\n              }), \" object for in-place modification.\"]\n            })]\n          })]\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"@spacy.registry.callbacks\"\n        }), \" decorator lets you register your custom function\\nin the \", _jsx(InlineCode, {\n          children: \"callbacks\"\n        }), \" \", _jsx(_components.a, {\n          href: \"/api/top-level#registry\",\n          children: \"registry\"\n        }), \" under a given name. You\\ncan then reference the function in a config block using the \", _jsx(InlineCode, {\n          children: \"@callbacks\"\n        }), \" key. If\\na block contains a key starting with an \", _jsx(InlineCode, {\n          children: \"@\"\n        }), \", it’s interpreted as a reference to\\na function. Because you’ve registered the function, spaCy knows how to create it\\nwhen you reference \", _jsx(InlineCode, {\n          children: \"\\\"customize_language_data\\\"\"\n        }), \" in your config. Here’s an example\\nof a callback that runs before the \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object is created and adds a custom\\nstop word to the defaults:\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[nlp.before_creation]\\n@callbacks = \\\"customize_language_data\\\"\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"functions.py\",\n          highlight: \"3,6\",\n          children: \"import spacy\\n\\n@spacy.registry.callbacks(\\\"customize_language_data\\\")\\ndef create_callback():\\n    def customize_language_data(lang_cls):\\n        lang_cls.Defaults.stop_words.add(\\\"good\\\")\\n        return lang_cls\\n\\n    return customize_language_data\\n\"\n        })\n      }), _jsx(Infobox, {\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Remember that a registered function should always be a function that spaCy\\n\", _jsx(_components.strong, {\n            children: \"calls to create something\"\n          }), \". In this case, it \", _jsx(_components.strong, {\n            children: \"creates a callback\"\n          }), \" – it’s\\nnot the callback itself.\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Any registered function – in this case \", _jsx(InlineCode, {\n          children: \"create_callback\"\n        }), \" – can also take\\n\", _jsx(_components.strong, {\n          children: \"arguments\"\n        }), \" that can be \", _jsx(_components.strong, {\n          children: \"set by the config\"\n        }), \". This lets you implement and\\nkeep track of different configurations, without having to hack at your code. You\\ncan choose any arguments that make sense for your use case. In this example,\\nwe’re adding the arguments \", _jsx(InlineCode, {\n          children: \"extra_stop_words\"\n        }), \" (a list of strings) and \", _jsx(InlineCode, {\n          children: \"debug\"\n        }), \"\\n(boolean) for printing additional info when the function runs.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[nlp.before_creation]\\n@callbacks = \\\"customize_language_data\\\"\\nextra_stop_words = [\\\"ooh\\\", \\\"aah\\\"]\\ndebug = true\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"functions.py\",\n          highlight: \"5,7-9\",\n          children: \"from typing import List\\nimport spacy\\n\\n@spacy.registry.callbacks(\\\"customize_language_data\\\")\\ndef create_callback(extra_stop_words: List[str] = [], debug: bool = False):\\n    def customize_language_data(lang_cls):\\n        lang_cls.Defaults.stop_words.update(extra_stop_words)\\n        if debug:\\n            print(\\\"Updated stop words\\\")\\n        return lang_cls\\n\\n    return customize_language_data\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Tip: Use Python type hints\",\n        emoji: \"💡\",\n        children: _jsxs(_components.p, {\n          children: [\"spaCy’s configs are powered by our machine learning library Thinc’s\\n\", _jsx(_components.a, {\n            href: \"https://thinc.ai/docs/usage-config\",\n            children: \"configuration system\"\n          }), \", which supports\\n\", _jsx(_components.a, {\n            href: \"https://docs.python.org/3/library/typing.html\",\n            children: \"type hints\"\n          }), \" and even\\n\", _jsx(_components.a, {\n            href: \"https://thinc.ai/docs/usage-config#advanced-types\",\n            children: \"advanced type annotations\"\n          }), \"\\nusing \", _jsx(_components.a, {\n            href: \"https://github.com/samuelcolvin/pydantic\",\n            children: _jsx(InlineCode, {\n              children: \"pydantic\"\n            })\n          }), \". If your registered\\nfunction provides type hints, the values that are passed in will be checked\\nagainst the expected types. For example, \", _jsx(InlineCode, {\n            children: \"debug: bool\"\n          }), \" in the example above will\\nensure that the value received as the argument \", _jsx(InlineCode, {\n            children: \"debug\"\n          }), \" is a boolean. If the\\nvalue can’t be coerced into a boolean, spaCy will raise an error.\\n\", _jsx(InlineCode, {\n            children: \"debug: pydantic.StrictBool\"\n          }), \" will force the value to be a boolean and raise an\\nerror if it’s not – for instance, if your config defines \", _jsx(InlineCode, {\n            children: \"1\"\n          }), \" instead of \", _jsx(InlineCode, {\n            children: \"true\"\n          }), \".\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"With your \", _jsx(InlineCode, {\n          children: \"functions.py\"\n        }), \" defining additional code and the updated \", _jsx(InlineCode, {\n          children: \"config.cfg\"\n        }), \",\\nyou can now run \", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \" and point the argument \", _jsx(InlineCode, {\n          children: \"--code\"\n        }), \"\\nto your Python file. Before loading the config, spaCy will import the\\n\", _jsx(InlineCode, {\n          children: \"functions.py\"\n        }), \" module and your custom functions will be registered.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ python -m spacy train config.cfg --output ./output --code ./functions.py\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"custom-tokenizer\",\n        children: \"Example: Modifying tokenizer settings \"\n      }), _jsxs(_components.p, {\n        children: [\"Use the \", _jsx(InlineCode, {\n          children: \"initialize.before_init\"\n        }), \" callback to modify the tokenizer settings when\\ntraining a new pipeline. Write a registered callback that modifies the tokenizer\\nsettings and specify this callback in your config:\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[initialize]\\n\\n[initialize.before_init]\\n@callbacks = \\\"customize_tokenizer\\\"\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"functions.py\",\n          children: \"from spacy.util import registry, compile_suffix_regex\\n\\n@registry.callbacks(\\\"customize_tokenizer\\\")\\ndef make_customize_tokenizer():\\n    def customize_tokenizer(nlp):\\n        # remove a suffix\\n        suffixes = list(nlp.Defaults.suffixes)\\n        suffixes.remove(\\\"\\\\\\\\[\\\")\\n        suffix_regex = compile_suffix_regex(suffixes)\\n        nlp.tokenizer.suffix_search = suffix_regex.search\\n\\n        # add a special case\\n        nlp.tokenizer.add_special_case(\\\"_SPECIAL_\\\", [{\\\"ORTH\\\": \\\"_SPECIAL_\\\"}])\\n    return customize_tokenizer\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"When training, provide the function above with the \", _jsx(InlineCode, {\n          children: \"--code\"\n        }), \" option:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ python -m spacy train config.cfg --code ./functions.py\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"Because this callback is only called in the one-time initialization step before\\ntraining, the callback code does not need to be packaged with the final pipeline\\npackage. However, to make it easier for others to replicate your training setup,\\nyou can choose to package the initialization callbacks with the pipeline package\\nor to publish them separately.\"\n      }), _jsxs(Infobox, {\n        variant: \"warning\",\n        title: \"nlp.before_creation vs. initialize.before_init\",\n        children: [_jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"nlp.before_creation\"\n            }), \" is the best place to modify language defaults other than\\nthe tokenizer settings.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"initialize.before_init\"\n            }), \" is the best place to modify tokenizer settings when\\ntraining a new pipeline.\"]\n          }), \"\\n\"]\n        }), _jsxs(_components.p, {\n          children: [\"Unlike the other language defaults, the tokenizer settings are saved with the\\npipeline with \", _jsx(InlineCode, {\n            children: \"nlp.to_disk()\"\n          }), \", so modifications made in \", _jsx(InlineCode, {\n            children: \"nlp.before_creation\"\n          }), \"\\nwill be clobbered by the saved settings when the trained pipeline is loaded from\\ndisk.\"]\n        })]\n      }), _jsx(_components.h4, {\n        id: \"custom-logging\",\n        children: \"Example: Custom logging function \"\n      }), _jsxs(_components.p, {\n        children: [\"During training, the results of each step are passed to a logger function. By\\ndefault, these results are written to the console with the\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#ConsoleLogger\",\n          children: _jsx(InlineCode, {\n            children: \"ConsoleLogger\"\n          })\n        }), \". There is also built-in support\\nfor writing the log files to \", _jsx(_components.a, {\n          href: \"https://www.wandb.com/\",\n          children: \"Weights \u0026 Biases\"\n        }), \" with the\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spacy-loggers#wandblogger\",\n          children: _jsx(InlineCode, {\n            children: \"WandbLogger\"\n          })\n        }), \". On each\\nstep, the logger function receives a \", _jsx(_components.strong, {\n          children: \"dictionary\"\n        }), \" with the following keys:\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Key\"\n            }), _jsx(_components.th, {\n              children: \"Value\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"epoch\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"How many passes over the data have been completed. \", _jsx(_components.del, {\n                children: \"int\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"step\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"How many steps have been completed. \", _jsx(_components.del, {\n                children: \"int\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"score\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The main score from the last evaluation, measured on the dev set. \", _jsx(_components.del, {\n                children: \"float\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"other_scores\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The other scores from the last evaluation, measured on the dev set. \", _jsx(_components.del, {\n                children: \"Dict[str, Any]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"losses\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The accumulated training losses, keyed by component name. \", _jsx(_components.del, {\n                children: \"Dict[str, float]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"checkpoints\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A list of previous results, where each result is a \", _jsx(InlineCode, {\n                children: \"(score, step)\"\n              }), \" tuple. \", _jsx(_components.del, {\n                children: \"List[Tuple[float, int]]\"\n              })]\n            })]\n          })]\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"You can easily implement and plug in your own logger that records the training\\nresults in a custom way, or sends them to an experiment management tracker of\\nyour choice. In this example, the function \", _jsx(InlineCode, {\n          children: \"my_custom_logger.v1\"\n        }), \" writes the\\ntabular results to a file:\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            title: \"config.cfg (excerpt)\",\n            children: \"[training.logger]\\n@loggers = \\\"my_custom_logger.v1\\\"\\nlog_path = \\\"my_file.tab\\\"\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"functions.py\",\n          children: \"import sys\\nfrom typing import IO, Tuple, Callable, Dict, Any, Optional\\nimport spacy\\nfrom spacy import Language\\nfrom pathlib import Path\\n\\n@spacy.registry.loggers(\\\"my_custom_logger.v1\\\")\\ndef custom_logger(log_path):\\n    def setup_logger(\\n        nlp: Language,\\n        stdout: IO=sys.stdout,\\n        stderr: IO=sys.stderr\\n    ) -\u003e Tuple[Callable, Callable]:\\n        stdout.write(f\\\"Logging to {log_path}\\\\\\\\n\\\")\\n        log_file = Path(log_path).open(\\\"w\\\", encoding=\\\"utf8\\\")\\n        log_file.write(\\\"step\\\\\\\\t\\\")\\n        log_file.write(\\\"score\\\\\\\\t\\\")\\n        for pipe in nlp.pipe_names:\\n            log_file.write(f\\\"loss_{pipe}\\\\\\\\t\\\")\\n        log_file.write(\\\"\\\\\\\\n\\\")\\n\\n        def log_step(info: Optional[Dict[str, Any]]):\\n            if info:\\n                log_file.write(f\\\"{info['step']}\\\\\\\\t\\\")\\n                log_file.write(f\\\"{info['score']}\\\\\\\\t\\\")\\n                for pipe in nlp.pipe_names:\\n                    log_file.write(f\\\"{info['losses'][pipe]}\\\\\\\\t\\\")\\n                log_file.write(\\\"\\\\\\\\n\\\")\\n\\n        def finalize():\\n            log_file.close()\\n\\n        return log_step, finalize\\n\\n    return setup_logger\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"custom-code-schedule\",\n        children: \"Example: Custom batch size schedule \"\n      }), _jsxs(_components.p, {\n        children: [\"You can also implement your own batch size schedule to use during training. The\\n\", _jsx(InlineCode, {\n          children: \"@spacy.registry.schedules\"\n        }), \" decorator lets you register that function in the\\n\", _jsx(InlineCode, {\n          children: \"schedules\"\n        }), \" \", _jsx(_components.a, {\n          href: \"/api/top-level#registry\",\n          children: \"registry\"\n        }), \" and assign it a string name:\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Why the version in the name?\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"A big benefit of the config system is that it makes your experiments\\nreproducible. We recommend versioning the functions you register, especially\\nif you expect them to change (like a new model architecture). This way, you\\nknow that a config referencing \", _jsx(InlineCode, {\n            children: \"v1\"\n          }), \" means a different function than a config\\nreferencing \", _jsx(InlineCode, {\n            children: \"v2\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"functions.py\",\n          children: \"import spacy\\n\\n@spacy.registry.schedules(\\\"my_custom_schedule.v1\\\")\\ndef my_custom_schedule(start: int = 1, factor: float = 1.001):\\n   while True:\\n      yield start\\n      start = start * factor\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"In your config, you can now reference the schedule in the\\n\", _jsx(InlineCode, {\n          children: \"[training.batch_size]\"\n        }), \" block via \", _jsx(InlineCode, {\n          children: \"@schedules\"\n        }), \". If a block contains a key\\nstarting with an \", _jsx(InlineCode, {\n          children: \"@\"\n        }), \", it’s interpreted as a reference to a function. All other\\nsettings in the block will be passed to the function as keyword arguments. Keep\\nin mind that the config shouldn’t have any hidden defaults and all arguments on\\nthe functions need to be represented in the config.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"config.cfg (excerpt)\",\n          children: \"[training.batch_size]\\n@schedules = \\\"my_custom_schedule.v1\\\"\\nstart = 2\\nfactor = 1.005\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"custom-architectures\",\n        children: \"Defining custom architectures \"\n      }), _jsxs(_components.p, {\n        children: [\"Built-in pipeline components such as the tagger or named entity recognizer are\\nconstructed with default neural network \", _jsx(_components.a, {\n          href: \"/api/architectures\",\n          children: \"models\"\n        }), \". You can\\nchange the model architecture entirely by implementing your own custom models\\nand providing those in the config when creating the pipeline component. See the\\ndocumentation on \", _jsx(_components.a, {\n          href: \"/usage/layers-architectures\",\n          children: \"layers and model architectures\"\n        }), \"\\nfor more details.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            title: \"config.cfg\",\n            children: \"[components.tagger]\\nfactory = \\\"tagger\\\"\\n\\n[components.tagger.model]\\n@architectures = \\\"custom_neural_network.v1\\\"\\noutput_width = 512\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"functions.py\",\n          children: \"from typing import List\\nfrom thinc.types import Floats2d\\nfrom thinc.api import Model\\nimport spacy\\nfrom spacy.tokens import Doc\\n\\n@spacy.registry.architectures(\\\"custom_neural_network.v1\\\")\\ndef custom_neural_network(output_width: int) -\u003e Model[List[Doc], List[Floats2d]]:\\n    return create_model(output_width)\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-initialization\",\n      children: [_jsx(_components.h2, {\n        id: \"initialization\",\n        children: \"Customizing the initialization \"\n      }), _jsxs(_components.p, {\n        children: [\"When you start training a new model from scratch,\\n\", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \" will call\\n\", _jsx(_components.a, {\n          href: \"/api/language#initialize\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.initialize\"\n          })\n        }), \" to initialize the pipeline and load\\nthe required data. All settings for this are defined in the\\n\", _jsx(_components.a, {\n          href: \"/api/data-formats#config-initialize\",\n          children: _jsx(InlineCode, {\n            children: \"[initialize]\"\n          })\n        }), \" block of the config, so\\nyou can keep track of how the initial \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object was created. The\\ninitialization process typically includes the following:\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg (excerpt)\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[initialize]\\nvectors = ${paths.vectors}\\ninit_tok2vec = ${paths.init_tok2vec}\\n\\n[initialize.components]\\n# Settings for components\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"Load in \", _jsx(_components.strong, {\n            children: \"data resources\"\n          }), \" defined in the \", _jsx(InlineCode, {\n            children: \"[initialize]\"\n          }), \" config, including\\n\", _jsx(_components.strong, {\n            children: \"word vectors\"\n          }), \" and\\n\", _jsx(_components.a, {\n            href: \"/usage/embeddings-transformers/#pretraining\",\n            children: \"pretrained\"\n          }), \" \", _jsx(_components.strong, {\n            children: \"tok2vec\\nweights\"\n          }), \".\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Call the \", _jsx(InlineCode, {\n            children: \"initialize\"\n          }), \" methods of the tokenizer (if implemented, e.g. for\\n\", _jsx(_components.a, {\n            href: \"/usage/models#chinese\",\n            children: \"Chinese\"\n          }), \") and pipeline components with a callback to\\naccess the training data, the current \", _jsx(InlineCode, {\n            children: \"nlp\"\n          }), \" object and any \", _jsx(_components.strong, {\n            children: \"custom\\narguments\"\n          }), \" defined in the \", _jsx(InlineCode, {\n            children: \"[initialize]\"\n          }), \" config.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"In \", _jsx(_components.strong, {\n            children: \"pipeline components\"\n          }), \": if needed, use the data to\\n\", _jsx(_components.a, {\n            href: \"/usage/layers-architectures#thinc-shape-inference\",\n            children: \"infer missing shapes\"\n          }), \" and\\nset up the label scheme if no labels are provided. Components may also load\\nother data like lookup tables or dictionaries.\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"The initialization step allows the config to define \", _jsx(_components.strong, {\n          children: \"all settings\"\n        }), \" required\\nfor the pipeline, while keeping a separation between settings and functions that\\nshould only be used \", _jsx(_components.strong, {\n          children: \"before training\"\n        }), \" to set up the initial pipeline, and\\nlogic and configuration that needs to be available \", _jsx(_components.strong, {\n          children: \"at runtime\"\n        }), \". Without that\\nseparation, it would be very difficult to use the same, reproducible config file\\nbecause the component settings required for training (load data from an external\\nfile) wouldn’t match the component settings required at runtime (load what’s\\nincluded with the saved \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object and don’t depend on external file).\"]\n      }), _jsx(_components.img, {\n        src: \"/images/lifecycle.svg\",\n        alt: \"Illustration of pipeline lifecycle\"\n      }), _jsx(Infobox, {\n        title: \"How components save and load data\",\n        emoji: \"📖\",\n        children: _jsxs(_components.p, {\n          children: [\"For details and examples of how pipeline components can \", _jsx(_components.strong, {\n            children: \"save and load data\\nassets\"\n          }), \" like model weights or lookup tables, and how the component\\ninitialization is implemented under the hood, see the usage guide on\\n\", _jsx(_components.a, {\n            href: \"/usage/processing-pipelines#component-data-initialization\",\n            children: \"serializing and initializing component data\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h4, {\n        id: \"initialization-labels\",\n        children: \"Initializing labels \"\n      }), _jsxs(_components.p, {\n        children: [\"Built-in pipeline components like the\\n\", _jsx(_components.a, {\n          href: \"/api/entityrecognizer\",\n          children: _jsx(InlineCode, {\n            children: \"EntityRecognizer\"\n          })\n        }), \" or\\n\", _jsx(_components.a, {\n          href: \"/api/dependencyparser\",\n          children: _jsx(InlineCode, {\n            children: \"DependencyParser\"\n          })\n        }), \" need to know their available labels\\nand associated internal meta information to initialize their model weights.\\nUsing the \", _jsx(InlineCode, {\n          children: \"get_examples\"\n        }), \" callback provided on initialization, they’re able to\\n\", _jsx(_components.strong, {\n          children: \"read the labels off the training data\"\n        }), \" automatically, which is very\\nconvenient – but it can also slow down the training process to compute this\\ninformation on every run.\"]\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/cli#init-labels\",\n          children: _jsx(InlineCode, {\n            children: \"init labels\"\n          })\n        }), \" command lets you auto-generate JSON\\nfiles containing the label data for all supported components. You can then pass\\nin the labels in the \", _jsx(InlineCode, {\n          children: \"[initialize]\"\n        }), \" settings for the respective components to\\nallow them to initialize faster.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[initialize.components.ner]\\n\\n[initialize.components.ner.labels]\\n@readers = \\\"spacy.read_labels.v1\\\"\\npath = \\\"corpus/labels/ner.json\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ python -m spacy init labels config.cfg ./corpus --paths.train ./corpus/train.spacy\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Under the hood, the command delegates to the \", _jsx(InlineCode, {\n          children: \"label_data\"\n        }), \" property of the\\npipeline components, for instance\\n\", _jsx(_components.a, {\n          href: \"/api/entityrecognizer#label_data\",\n          children: _jsx(InlineCode, {\n            children: \"EntityRecognizer.label_data\"\n          })\n        }), \".\"]\n      }), _jsx(Infobox, {\n        variant: \"warning\",\n        title: \"Important note\",\n        children: _jsxs(_components.p, {\n          children: [\"The JSON format differs for each component and some components need additional\\nmeta information about their labels. The format exported by\\n\", _jsx(_components.a, {\n            href: \"/api/cli#init-labels\",\n            children: _jsx(InlineCode, {\n              children: \"init labels\"\n            })\n          }), \" matches what the components need, so you\\nshould always let spaCy \", _jsx(_components.strong, {\n            children: \"auto-generate the labels\"\n          }), \" for you.\"]\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-data\",\n      children: [_jsx(_components.h2, {\n        id: \"data\",\n        children: \"Data utilities \"\n      }), _jsx(_components.p, {\n        children: \"spaCy includes various features and utilities to make it easy to train models\\nusing your own data, manage training and evaluation corpora, convert existing\\nannotations and configure data augmentation strategies for more robust models.\"\n      }), _jsx(_components.h3, {\n        id: \"data-convert\",\n        children: \"Converting existing corpora and annotations \"\n      }), _jsxs(_components.p, {\n        children: [\"If you have training data in a standard format like \", _jsx(InlineCode, {\n          children: \".conll\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \".conllu\"\n        }), \", the\\neasiest way to convert it for use with spaCy is to run\\n\", _jsx(_components.a, {\n          href: \"/api/cli#convert\",\n          children: _jsx(InlineCode, {\n            children: \"spacy convert\"\n          })\n        }), \" and pass it a file and an output directory.\\nBy default, the command will pick the converter based on the file extension.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ python -m spacy convert ./train.gold.conll ./corpus\\n\"\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"💡 Tip: Converting from Prodigy\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"If you’re using the \", _jsx(_components.a, {\n            href: \"https://prodi.gy\",\n            children: \"Prodigy\"\n          }), \" annotation tool to create\\ntraining data, you can run the\\n\", _jsxs(_components.a, {\n            href: \"https://prodi.gy/docs/recipes#data-to-spacy\",\n            children: [_jsx(InlineCode, {\n              children: \"data-to-spacy\"\n            }), \" command\"]\n          }), \" to\\nmerge and export multiple datasets for use with\\n\", _jsx(_components.a, {\n            href: \"/api/cli#train\",\n            children: _jsx(InlineCode, {\n              children: \"spacy train\"\n            })\n          }), \". Different types of annotations on the same\\ntext will be combined, giving you one corpus to train multiple components.\"]\n        }), \"\\n\"]\n      }), _jsx(Infobox, {\n        title: \"Tip: Manage multi-step workflows with projects\",\n        emoji: \"💡\",\n        children: _jsxs(_components.p, {\n          children: [\"Training workflows often consist of multiple steps, from preprocessing the data\\nall the way to packaging and deploying the trained model.\\n\", _jsx(_components.a, {\n            href: \"/usage/projects\",\n            children: \"spaCy projects\"\n          }), \" let you define all steps in one file, manage\\ndata assets, track changes and share your end-to-end processes with your team.\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The binary \", _jsx(InlineCode, {\n          children: \".spacy\"\n        }), \" format is a serialized \", _jsx(_components.a, {\n          href: \"/api/docbin\",\n          children: _jsx(InlineCode, {\n            children: \"DocBin\"\n          })\n        }), \" containing\\none or more \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" objects. It’s extremely \", _jsx(_components.strong, {\n          children: \"efficient in storage\"\n        }), \",\\nespecially when packing multiple documents together. You can also create \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \"\\nobjects manually, so you can write your own custom logic to convert and store\\nexisting annotations for use in spaCy.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Training data from Doc objects\",\n          highlight: \"6-9\",\n          children: \"import spacy\\nfrom spacy.tokens import Doc, DocBin\\n\\nnlp = spacy.blank(\\\"en\\\")\\ndocbin = DocBin()\\nwords = [\\\"Apple\\\", \\\"is\\\", \\\"looking\\\", \\\"at\\\", \\\"buying\\\", \\\"U.K.\\\", \\\"startup\\\", \\\".\\\"]\\nspaces = [True, True, True, True, True, True, True, False]\\nents = [\\\"B-ORG\\\", \\\"O\\\", \\\"O\\\", \\\"O\\\", \\\"O\\\", \\\"B-GPE\\\", \\\"O\\\", \\\"O\\\"]\\ndoc = Doc(nlp.vocab, words=words, spaces=spaces, ents=ents)\\ndocbin.add(doc)\\ndocbin.to_disk(\\\"./train.spacy\\\")\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"data-corpora\",\n        children: \"Working with corpora \"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[corpora]\\n\\n[corpora.train]\\n@readers = \\\"spacy.Corpus.v1\\\"\\npath = ${paths.train}\\ngold_preproc = false\\nmax_length = 0\\nlimit = 0\\naugmenter = null\\n\\n[training]\\ntrain_corpus = \\\"corpora.train\\\"\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/data-formats#config-corpora\",\n          children: _jsx(InlineCode, {\n            children: \"[corpora]\"\n          })\n        }), \" block in your config lets\\nyou define \", _jsx(_components.strong, {\n          children: \"data resources\"\n        }), \" to use for training, evaluation, pretraining or\\nany other custom workflows. \", _jsx(InlineCode, {\n          children: \"corpora.train\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"corpora.dev\"\n        }), \" are used as\\nconventions within spaCy’s default configs, but you can also define any other\\ncustom blocks. Each section in the corpora config should resolve to a\\n\", _jsx(_components.a, {\n          href: \"/api/corpus\",\n          children: _jsx(InlineCode, {\n            children: \"Corpus\"\n          })\n        }), \" – for example, using spaCy’s built-in\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#corpus-readers\",\n          children: \"corpus reader\"\n        }), \" that takes a path to a binary\\n\", _jsx(InlineCode, {\n          children: \".spacy\"\n        }), \" file. The \", _jsx(InlineCode, {\n          children: \"train_corpus\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"dev_corpus\"\n        }), \" fields in the\\n\", _jsx(_components.a, {\n          href: \"/api/data-formats#config-training\",\n          children: _jsx(InlineCode, {\n            children: \"[training]\"\n          })\n        }), \" block specify where to find\\nthe corpus in your config. This makes it easy to \", _jsx(_components.strong, {\n          children: \"swap out\"\n        }), \" different corpora\\nby only changing a single config setting.\"]\n      }), _jsxs(_components.p, {\n        children: [\"Instead of making \", _jsx(InlineCode, {\n          children: \"[corpora]\"\n        }), \" a block with multiple subsections for each portion\\nof the data, you can also use a single function that returns a dictionary of\\ncorpora, keyed by corpus name, e.g. \", _jsx(InlineCode, {\n          children: \"\\\"train\\\"\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"\\\"dev\\\"\"\n        }), \". This can be\\nespecially useful if you need to split a single file into corpora for training\\nand evaluation, without loading the same file twice.\"]\n      }), _jsxs(_components.p, {\n        children: [\"By default, the training data is loaded into memory and shuffled before each\\nepoch. If the corpus is \", _jsx(_components.strong, {\n          children: \"too large to fit into memory\"\n        }), \" during training, stream\\nthe corpus using a custom reader as described in the next section.\"]\n      }), _jsx(_components.h3, {\n        id: \"custom-code-readers-batchers\",\n        children: \"Custom data reading and batching \"\n      }), _jsxs(_components.p, {\n        children: [\"Some use-cases require \", _jsx(_components.strong, {\n          children: \"streaming in data\"\n        }), \" or manipulating datasets on the\\nfly, rather than generating all data beforehand and storing it to disk. Instead\\nof using the built-in \", _jsx(_components.a, {\n          href: \"/api/corpus\",\n          children: _jsx(InlineCode, {\n            children: \"Corpus\"\n          })\n        }), \" reader, which uses static file\\npaths, you can create and register a custom function that generates\\n\", _jsx(_components.a, {\n          href: \"/api/example\",\n          children: _jsx(InlineCode, {\n            children: \"Example\"\n          })\n        }), \" objects.\"]\n      }), _jsxs(_components.p, {\n        children: [\"In the following example we assume a custom function \", _jsx(InlineCode, {\n          children: \"read_custom_data\"\n        }), \" which\\nloads or generates texts with relevant text classification annotations. Then,\\nsmall lexical variations of the input text are created before generating the\\nfinal \", _jsx(_components.a, {\n          href: \"/api/example\",\n          children: _jsx(InlineCode, {\n            children: \"Example\"\n          })\n        }), \" objects. The \", _jsx(InlineCode, {\n          children: \"@spacy.registry.readers\"\n        }), \" decorator\\nlets you register the function creating the custom reader in the \", _jsx(InlineCode, {\n          children: \"readers\"\n        }), \"\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#registry\",\n          children: \"registry\"\n        }), \" and assign it a string name, so it can be\\nused in your config. All arguments on the registered function become available\\nas \", _jsx(_components.strong, {\n          children: \"config settings\"\n        }), \" – in this case, \", _jsx(InlineCode, {\n          children: \"source\"\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[corpora.train]\\n@readers = \\\"corpus_variants.v1\\\"\\nsource = \\\"s3://your_bucket/path/data.csv\\\"\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"functions.py\",\n          highlight: \"7-8\",\n          children: \"from typing import Callable, Iterator, List\\nimport spacy\\nfrom spacy.training import Example\\nfrom spacy.language import Language\\nimport random\\n\\n@spacy.registry.readers(\\\"corpus_variants.v1\\\")\\ndef stream_data(source: str) -\u003e Callable[[Language], Iterator[Example]]:\\n    def generate_stream(nlp):\\n        for text, cats in read_custom_data(source):\\n            # Create a random variant of the example text\\n            i = random.randint(0, len(text) - 1)\\n            variant = text[:i] + text[i].upper() + text[i + 1:]\\n            doc = nlp.make_doc(variant)\\n            example = Example.from_dict(doc, {\\\"cats\\\": cats})\\n            yield example\\n\\n    return generate_stream\\n\"\n        })\n      }), _jsx(Infobox, {\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Remember that a registered function should always be a function that spaCy\\n\", _jsx(_components.strong, {\n            children: \"calls to create something\"\n          }), \". In this case, it \", _jsx(_components.strong, {\n            children: \"creates the reader function\"\n          }), \"\\n– it’s not the reader itself.\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"If the corpus is \", _jsx(_components.strong, {\n          children: \"too large to load into memory\"\n        }), \" or the corpus reader is an\\n\", _jsx(_components.strong, {\n          children: \"infinite generator\"\n        }), \", use the setting \", _jsx(InlineCode, {\n          children: \"max_epochs = -1\"\n        }), \" to indicate that the\\ntrain corpus should be streamed. With this setting the train corpus is merely\\nstreamed and batched, not shuffled, so any shuffling needs to be implemented in\\nthe corpus reader itself. In the example below, a corpus reader that generates\\nsentences containing even or odd numbers is used with an unlimited number of\\nexamples for the train corpus and a limited number of examples for the dev\\ncorpus. The dev corpus should always be finite and fit in memory during the\\nevaluation step. \", _jsx(InlineCode, {\n          children: \"max_steps\"\n        }), \" and/or \", _jsx(InlineCode, {\n          children: \"patience\"\n        }), \" are used to determine when the\\ntraining should stop.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[corpora.dev]\\n@readers = \\\"even_odd.v1\\\"\\nlimit = 100\\n\\n[corpora.train]\\n@readers = \\\"even_odd.v1\\\"\\nlimit = -1\\n\\n[training]\\nmax_epochs = -1\\npatience = 500\\nmax_steps = 2000\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"functions.py\",\n          children: \"from typing import Callable, Iterable, Iterator\\nfrom spacy import util\\nimport random\\nfrom spacy.training import Example\\nfrom spacy import Language\\n\\n\\n@util.registry.readers(\\\"even_odd.v1\\\")\\ndef create_even_odd_corpus(limit: int = -1) -\u003e Callable[[Language], Iterable[Example]]:\\n    return EvenOddCorpus(limit)\\n\\n\\nclass EvenOddCorpus:\\n    def __init__(self, limit):\\n        self.limit = limit\\n\\n    def __call__(self, nlp: Language) -\u003e Iterator[Example]:\\n        i = 0\\n        while i \u003c self.limit or self.limit \u003c 0:\\n            r = random.randint(0, 1000)\\n            cat = r % 2 == 0\\n            text = \\\"This is sentence \\\" + str(r)\\n            yield Example.from_dict(\\n                nlp.make_doc(text), {\\\"cats\\\": {\\\"EVEN\\\": cat, \\\"ODD\\\": not cat}}\\n            )\\n            i += 1\\n\"\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[initialize.components.textcat.labels]\\n@readers = \\\"spacy.read_labels.v1\\\"\\npath = \\\"labels/textcat.json\\\"\\nrequire = true\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"If the train corpus is streamed, the initialize step peeks at the first 100\\nexamples in the corpus to find the labels for each component. If this isn’t\\nsufficient, you’ll need to \", _jsx(_components.a, {\n          href: \"#initialization-labels\",\n          children: \"provide the labels\"\n        }), \" for each\\ncomponent in the \", _jsx(InlineCode, {\n          children: \"[initialize]\"\n        }), \" block. \", _jsx(_components.a, {\n          href: \"/api/cli#init-labels\",\n          children: _jsx(InlineCode, {\n            children: \"init labels\"\n          })\n        }), \" can\\nbe used to generate JSON files in the correct format, which you can extend with\\nthe full label set.\"]\n      }), _jsxs(_components.p, {\n        children: [\"We can also customize the \", _jsx(_components.strong, {\n          children: \"batching strategy\"\n        }), \" by registering a new batcher\\nfunction in the \", _jsx(InlineCode, {\n          children: \"batchers\"\n        }), \" \", _jsx(_components.a, {\n          href: \"/api/top-level#registry\",\n          children: \"registry\"\n        }), \". A batcher turns\\na stream of items into a stream of batches. spaCy has several useful built-in\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#batchers\",\n          children: \"batching strategies\"\n        }), \" with customizable sizes, but it’s\\nalso easy to implement your own. For instance, the following function takes the\\nstream of generated \", _jsx(_components.a, {\n          href: \"/api/example\",\n          children: _jsx(InlineCode, {\n            children: \"Example\"\n          })\n        }), \" objects, and removes those which\\nhave the same underlying raw text, to avoid duplicates within each batch. Note\\nthat in a more realistic implementation, you’d also want to check whether the\\nannotations are the same.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[training.batcher]\\n@batchers = \\\"filtering_batch.v1\\\"\\nsize = 150\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"functions.py\",\n          children: \"from typing import Callable, Iterable, Iterator, List\\nimport spacy\\nfrom spacy.training import Example\\n\\n@spacy.registry.batchers(\\\"filtering_batch.v1\\\")\\ndef filter_batch(size: int) -\u003e Callable[[Iterable[Example]], Iterator[List[Example]]]:\\n    def create_filtered_batches(examples):\\n        batch = []\\n        for eg in examples:\\n            # Remove duplicate examples with the same text from batch\\n            if eg.text not in [x.text for x in batch]:\\n                batch.append(eg)\\n            if len(batch) == size:\\n                yield batch\\n                batch = []\\n\\n    return create_filtered_batches\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"data-augmentation\",\n        children: \"Data augmentation \"\n      }), _jsxs(_components.p, {\n        children: [\"Data augmentation is the process of applying small \", _jsx(_components.strong, {\n          children: \"modifications\"\n        }), \" to the\\ntraining data. It can be especially useful for punctuation and case replacement\\n– for example, if your corpus only uses smart quotes and you want to include\\nvariations using regular quotes, or to make the model less sensitive to\\ncapitalization by including a mix of capitalized and lowercase examples.\"]\n      }), _jsxs(_components.p, {\n        children: [\"The easiest way to use data augmentation during training is to provide an\\n\", _jsx(InlineCode, {\n          children: \"augmenter\"\n        }), \" to the training corpus, e.g. in the \", _jsx(InlineCode, {\n          children: \"[corpora.train]\"\n        }), \" section of\\nyour config. The built-in \", _jsx(_components.a, {\n          href: \"/api/top-level#orth_variants\",\n          children: _jsx(InlineCode, {\n            children: \"orth_variants\"\n          })\n        }), \"\\naugmenter creates a data augmentation callback that uses orth-variant\\nreplacement.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-ini\",\n          lang: \"ini\",\n          title: \"config.cfg (excerpt)\",\n          highlight: \"8,14\",\n          children: \"[corpora.train]\\n@readers = \\\"spacy.Corpus.v1\\\"\\npath = ${paths.train}\\ngold_preproc = false\\nmax_length = 0\\nlimit = 0\\n\\n[corpora.train.augmenter]\\n@augmenters = \\\"spacy.orth_variants.v1\\\"\\n# Percentage of texts that will be augmented / lowercased\\nlevel = 0.1\\nlower = 0.5\\n\\n[corpora.train.augmenter.orth_variants]\\n@readers = \\\"srsly.read_json.v1\\\"\\npath = \\\"corpus/orth_variants.json\\\"\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"orth_variants\"\n        }), \" argument lets you pass in a dictionary of replacement rules,\\ntypically loaded from a JSON file. There are two types of orth variant rules:\\n\", _jsx(InlineCode, {\n          children: \"\\\"single\\\"\"\n        }), \" for single tokens that should be replaced (e.g. hyphens) and\\n\", _jsx(InlineCode, {\n          children: \"\\\"paired\\\"\"\n        }), \" for pairs of tokens (e.g. quotes).\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-json\",\n          lang: \"json\",\n          title: \"orth_variants.json\",\n          children: \"{\\n  \\\"single\\\": [{ \\\"tags\\\": [\\\"NFP\\\"], \\\"variants\\\": [\\\"…\\\", \\\"...\\\"] }],\\n  \\\"paired\\\": [\\n    {\\n      \\\"tags\\\": [\\\"``\\\", \\\"''\\\"],\\n      \\\"variants\\\": [\\n        [\\\"'\\\", \\\"'\\\"],\\n        [\\\"‘\\\", \\\"’\\\"]\\n      ]\\n    }\\n  ]\\n}\\n\"\n        })\n      }), _jsxs(Accordion, {\n        title: \"Full examples for English and German\",\n        spaced: true,\n        children: [_jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-json\",\n            lang: \"json\",\n            github: \"https://github.com/explosion/spacy-lookups-data/blob/master/spacy_lookups_data/data/en_orth_variants.json\",\n            children: \"https://github.com/explosion/spacy-lookups-data/blob/master/spacy_lookups_data/data/en_orth_variants.json\\n\"\n          })\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-json\",\n            lang: \"json\",\n            github: \"https://github.com/explosion/spacy-lookups-data/blob/master/spacy_lookups_data/data/de_orth_variants.json\",\n            children: \"https://github.com/explosion/spacy-lookups-data/blob/master/spacy_lookups_data/data/de_orth_variants.json\\n\"\n          })\n        })]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"When adding data augmentation, keep in mind that it typically only makes sense\\nto apply it to the \", _jsx(_components.strong, {\n            children: \"training corpus\"\n          }), \", not the development data.\"]\n        })\n      }), _jsx(_components.h4, {\n        id: \"data-augmentation-custom\",\n        children: \"Writing custom data augmenters \"\n      }), _jsxs(_components.p, {\n        children: [\"Using the \", _jsx(_components.a, {\n          href: \"/api/top-level#registry\",\n          children: _jsx(InlineCode, {\n            children: \"@spacy.augmenters\"\n          })\n        }), \" registry, you can also\\nregister your own data augmentation callbacks. The callback should be a function\\nthat takes the current \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object and a training \", _jsx(_components.a, {\n          href: \"/api/example\",\n          children: _jsx(InlineCode, {\n            children: \"Example\"\n          })\n        }), \" and\\nyields \", _jsx(InlineCode, {\n          children: \"Example\"\n        }), \" objects. Keep in mind that the augmenter should yield \", _jsx(_components.strong, {\n          children: \"all\\nexamples\"\n        }), \" you want to use in your corpus, not only the augmented examples\\n(unless you want to augment all examples).\"]\n      }), _jsxs(_components.p, {\n        children: [\"Here’a an example of a custom augmentation callback that produces text variants\\nin \", _jsx(_components.a, {\n          href: \"https://knowyourmeme.com/memes/mocking-spongebob\",\n          children: \"“SpOnGeBoB cAsE”\"\n        }), \". The\\nregistered function takes one argument \", _jsx(InlineCode, {\n          children: \"randomize\"\n        }), \" that can be set via the\\nconfig and decides whether the uppercase/lowercase transformation is applied\\nrandomly or not. The augmenter yields two \", _jsx(InlineCode, {\n          children: \"Example\"\n        }), \" objects: the original\\nexample and the augmented example.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[corpora.train.augmenter]\\n@augmenters = \\\"spongebob_augmenter.v1\\\"\\nrandomize = false\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"import spacy\\nimport random\\n\\n@spacy.registry.augmenters(\\\"spongebob_augmenter.v1\\\")\\ndef create_augmenter(randomize: bool = False):\\n    def augment(nlp, example):\\n        text = example.text\\n        if randomize:\\n            # Randomly uppercase/lowercase characters\\n            chars = [c.lower() if random.random() \u003c 0.5 else c.upper() for c in text]\\n        else:\\n            # Uppercase followed by lowercase\\n            chars = [c.lower() if i % 2 else c.upper() for i, c in enumerate(text)]\\n        # Create augmented training example\\n        example_dict = example.to_dict()\\n        doc = nlp.make_doc(\\\"\\\".join(chars))\\n        example_dict[\\\"token_annotation\\\"][\\\"ORTH\\\"] = [t.text for t in doc]\\n        # Original example followed by augmented example\\n        yield example\\n        yield example.from_dict(doc, example_dict)\\n\\n    return augment\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"An easy way to create modified \", _jsx(InlineCode, {\n          children: \"Example\"\n        }), \" objects is to use the\\n\", _jsx(_components.a, {\n          href: \"/api/example#from_dict\",\n          children: _jsx(InlineCode, {\n            children: \"Example.from_dict\"\n          })\n        }), \" method with a new reference\\n\", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" created from the modified text. In this case, only the\\ncapitalization changes, so only the \", _jsx(InlineCode, {\n          children: \"ORTH\"\n        }), \" values of the tokens will be\\ndifferent between the original and augmented examples.\"]\n      }), _jsxs(_components.p, {\n        children: [\"Note that if your data augmentation strategy involves changing the tokenization\\n(for instance, removing or adding tokens) and your training examples include\\ntoken-based annotations like the dependency parse or entity labels, you’ll need\\nto take care to adjust the \", _jsx(InlineCode, {\n          children: \"Example\"\n        }), \" object so its annotations match and remain\\nvalid.\"]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-parallel-training\",\n      children: [_jsx(_components.h2, {\n        id: \"parallel-training\",\n        children: \"Parallel \u0026 distributed training with Ray \"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Installation\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-bash\",\n            lang: \"bash\",\n            children: \"$ pip install -U spacy[ray]\\n# Check that the CLI is registered\\n$ python -m spacy ray --help\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [_jsx(_components.a, {\n          href: \"https://ray.io/\",\n          children: \"Ray\"\n        }), \" is a fast and simple framework for building and running\\n\", _jsx(_components.strong, {\n          children: \"distributed applications\"\n        }), \". You can use Ray to train spaCy on one or more\\nremote machines, potentially speeding up your training process. Parallel\\ntraining won’t always be faster though – it depends on your batch size, models,\\nand hardware.\"]\n      }), _jsx(Infobox, {\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"To use Ray with spaCy, you need the\\n\", _jsx(_components.a, {\n            href: \"https://github.com/explosion/spacy-ray\",\n            children: _jsx(InlineCode, {\n              children: \"spacy-ray\"\n            })\n          }), \" package installed.\\nInstalling the package will automatically add the \", _jsx(InlineCode, {\n            children: \"ray\"\n          }), \" command to the spaCy\\nCLI.\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/cli#ray-train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy ray train\"\n          })\n        }), \" command follows the same API as\\n\", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \", with a few extra options to configure the Ray\\nsetup. You can optionally set the \", _jsx(InlineCode, {\n          children: \"--address\"\n        }), \" option to point to your Ray\\ncluster. If it’s not set, Ray will run locally.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"python -m spacy ray train config.cfg --n-workers 2\\n\"\n        })\n      }), _jsx(Project, {\n        id: \"integrations/ray\",\n        children: _jsx(_components.p, {\n          children: \"Get started with parallel training using our project template. It trains a\\nsimple model on a Universal Dependencies Treebank and lets you parallelize the\\ntraining with Ray.\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"parallel-training-details\",\n        children: \"How parallel training works \"\n      }), _jsxs(_components.p, {\n        children: [\"Each worker receives a shard of the \", _jsx(_components.strong, {\n          children: \"data\"\n        }), \" and builds a copy of the \", _jsx(_components.strong, {\n          children: \"model\\nand optimizer\"\n        }), \" from the \", _jsx(_components.a, {\n          href: \"#config\",\n          children: _jsx(InlineCode, {\n            children: \"config.cfg\"\n          })\n        }), \". It also has a communication\\nchannel to \", _jsx(_components.strong, {\n          children: \"pass gradients and parameters\"\n        }), \" to the other workers. Additionally,\\neach worker is given ownership of a subset of the parameter arrays. Every\\nparameter array is owned by exactly one worker, and the workers are given a\\nmapping so they know which worker owns which parameter.\"]\n      }), _jsx(_components.img, {\n        src: \"/images/spacy-ray.svg\",\n        alt: \"Illustration of setup\"\n      }), _jsxs(_components.p, {\n        children: [\"As training proceeds, every worker will be computing gradients for \", _jsx(_components.strong, {\n          children: \"all\"\n        }), \" of\\nthe model parameters. When they compute gradients for parameters they don’t own,\\nthey’ll \", _jsx(_components.strong, {\n          children: \"send them to the worker\"\n        }), \" that does own that parameter, along with a\\nversion identifier so that the owner can decide whether to discard the gradient.\\nWorkers use the gradients they receive and the ones they compute locally to\\nupdate the parameters they own, and then broadcast the updated array and a new\\nversion ID to the other workers.\"]\n      }), _jsxs(_components.p, {\n        children: [\"This training procedure is \", _jsx(_components.strong, {\n          children: \"asynchronous\"\n        }), \" and \", _jsx(_components.strong, {\n          children: \"non-blocking\"\n        }), \". Workers always\\npush their gradient increments and parameter updates, they do not have to pull\\nthem and block on the result, so the transfers can happen in the background,\\noverlapped with the actual training work. The workers also do not have to stop\\nand wait for each other (“synchronize”) at the start of each batch. This is very\\nuseful for spaCy, because spaCy is often trained on long documents, which means\\n\", _jsx(_components.strong, {\n          children: \"batches can vary in size\"\n        }), \" significantly. Uneven workloads make synchronous\\ngradient descent inefficient, because if one batch is slow, all of the other\\nworkers are stuck waiting for it to complete before they can continue.\"]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-api\",\n      children: [_jsx(_components.h2, {\n        id: \"api\",\n        children: \"Internal training API \"\n      }), _jsx(Infobox, {\n        variant: \"danger\",\n        children: _jsxs(_components.p, {\n          children: [\"spaCy gives you full control over the training loop. However, for most use\\ncases, it’s recommended to train your pipelines via the\\n\", _jsx(_components.a, {\n            href: \"/api/cli#train\",\n            children: _jsx(InlineCode, {\n              children: \"spacy train\"\n            })\n          }), \" command with a \", _jsx(_components.a, {\n            href: \"#config\",\n            children: _jsx(InlineCode, {\n              children: \"config.cfg\"\n            })\n          }), \" to keep\\ntrack of your settings and hyperparameters, instead of writing your own training\\nscripts from scratch. \", _jsx(_components.a, {\n            href: \"#custom-code\",\n            children: \"Custom registered functions\"\n          }), \" should\\ntypically give you everything you need to train fully custom pipelines with\\n\", _jsx(_components.a, {\n            href: \"/api/cli#train\",\n            children: _jsx(InlineCode, {\n              children: \"spacy train\"\n            })\n          }), \".\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"api-train\",\n        version: \"3.2\",\n        children: \"Training from a Python script \"\n      }), _jsxs(_components.p, {\n        children: [\"If you want to run the training from a Python script instead of using the\\n\", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \" CLI command, you can call into the\\n\", _jsx(_components.a, {\n          href: \"/api/cli#train-function\",\n          children: _jsx(InlineCode, {\n            children: \"train\"\n          })\n        }), \" helper function directly. It takes the path\\nto the config file, an optional output directory and an optional dictionary of\\n\", _jsx(_components.a, {\n          href: \"#config-overrides\",\n          children: \"config overrides\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"from spacy.cli.train import train\\n\\ntrain(\\\"./config.cfg\\\", overrides={\\\"paths.train\\\": \\\"./train.spacy\\\", \\\"paths.dev\\\": \\\"./dev.spacy\\\"})\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"api-loop\",\n        children: \"Internal training loop API \"\n      }), _jsx(Infobox, {\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"This section documents how the training loop and updates to the \", _jsx(InlineCode, {\n            children: \"nlp\"\n          }), \" object\\nwork internally. You typically shouldn’t have to implement this in Python unless\\nyou’re writing your own trainable components. To train a pipeline, use\\n\", _jsx(_components.a, {\n            href: \"/api/cli#train\",\n            children: _jsx(InlineCode, {\n              children: \"spacy train\"\n            })\n          }), \" or the \", _jsx(_components.a, {\n            href: \"/api/cli#train-function\",\n            children: _jsx(InlineCode, {\n              children: \"train\"\n            })\n          }), \" helper\\nfunction instead.\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/example\",\n          children: _jsx(InlineCode, {\n            children: \"Example\"\n          })\n        }), \" object contains annotated training data, also\\ncalled the \", _jsx(_components.strong, {\n          children: \"gold standard\"\n        }), \". It’s initialized with a \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" object\\nthat will hold the predictions, and another \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object that holds the\\ngold-standard annotations. It also includes the \", _jsx(_components.strong, {\n          children: \"alignment\"\n        }), \" between those two\\ndocuments if they differ in tokenization. The \", _jsx(InlineCode, {\n          children: \"Example\"\n        }), \" class ensures that spaCy\\ncan rely on one \", _jsx(_components.strong, {\n          children: \"standardized format\"\n        }), \" that’s passed through the pipeline. For\\ninstance, let’s say we want to define gold-standard part-of-speech tags:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"words = [\\\"I\\\", \\\"like\\\", \\\"stuff\\\"]\\npredicted = Doc(vocab, words=words)\\n# create the reference Doc with gold-standard TAG annotations\\ntags = [\\\"NOUN\\\", \\\"VERB\\\", \\\"NOUN\\\"]\\ntag_ids = [vocab.strings.add(tag) for tag in tags]\\nreference = Doc(vocab, words=words).from_array(\\\"TAG\\\", numpy.array(tag_ids, dtype=\\\"uint64\\\"))\\nexample = Example(predicted, reference)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"As this is quite verbose, there’s an alternative way to create the reference\\n\", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" with the gold-standard annotations. The function \", _jsx(InlineCode, {\n          children: \"Example.from_dict\"\n        }), \" takes\\na dictionary with keyword arguments specifying the annotations, like \", _jsx(InlineCode, {\n          children: \"tags\"\n        }), \" or\\n\", _jsx(InlineCode, {\n          children: \"entities\"\n        }), \". Using the resulting \", _jsx(InlineCode, {\n          children: \"Example\"\n        }), \" object and its gold-standard\\nannotations, the model can be updated to learn a sentence of three words with\\ntheir assigned part-of-speech tags.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"words = [\\\"I\\\", \\\"like\\\", \\\"stuff\\\"]\\ntags = [\\\"NOUN\\\", \\\"VERB\\\", \\\"NOUN\\\"]\\npredicted = Doc(nlp.vocab, words=words)\\nexample = Example.from_dict(predicted, {\\\"tags\\\": tags})\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Here’s another example that shows how to define gold-standard named entities.\\nThe letters added before the labels refer to the tags of the\\n\", _jsx(_components.a, {\n          href: \"/usage/linguistic-features#updating-biluo\",\n          children: \"BILUO scheme\"\n        }), \" – \", _jsx(InlineCode, {\n          children: \"O\"\n        }), \" is a token\\noutside an entity, \", _jsx(InlineCode, {\n          children: \"U\"\n        }), \" a single entity unit, \", _jsx(InlineCode, {\n          children: \"B\"\n        }), \" the beginning of an entity, \", _jsx(InlineCode, {\n          children: \"I\"\n        }), \"\\na token inside an entity and \", _jsx(InlineCode, {\n          children: \"L\"\n        }), \" the last token of an entity.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"doc = Doc(nlp.vocab, words=[\\\"Facebook\\\", \\\"released\\\", \\\"React\\\", \\\"in\\\", \\\"2014\\\"])\\nexample = Example.from_dict(doc, {\\\"entities\\\": [\\\"U-ORG\\\", \\\"O\\\", \\\"U-TECHNOLOGY\\\", \\\"O\\\", \\\"U-DATE\\\"]})\\n\"\n        })\n      }), _jsxs(Infobox, {\n        title: \"Migrating from v2.x\",\n        variant: \"warning\",\n        children: [_jsxs(_components.p, {\n          children: [\"As of v3.0, the \", _jsx(_components.a, {\n            href: \"/api/example\",\n            children: _jsx(InlineCode, {\n              children: \"Example\"\n            })\n          }), \" object replaces the \", _jsx(InlineCode, {\n            children: \"GoldParse\"\n          }), \" class.\\nIt can be constructed in a very similar way – from a \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \" and a dictionary of\\nannotations. For more details, see the\\n\", _jsx(_components.a, {\n            href: \"/usage/v3#migrating-training\",\n            children: \"migration guide\"\n          }), \".\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-diff\",\n            lang: \"diff\",\n            children: \"- gold = GoldParse(doc, entities=entities)\\n+ example = Example.from_dict(doc, {\\\"entities\\\": entities})\\n\"\n          })\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"Of course, it’s not enough to only show a model a single example once.\\nEspecially if you only have few examples, you’ll want to train for a \", _jsx(_components.strong, {\n          children: \"number of\\niterations\"\n        }), \". At each iteration, the training data is \", _jsx(_components.strong, {\n          children: \"shuffled\"\n        }), \" to ensure the\\nmodel doesn’t make any generalizations based on the order of examples. Another\\ntechnique to improve the learning results is to set a \", _jsx(_components.strong, {\n          children: \"dropout rate\"\n        }), \", a rate\\nat which to randomly “drop” individual features and representations. This makes\\nit harder for the model to memorize the training data. For example, a \", _jsx(InlineCode, {\n          children: \"0.25\"\n        }), \"\\ndropout means that each feature or internal representation has a 1/4 likelihood\\nof being dropped.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.a, {\n              href: \"/api/language\",\n              children: _jsx(InlineCode, {\n                children: \"nlp\"\n              })\n            }), \": The \", _jsx(InlineCode, {\n              children: \"nlp\"\n            }), \" object with the pipeline components and\\ntheir models.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.a, {\n              href: \"/api/language#initialize\",\n              children: _jsx(InlineCode, {\n                children: \"nlp.initialize\"\n              })\n            }), \": Initialize the pipeline and\\nreturn an optimizer to update the component model weights.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.a, {\n              href: \"https://thinc.ai/docs/api-optimizers\",\n              children: _jsx(InlineCode, {\n                children: \"Optimizer\"\n              })\n            }), \": Function that holds\\nstate between updates.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.a, {\n              href: \"/api/language#update\",\n              children: _jsx(InlineCode, {\n                children: \"nlp.update\"\n              })\n            }), \": Update component models with examples.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.a, {\n              href: \"/api/example\",\n              children: _jsx(InlineCode, {\n                children: \"Example\"\n              })\n            }), \": object holding predictions and gold-standard\\nannotations.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.a, {\n              href: \"/api/language#to_disk\",\n              children: _jsx(InlineCode, {\n                children: \"nlp.to_disk\"\n              })\n            }), \": Save the updated pipeline to a\\ndirectory.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Example training loop\",\n          children: \"optimizer = nlp.initialize()\\nfor itn in range(100):\\n    random.shuffle(train_data)\\n    for raw_text, entity_offsets in train_data:\\n        doc = nlp.make_doc(raw_text)\\n        example = Example.from_dict(doc, {\\\"entities\\\": entity_offsets})\\n        nlp.update([example], sgd=optimizer)\\nnlp.to_disk(\\\"/output\\\")\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/language#update\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.update\"\n          })\n        }), \" method takes the following arguments:\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"examples\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(_components.a, {\n                href: \"/api/example\",\n                children: _jsx(InlineCode, {\n                  children: \"Example\"\n                })\n              }), \" objects. The \", _jsx(InlineCode, {\n                children: \"update\"\n              }), \" method takes a sequence of them, so you can batch up your training examples.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"drop\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Dropout rate. Makes it harder for the model to just memorize the data.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"sgd\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"An \", _jsx(_components.a, {\n                href: \"https://thinc.ai/docs/api-optimizers\",\n                children: _jsx(InlineCode, {\n                  children: \"Optimizer\"\n                })\n              }), \" object, which updates the model’s weights. If not set, spaCy will create a new one and save it for further use.\"]\n            })]\n          })]\n        })]\n      }), _jsxs(Infobox, {\n        title: \"Migrating from v2.x\",\n        variant: \"warning\",\n        children: [_jsxs(_components.p, {\n          children: [\"As of v3.0, the \", _jsx(_components.a, {\n            href: \"/api/example\",\n            children: _jsx(InlineCode, {\n              children: \"Example\"\n            })\n          }), \" object replaces the \", _jsx(InlineCode, {\n            children: \"GoldParse\"\n          }), \" class\\nand the “simple training style” of calling \", _jsx(InlineCode, {\n            children: \"nlp.update\"\n          }), \" with a text and a\\ndictionary of annotations. Updating your code to use the \", _jsx(InlineCode, {\n            children: \"Example\"\n          }), \" object should\\nbe very straightforward: you can call\\n\", _jsx(_components.a, {\n            href: \"/api/example#from_dict\",\n            children: _jsx(InlineCode, {\n              children: \"Example.from_dict\"\n            })\n          }), \" with a \", _jsx(_components.a, {\n            href: \"/api/doc\",\n            children: _jsx(InlineCode, {\n              children: \"Doc\"\n            })\n          }), \" and the\\ndictionary of annotations:\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-diff\",\n            lang: \"diff\",\n            children: \"text = \\\"Facebook released React in 2014\\\"\\nannotations = {\\\"entities\\\": [\\\"U-ORG\\\", \\\"O\\\", \\\"U-TECHNOLOGY\\\", \\\"O\\\", \\\"U-DATE\\\"]}\\n+ example = Example.from_dict(nlp.make_doc(text), annotations)\\n- nlp.update([text], [annotations])\\n+ nlp.update([example])\\n\"\n          })\n        })]\n      })]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{"title":"Training Pipelines \u0026 Models","teaser":"Train and update components on your own data and integrate custom models","next":"/usage/layers-architectures","menu":[["Introduction","basics"],["Quickstart","quickstart"],["Config System","config"],["Training Data","training-data"],["Custom Training","config-custom"],["Custom Functions","custom-functions"],["Initialization","initialization"],["Data Utilities","data"],["Parallel Training","parallel-training"],["Internal API","api"]]},"scope":{}},"sectionTitle":"Usage Documentation","theme":"blue","section":"usage","apiDetails":{"stringName":null,"baseClass":null,"trainable":null},"isIndex":false},"__N_SSG":true},"page":"/[...listPathPage]","query":{"listPathPage":["usage","training"]},"buildId":"Ugre-usgT1EZhnSeYcBR9","isFallback":false,"dynamicIds":[728],"gsp":true,"scriptLoader":[]}</script></body></html>