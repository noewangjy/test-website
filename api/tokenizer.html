<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="sitemap" type="application/xml" href="/sitemap.xml"/><link rel="shortcut icon" href="/icons/icon-192x192.png"/><link rel="manifest" href="/manifest.webmanifest"/><meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=5.0, shrink-to-fit=no, viewport-fit=cover"/><meta name="theme-color" content="#09a3d5"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png"/><title>Tokenizer Â· spaCy API Documentation</title><meta name="description" content="Segment text into words, punctuations marks, etc."/><meta property="og:title" content="Tokenizer Â· spaCy API Documentation"/><meta property="og:description" content="Segment text into words, punctuations marks, etc."/><meta property="og:type" content="website"/><meta property="og:site_name" content="Tokenizer"/><meta property="og:image" content="[object Object]"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image" content="[object Object]"/><meta name="twitter:creator" content="@spacy_io"/><meta name="twitter:site" content="@spacy_io"/><meta name="twitter:title" content="Tokenizer Â· spaCy API Documentation"/><meta name="twitter:description" content="Segment text into words, punctuations marks, etc."/><meta name="docsearch:language" content="en"/><meta name="next-head-count" content="24"/><link rel="preload" href="/_next/static/css/8f0b94edbc18d62d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8f0b94edbc18d62d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e6995e0e8addcf99.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e6995e0e8addcf99.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script defer="" src="/_next/static/chunks/262.c647d33d06232ef6.js"></script><script defer="" src="/_next/static/chunks/728.cf6ba0da2700fa1b.js"></script><script src="/_next/static/chunks/webpack-8161fc2bb14cec39.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-a0f603ce323043fd.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eb3ea1261af64e73.js" defer=""></script><script src="/_next/static/chunks/94-57434c8b7a6c3878.js" defer=""></script><script src="/_next/static/chunks/128-76b45627a109219b.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...listPathPage%5D-45eea57fe8c2902c.js" defer=""></script><script src="/_next/static/Ugre-usgT1EZhnSeYcBR9/_buildManifest.js" defer=""></script><script src="/_next/static/Ugre-usgT1EZhnSeYcBR9/_ssgManifest.js" defer=""></script></head><body class="theme-blue"><div id="__next"><div class="theme-green"><nav class="navigation_root__yPL8O"><span class="navigation_has-alert__s0Drf"><a class="link_root__1Me7D link_no-link-layout__RPvod" aria-label="spaCy" href="/"><h1 class="navigation_title__pm49s">spaCy</h1></a> <span class="navigation_alert__ZOXon"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/usage/v3-5"><strong>ðŸ’¥ Out now:</strong> spaCy v3.5</a></span></span><div class="navigation_menu__ZMJxN"><select class="dropdown_root__3uiQq navigation_dropdown__4j4pI"><option value="title" disabled="">Menu</option><option value="/usage">Usage</option><option value="/models">Models</option><option value="/api" selected="">API</option><option value="/universe">Universe</option></select><ul class="navigation_list__DCzqi"><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/usage">Usage</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/models">Models</a></li><li class="navigation_item__ln1O1 navigation_is-active__RjVJG"><a class="link_root__1Me7D link_no-link-layout__RPvod" tabindex="-1" href="/api">API</a></li><li class="navigation_item__ln1O1"><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/universe">Universe</a></li><li class="navigation_item__ln1O1 navigation_github__MpFNv"><span><a href="https://github.com/explosion/spaCy" data-size="large" data-show-count="true" aria-label="Star spaCy on GitHub"></a></span></li></ul><div class="navigation_search__BKZCn"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div><progress class="progress_root__9huWN" value="0" max="100"></progress></nav><menu class="sidebar sidebar_root__s2No7"><h1 hidden="" aria-hidden="true" class="h0 sidebar_active-heading___dkf9">Pipeline</h1><div class="sidebar_dropdown__vyqjz"><select class="dropdown_root__3uiQq sidebar_dropdown-select__Nwbq9"><option disabled="">Select page...</option><option value="/api">Overview<!-- --> â€º <!-- -->Library Architecture</option><option value="/api/architectures">Overview<!-- --> â€º <!-- -->Model Architectures</option><option value="/api/data-formats">Overview<!-- --> â€º <!-- -->Data Formats</option><option value="/api/cli">Overview<!-- --> â€º <!-- -->Command Line</option><option value="/api/top-level">Overview<!-- --> â€º <!-- -->Functions</option><option value="/api/doc">Containers<!-- --> â€º <!-- -->Doc</option><option value="/api/docbin">Containers<!-- --> â€º <!-- -->DocBin</option><option value="/api/example">Containers<!-- --> â€º <!-- -->Example</option><option value="/api/language">Containers<!-- --> â€º <!-- -->Language</option><option value="/api/lexeme">Containers<!-- --> â€º <!-- -->Lexeme</option><option value="/api/span">Containers<!-- --> â€º <!-- -->Span</option><option value="/api/spangroup">Containers<!-- --> â€º <!-- -->SpanGroup</option><option value="/api/token">Containers<!-- --> â€º <!-- -->Token</option><option value="/api/attributeruler">Pipeline<!-- --> â€º <!-- -->AttributeRuler</option><option value="/api/coref">Pipeline<!-- --> â€º <!-- -->CoreferenceResolver</option><option value="/api/dependencyparser">Pipeline<!-- --> â€º <!-- -->DependencyParser</option><option value="/api/edittreelemmatizer">Pipeline<!-- --> â€º <!-- -->EditTreeLemmatizer</option><option value="/api/entitylinker">Pipeline<!-- --> â€º <!-- -->EntityLinker</option><option value="/api/entityrecognizer">Pipeline<!-- --> â€º <!-- -->EntityRecognizer</option><option value="/api/entityruler">Pipeline<!-- --> â€º <!-- -->EntityRuler</option><option value="/api/lemmatizer">Pipeline<!-- --> â€º <!-- -->Lemmatizer</option><option value="/api/morphologizer">Pipeline<!-- --> â€º <!-- -->Morphologizer</option><option value="/api/sentencerecognizer">Pipeline<!-- --> â€º <!-- -->SentenceRecognizer</option><option value="/api/sentencizer">Pipeline<!-- --> â€º <!-- -->Sentencizer</option><option value="/api/spancategorizer">Pipeline<!-- --> â€º <!-- -->SpanCategorizer</option><option value="/api/span-resolver">Pipeline<!-- --> â€º <!-- -->SpanResolver</option><option value="/api/spanruler">Pipeline<!-- --> â€º <!-- -->SpanRuler</option><option value="/api/tagger">Pipeline<!-- --> â€º <!-- -->Tagger</option><option value="/api/textcategorizer">Pipeline<!-- --> â€º <!-- -->TextCategorizer</option><option value="/api/tok2vec">Pipeline<!-- --> â€º <!-- -->Tok2Vec</option><option value="/api/tokenizer" selected="">Pipeline<!-- --> â€º <!-- -->Tokenizer</option><option value="/api/pipe">Pipeline<!-- --> â€º <!-- -->TrainablePipe</option><option value="/api/transformer">Pipeline<!-- --> â€º <!-- -->Transformer</option><option value="/api/pipeline-functions">Pipeline<!-- --> â€º <!-- -->Other Functions</option><option value="/api/dependencymatcher">Matchers<!-- --> â€º <!-- -->DependencyMatcher</option><option value="/api/matcher">Matchers<!-- --> â€º <!-- -->Matcher</option><option value="/api/phrasematcher">Matchers<!-- --> â€º <!-- -->PhraseMatcher</option><option value="/api/attributes">Other<!-- --> â€º <!-- -->Attributes</option><option value="/api/corpus">Other<!-- --> â€º <!-- -->Corpus</option><option value="/api/inmemorylookupkb">Other<!-- --> â€º <!-- -->InMemoryLookupKB</option><option value="/api/kb">Other<!-- --> â€º <!-- -->KnowledgeBase</option><option value="/api/lookups">Other<!-- --> â€º <!-- -->Lookups</option><option value="/api/morphology#morphanalysis">Other<!-- --> â€º <!-- -->MorphAnalysis</option><option value="/api/morphology">Other<!-- --> â€º <!-- -->Morphology</option><option value="/api/scorer">Other<!-- --> â€º <!-- -->Scorer</option><option value="/api/stringstore">Other<!-- --> â€º <!-- -->StringStore</option><option value="/api/vectors">Other<!-- --> â€º <!-- -->Vectors</option><option value="/api/vocab">Other<!-- --> â€º <!-- -->Vocab</option><option value="/api/cython">Cython<!-- --> â€º <!-- -->Architecture</option><option value="/api/cython-classes">Cython<!-- --> â€º <!-- -->Classes</option><option value="/api/cython-structs">Cython<!-- --> â€º <!-- -->Structs</option><option value="/api/legacy">Legacy<!-- --> â€º <!-- -->Legacy functions</option></select></div><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Overview</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api">Library Architecture</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/architectures">Model Architectures</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/data-formats">Data Formats</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/cli">Command Line</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/top-level">Functions</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Containers</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/doc">Doc</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/docbin">DocBin</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/example">Example</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/language">Language</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/lexeme">Lexeme</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/span">Span</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/spangroup">SpanGroup</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/token">Token</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Pipeline</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/attributeruler">AttributeRuler</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/coref">CoreferenceResolver</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/dependencyparser">DependencyParser</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/edittreelemmatizer">EditTreeLemmatizer</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/entitylinker">EntityLinker</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/entityrecognizer">EntityRecognizer</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/entityruler">EntityRuler</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/lemmatizer">Lemmatizer</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/morphologizer">Morphologizer</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/sentencerecognizer">SentenceRecognizer</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/sentencizer">Sentencizer</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/spancategorizer">SpanCategorizer</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/span-resolver">SpanResolver</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/spanruler">SpanRuler</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/tagger">Tagger</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/textcategorizer">TextCategorizer</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/tok2vec">Tok2Vec</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP sidebar_is-active__yVTtL is-active" href="/api/tokenizer">Tokenizer</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/pipe">TrainablePipe</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/transformer">Transformer</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/pipeline-functions">Other Functions</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Matchers</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/dependencymatcher">DependencyMatcher</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/matcher">Matcher</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/phrasematcher">PhraseMatcher</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Other</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/attributes">Attributes</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/corpus">Corpus</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/inmemorylookupkb">InMemoryLookupKB</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/kb">KnowledgeBase</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/lookups">Lookups</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/morphology#morphanalysis">MorphAnalysis</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/morphology">Morphology</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/scorer">Scorer</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/stringstore">StringStore</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/vectors">Vectors</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/vocab">Vocab</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Cython</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/cython">Architecture</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/cython-classes">Classes</a></li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/cython-structs">Structs</a></li></ul><ul class="sidebar_section__DArOO"><li class="sidebar_label__V3K28">Legacy</li><li><a class="link_root__1Me7D sidebar_link__sKXFP" href="/api/legacy">Legacy functions</a></li></ul></menu><main class="main_root__7f6Tj main_with-sidebar__uH1df main_with-asides__ikQT6"><article class="main_content__8zFCH"><header class="title_root__pS2WQ"><div class="title_corner__S0Hac"><a class="link_root__1Me7D button_root__jwipc button_secondary__ukZAk" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/spacy/tokenizer.pyx">Source</a></div><h1 id="_title" class="typography_heading__D82WZ typography_h1__b7dt9 title_h1__l3CW1"><span class="heading-text">Tokenizer<!-- --> </span></h1><div class="title_tags__PljaK"><span class="tag_root__NTSnK tag_spaced__Q9amH">class</span></div><div class="heading-teaser title_teaser__QhwCH">Segment text into words, punctuations marks, etc.</div></header><section class="section_root__k1hUl"><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Default config<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-ini language-ini"></code></pre>
</div></div></aside><p>Segment text, and create <code class="code_inline-code__Bq7ot">Doc</code> objects with the discovered segment boundaries.
For a deeper understanding, see the docs on
<a class="link_root__1Me7D" href="/usage/linguistic-features#how-tokenizer-works">how spaCyâ€™s tokenizer works</a>.
The tokenizer is typically created automatically when a
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language"><code class="code_inline-code__Bq7ot">Language</code></a> subclass is initialized and it reads its settings
like punctuation and special case rules from the
<a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/language#defaults"><code class="code_inline-code__Bq7ot">Language.Defaults</code></a> provided by the language subclass.</p></section>
<section id="section-init" class="section_root__k1hUl"><h2 id="init" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#init" class="heading-text typography_permalink__UiIRy">Tokenizer.__init__ <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH">method</span></h2><p>Create a <code class="code_inline-code__Bq7ot">Tokenizer</code> to create <code class="code_inline-code__Bq7ot">Doc</code> objects given unicode text. For examples of
how to construct a custom tokenizer with different tokenization rules, see the
<a class="link_root__1Me7D" href="https://spacy.io/usage/linguistic-features#native-tokenizers">usage documentation</a>.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">vocab</code></td><td class="table_td__rmpJx">A storage container for lexical types. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" href="/api/vocab">Vocab</a></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">rules</code></td><td class="table_td__rmpJx">Exceptions and special-cases for the tokenizer. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation">Optional<span class="code_cli-arg-subtle__IgB5m">[</span>Dict<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">,</span> List<span class="code_cli-arg-subtle__IgB5m">[</span>Dict<span class="code_cli-arg-subtle__IgB5m">[</span>int<span class="code_cli-arg-subtle__IgB5m">,</span> str<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">prefix_search</code></td><td class="table_td__rmpJx">A function matching the signature of <code class="code_inline-code__Bq7ot">re.compile(string).search</code> to match prefixes. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation">Optional<span class="code_cli-arg-subtle__IgB5m">[</span>Callable<span class="code_cli-arg-subtle__IgB5m">[</span><span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> Optional<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/re.html#match-objects">Match</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">suffix_search</code></td><td class="table_td__rmpJx">A function matching the signature of <code class="code_inline-code__Bq7ot">re.compile(string).search</code> to match suffixes. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation">Optional<span class="code_cli-arg-subtle__IgB5m">[</span>Callable<span class="code_cli-arg-subtle__IgB5m">[</span><span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> Optional<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/re.html#match-objects">Match</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">infix_finditer</code></td><td class="table_td__rmpJx">A function matching the signature of <code class="code_inline-code__Bq7ot">re.compile(string).finditer</code> to find infixes. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation">Optional<span class="code_cli-arg-subtle__IgB5m">[</span>Callable<span class="code_cli-arg-subtle__IgB5m">[</span><span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> Iterator<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/re.html#match-objects">Match</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">token_match</code></td><td class="table_td__rmpJx">A function matching the signature of <code class="code_inline-code__Bq7ot">re.compile(string).match</code> to find token matches. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation">Optional<span class="code_cli-arg-subtle__IgB5m">[</span>Callable<span class="code_cli-arg-subtle__IgB5m">[</span><span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> Optional<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/re.html#match-objects">Match</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">url_match</code></td><td class="table_td__rmpJx">A function matching the signature of <code class="code_inline-code__Bq7ot">re.compile(string).match</code> to find token matches after considering prefixes and suffixes. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation">Optional<span class="code_cli-arg-subtle__IgB5m">[</span>Callable<span class="code_cli-arg-subtle__IgB5m">[</span><span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> Optional<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/re.html#match-objects">Match</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">faster_heuristics</code> <span class="tag_root__NTSnK" data-tooltip="This feature is new and was introduced in spaCy v3.3.0">v<!-- -->3.3.0</span></td><td class="table_td__rmpJx">Whether to restrict the final <code class="code_inline-code__Bq7ot">Matcher</code>-based pass for rules to those containing affixes or space. Defaults to <code class="code_inline-code__Bq7ot">True</code>. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">bool</span></td></tr></tbody></table></section>
<section id="section-call" class="section_root__k1hUl"><h2 id="call" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#call" class="heading-text typography_permalink__UiIRy">Tokenizer.__call__ <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH">method</span></h2><p>Tokenize a string.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">string</code></td><td class="table_td__rmpJx">The string to tokenize. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF table_footer__gJRIy table-footer"><td class="table_td__rmpJx"><strong>RETURNS</strong></td><td class="table_td__rmpJx">A container for linguistic annotations. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" href="/api/doc">Doc</a></span></td></tr></tbody></table></section>
<section id="section-pipe" class="section_root__k1hUl"><h2 id="pipe" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#pipe" class="heading-text typography_permalink__UiIRy">Tokenizer.pipe <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH">method</span></h2><p>Tokenize a stream of texts.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">texts</code></td><td class="table_td__rmpJx">A sequence of unicode texts. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Iterable<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">batch_size</code></td><td class="table_td__rmpJx">The number of texts to accumulate in an internal buffer. Defaults to <code class="code_inline-code__Bq7ot">1000</code>. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">int</span></td></tr><tr class="table_tr__K_tkF table_footer__gJRIy table-footer"><td class="table_td__rmpJx"><strong>YIELDS</strong></td><td class="table_td__rmpJx">The tokenized <code class="code_inline-code__Bq7ot">Doc</code> objects, in order. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" href="/api/doc">Doc</a></span></td></tr></tbody></table></section>
<section id="section-find_infix" class="section_root__k1hUl"><h2 id="find_infix" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#find_infix" class="heading-text typography_permalink__UiIRy">Tokenizer.find_infix <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH">method</span></h2><p>Find internal split points of the string.</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">string</code></td><td class="table_td__rmpJx">The string to split. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF table_footer__gJRIy table-footer"><td class="table_td__rmpJx"><strong>RETURNS</strong></td><td class="table_td__rmpJx">A list of <code class="code_inline-code__Bq7ot">re.MatchObject</code> objects that have <code class="code_inline-code__Bq7ot">.start()</code> and <code class="code_inline-code__Bq7ot">.end()</code> methods, denoting the placement of internal segment separators, e.g. hyphens. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">List<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/re.html#match-objects">Match</a><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr></tbody></table></section>
<section id="section-find_prefix" class="section_root__k1hUl"><h2 id="find_prefix" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#find_prefix" class="heading-text typography_permalink__UiIRy">Tokenizer.find_prefix <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH">method</span></h2><p>Find the length of a prefix that should be segmented from the string, or <code class="code_inline-code__Bq7ot">None</code>
if no prefix rules match.</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">string</code></td><td class="table_td__rmpJx">The string to segment. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF table_footer__gJRIy table-footer"><td class="table_td__rmpJx"><strong>RETURNS</strong></td><td class="table_td__rmpJx">The length of the prefix if present, otherwise <code class="code_inline-code__Bq7ot">None</code>. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Optional<span class="code_cli-arg-subtle__IgB5m">[</span>int<span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr></tbody></table></section>
<section id="section-find_suffix" class="section_root__k1hUl"><h2 id="find_suffix" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#find_suffix" class="heading-text typography_permalink__UiIRy">Tokenizer.find_suffix <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH">method</span></h2><p>Find the length of a suffix that should be segmented from the string, or <code class="code_inline-code__Bq7ot">None</code>
if no suffix rules match.</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">string</code></td><td class="table_td__rmpJx">The string to segment. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF table_footer__gJRIy table-footer"><td class="table_td__rmpJx"><strong>RETURNS</strong></td><td class="table_td__rmpJx">The length of the suffix if present, otherwise <code class="code_inline-code__Bq7ot">None</code>. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Optional<span class="code_cli-arg-subtle__IgB5m">[</span>int<span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr></tbody></table></section>
<section id="section-add_special_case" class="section_root__k1hUl"><h2 id="add_special_case" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#add_special_case" class="heading-text typography_permalink__UiIRy">Tokenizer.add_special_case <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH">method</span></h2><p>Add a special-case tokenization rule. This mechanism is also used to add custom
tokenizer exceptions to the language data. See the usage guide on the
<a class="link_root__1Me7D" href="/usage/linguistic-features#language-data">languages data</a> and
<a class="link_root__1Me7D" href="/usage/linguistic-features#special-cases">tokenizer special cases</a> for more
details and examples.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">string</code></td><td class="table_td__rmpJx">The string to specially tokenize. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">token_attrs</code></td><td class="table_td__rmpJx">A sequence of dicts, where each dict describes a token and its attributes. The <code class="code_inline-code__Bq7ot">ORTH</code> fields of the attributes must exactly match the string when they are concatenated. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Iterable<span class="code_cli-arg-subtle__IgB5m">[</span>Dict<span class="code_cli-arg-subtle__IgB5m">[</span>int<span class="code_cli-arg-subtle__IgB5m">,</span> str<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr></tbody></table></section>
<section id="section-explain" class="section_root__k1hUl"><h2 id="explain" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#explain" class="heading-text typography_permalink__UiIRy">Tokenizer.explain <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH">method</span></h2><p>Tokenize a string with a slow debugging tokenizer that provides information
about which tokenizer rule or pattern was matched for each token. The tokens
produced are identical to <code class="code_inline-code__Bq7ot">Tokenizer.__call__</code> except for whitespace tokens.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">string</code></td><td class="table_td__rmpJx">The string to tokenize with the debugging tokenizer. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">str</span></td></tr><tr class="table_tr__K_tkF table_footer__gJRIy table-footer"><td class="table_td__rmpJx"><strong>RETURNS</strong></td><td class="table_td__rmpJx">A list of <code class="code_inline-code__Bq7ot code_wrap__b41os">(pattern_string, token_string)</code> tuples. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">List<span class="code_cli-arg-subtle__IgB5m">[</span>Tuple<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">,</span> str<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr></tbody></table></section>
<section id="section-to_disk" class="section_root__k1hUl"><h2 id="to_disk" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#to_disk" class="heading-text typography_permalink__UiIRy">Tokenizer.to_disk <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH">method</span></h2><p>Serialize the tokenizer to disk.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">path</code></td><td class="table_td__rmpJx">A path to a directory, which will be created if it doesnâ€™t exist. Paths may be either strings or <code class="code_inline-code__Bq7ot">Path</code>-like objects. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Union<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">,</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/pathlib.html">Path</a><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF table_divider__DRQHe"><td class="table_td__rmpJx"><em>keyword-only</em></td><td class="table_td__rmpJx"></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">exclude</code></td><td class="table_td__rmpJx">String names of <a class="link_root__1Me7D" href="/api/tokenizer#serialization-fields">serialization fields</a> to exclude. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Iterable<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr></tbody></table></section>
<section id="section-from_disk" class="section_root__k1hUl"><h2 id="from_disk" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#from_disk" class="heading-text typography_permalink__UiIRy">Tokenizer.from_disk <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH">method</span></h2><p>Load the tokenizer from disk. Modifies the object in place and returns it.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">path</code></td><td class="table_td__rmpJx">A path to a directory. Paths may be either strings or <code class="code_inline-code__Bq7ot">Path</code>-like objects. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Union<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">,</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/pathlib.html">Path</a><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF table_divider__DRQHe"><td class="table_td__rmpJx"><em>keyword-only</em></td><td class="table_td__rmpJx"></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">exclude</code></td><td class="table_td__rmpJx">String names of <a class="link_root__1Me7D" href="/api/tokenizer#serialization-fields">serialization fields</a> to exclude. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Iterable<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF table_footer__gJRIy table-footer"><td class="table_td__rmpJx"><strong>RETURNS</strong></td><td class="table_td__rmpJx">The modified <code class="code_inline-code__Bq7ot">Tokenizer</code> object. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" href="/api/tokenizer">Tokenizer</a></span></td></tr></tbody></table></section>
<section id="section-to_bytes" class="section_root__k1hUl"><h2 id="to_bytes" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#to_bytes" class="heading-text typography_permalink__UiIRy">Tokenizer.to_bytes <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH">method</span></h2><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><p>Serialize the tokenizer to a bytestring.</p><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF table_divider__DRQHe"><td class="table_td__rmpJx"><em>keyword-only</em></td><td class="table_td__rmpJx"></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">exclude</code></td><td class="table_td__rmpJx">String names of <a class="link_root__1Me7D" href="/api/tokenizer#serialization-fields">serialization fields</a> to exclude. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Iterable<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF table_footer__gJRIy table-footer"><td class="table_td__rmpJx"><strong>RETURNS</strong></td><td class="table_td__rmpJx">The serialized form of the <code class="code_inline-code__Bq7ot">Tokenizer</code> object. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">bytes</span></td></tr></tbody></table></section>
<section id="section-from_bytes" class="section_root__k1hUl"><h2 id="from_bytes" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#from_bytes" class="heading-text typography_permalink__UiIRy">Tokenizer.from_bytes <!-- --> </a><span class="tag_root__NTSnK tag_spaced__Q9amH">method</span></h2><p>Load the tokenizer from a bytestring. Modifies the object in place and returns
it.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">bytes_data</code></td><td class="table_td__rmpJx">The data to load from. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">bytes</span></td></tr><tr class="table_tr__K_tkF table_divider__DRQHe"><td class="table_td__rmpJx"><em>keyword-only</em></td><td class="table_td__rmpJx"></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">exclude</code></td><td class="table_td__rmpJx">String names of <a class="link_root__1Me7D" href="/api/tokenizer#serialization-fields">serialization fields</a> to exclude. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation">Iterable<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF table_footer__gJRIy table-footer"><td class="table_td__rmpJx"><strong>RETURNS</strong></td><td class="table_td__rmpJx">The <code class="code_inline-code__Bq7ot">Tokenizer</code> object. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" href="/api/tokenizer">Tokenizer</a></span></td></tr></tbody></table></section>
<section id="section-attributes" class="section_root__k1hUl"><h2 id="attributes" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#attributes" class="heading-text typography_permalink__UiIRy">Attributes <!-- --> </a></h2><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">vocab</code></td><td class="table_td__rmpJx">The vocab object of the parent <code class="code_inline-code__Bq7ot">Doc</code>. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM" role="code" aria-label="Type annotation"><a class="link_root__1Me7D" href="/api/vocab">Vocab</a></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">prefix_search</code></td><td class="table_td__rmpJx">A function to find segment boundaries from the start of a string. Returns the length of the segment, or <code class="code_inline-code__Bq7ot">None</code>. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation">Optional<span class="code_cli-arg-subtle__IgB5m">[</span>Callable<span class="code_cli-arg-subtle__IgB5m">[</span><span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> Optional<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/re.html#match-objects">Match</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">suffix_search</code></td><td class="table_td__rmpJx">A function to find segment boundaries from the end of a string. Returns the length of the segment, or <code class="code_inline-code__Bq7ot">None</code>. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation">Optional<span class="code_cli-arg-subtle__IgB5m">[</span>Callable<span class="code_cli-arg-subtle__IgB5m">[</span><span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> Optional<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/re.html#match-objects">Match</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">infix_finditer</code></td><td class="table_td__rmpJx">A function to find internal segment separators, e.g. hyphens. Returns a (possibly empty) sequence of <code class="code_inline-code__Bq7ot">re.MatchObject</code> objects. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation">Optional<span class="code_cli-arg-subtle__IgB5m">[</span>Callable<span class="code_cli-arg-subtle__IgB5m">[</span><span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> Iterator<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/re.html#match-objects">Match</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">token_match</code></td><td class="table_td__rmpJx">A function matching the signature of <code class="code_inline-code__Bq7ot">re.compile(string).match</code> to find token matches. Returns an <code class="code_inline-code__Bq7ot">re.MatchObject</code> or <code class="code_inline-code__Bq7ot">None</code>. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation">Optional<span class="code_cli-arg-subtle__IgB5m">[</span>Callable<span class="code_cli-arg-subtle__IgB5m">[</span><span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">,</span> Optional<span class="code_cli-arg-subtle__IgB5m">[</span><a class="link_root__1Me7D" rel="noopener nofollow noreferrer" target="_blank" href="https://docs.python.org/3/library/re.html#match-objects">Match</a><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">rules</code></td><td class="table_td__rmpJx">A dictionary of tokenizer exceptions and special cases. <span class="type-annotation language-python code_inline-code__Bq7ot code_type-annotation__6N9RM code_wrap__b41os" role="code" aria-label="Type annotation">Optional<span class="code_cli-arg-subtle__IgB5m">[</span>Dict<span class="code_cli-arg-subtle__IgB5m">[</span>str<span class="code_cli-arg-subtle__IgB5m">,</span> List<span class="code_cli-arg-subtle__IgB5m">[</span>Dict<span class="code_cli-arg-subtle__IgB5m">[</span>int<span class="code_cli-arg-subtle__IgB5m">,</span> str<span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span><span class="code_cli-arg-subtle__IgB5m">]</span></span></td></tr></tbody></table></section>
<section id="section-serialization-fields" class="section_root__k1hUl"><h2 id="serialization-fields" class="typography_heading__D82WZ typography_h2__hzV3h"><a href="#serialization-fields" class="heading-text typography_permalink__UiIRy">Serialization fields <!-- --> </a></h2><p>During serialization, spaCy will export several data fields used to restore
different aspects of the object. If needed, you can exclude them from
serialization by passing in the string names via the <code class="code_inline-code__Bq7ot">exclude</code> argument.</p><aside class="aside_root__667WA"><div class="aside_content__ZN6qm" role="complementary"><div class="aside_text__LFH_Q">
<h4 class="typography_heading__D82WZ typography_h4__CDRaM"><span class="heading-text">Example<!-- --> </span></h4>
<pre class="code_pre__kzg60"><code class="code_code__CILJL language-python language-python"></code></pre>
</div></div></aside><table class="table_root__ZlA_w"><thead><tr class="table_tr__K_tkF"><th class="table_th__QJ9F8">Name</th><th class="table_th__QJ9F8">Description</th></tr></thead><tbody><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">vocab</code></td><td class="table_td__rmpJx">The shared <a class="link_root__1Me7D link_nowrap__H7Oxl link_with-icon__NAVDA" href="/api/vocab"><code class="code_inline-code__Bq7ot">Vocab</code></a>.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">prefix_search</code></td><td class="table_td__rmpJx">The prefix rules.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">suffix_search</code></td><td class="table_td__rmpJx">The suffix rules.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">infix_finditer</code></td><td class="table_td__rmpJx">The infix rules.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">token_match</code></td><td class="table_td__rmpJx">The token match expression.</td></tr><tr class="table_tr__K_tkF"><td class="table_td__rmpJx"><code class="code_inline-code__Bq7ot">exceptions</code></td><td class="table_td__rmpJx">The tokenizer exception rules.</td></tr></tbody></table></section><div class="grid_root__EfDZl grid_spacing__fhBCv grid_half__xoJZs"><div style="margin-top:var(--spacing-lg)"><a class="link_root__1Me7D button_root__jwipc button_secondary__ukZAk" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/tree/master/website/docs/api/tokenizer.mdx">Suggest edits</a></div></div></article><div class="main_asides__RITE5" style="background-image:url(/_next/static/media/pattern_green.92f07e1d.png"></div><footer class="footer_root__zlkjP"><div class="grid_root__EfDZl footer_content__LaE1F grid_narrow__x_6xS grid_spacing__fhBCv grid_third__edHuB"><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">spaCy</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/usage">Usage</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/models">Models</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/api">API Reference</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://course.spacy.io">Online Course</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/custom-solutions">Custom Solutions</a></li></ul></section><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Community</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="/universe">Universe</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/discussions">GitHub Discussions</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy/issues">Issue Tracker</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="http://stackoverflow.com/questions/tagged/spacy">Stack Overflow</a></li></ul></section><section><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Connect</li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://twitter.com/spacy_io">Twitter</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/explosion/spaCy">GitHub</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" rel="noopener nofollow noreferrer" target="_blank" href="https://youtube.com/c/ExplosionAI">YouTube</a></li><li><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/blog">Blog</a></li></ul></section><section class="footer_full___icln"><ul class="footer_column__DPe22"><li class="footer_label__xK7_s">Stay in the loop!</li><li>Receive updates about new releases, tutorials and more.</li><li><form id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" action="//spacy.us12.list-manage.com/subscribe/post?u=83b0498b1e7fa3c91ce68c3f1&amp;amp;id=ecc82e0493" method="post" target="_blank" novalidate=""><div style="position:absolute;left:-5000px" aria-hidden="true"><input type="text" name="b_83b0498b1e7fa3c91ce68c3f1_ecc82e0493" tabindex="-1" value=""/></div><div class="newsletter_root__uh6MU"><input class="newsletter_input___SMSB" id="mce-EMAIL" type="email" name="EMAIL" placeholder="Your email" aria-label="Your email"/><button class="newsletter_button__gKW8E" id="mc-embedded-subscribe" type="submit" name="subscribe">Sign up</button></div></form></li></ul></section></div><div class="footer_content__LaE1F footer_copy__rbjvc"><span>Â© 2016-<!-- -->2023<!-- --> <a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai">Explosion</a></span><a class="link_root__1Me7D footer_logo__BthsJ link_no-link-layout__RPvod" aria-label="Explosion" href="https://explosion.ai"></a><a class="link_root__1Me7D link_no-link-layout__RPvod" href="https://explosion.ai/legal">Legal / Imprint</a></div></footer></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"title":"Tokenizer","teaser":"Segment text into words, punctuations marks, etc.","tag":"class","source":"spacy/tokenizer.pyx","slug":"/api/tokenizer","mdx":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    section: \"section\",\n    blockquote: \"blockquote\",\n    h4: \"h4\",\n    pre: \"pre\",\n    code: \"code\",\n    p: \"p\",\n    a: \"a\",\n    h2: \"h2\",\n    table: \"table\",\n    thead: \"thead\",\n    tr: \"tr\",\n    th: \"th\",\n    tbody: \"tbody\",\n    td: \"td\",\n    del: \"del\",\n    strong: \"strong\",\n    em: \"em\"\n  }, _provideComponents(), props.components), {InlineCode, Tag} = _components;\n  if (!InlineCode) _missingMdxReference(\"InlineCode\", true);\n  if (!Tag) _missingMdxReference(\"Tag\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.section, {\n      children: [_jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Default config\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[nlp.tokenizer]\\n@tokenizers = \\\"spacy.Tokenizer.v1\\\"\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"Segment text, and create \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" objects with the discovered segment boundaries.\\nFor a deeper understanding, see the docs on\\n\", _jsx(_components.a, {\n          href: \"/usage/linguistic-features#how-tokenizer-works\",\n          children: \"how spaCyâ€™s tokenizer works\"\n        }), \".\\nThe tokenizer is typically created automatically when a\\n\", _jsx(_components.a, {\n          href: \"/api/language\",\n          children: _jsx(InlineCode, {\n            children: \"Language\"\n          })\n        }), \" subclass is initialized and it reads its settings\\nlike punctuation and special case rules from the\\n\", _jsx(_components.a, {\n          href: \"/api/language#defaults\",\n          children: _jsx(InlineCode, {\n            children: \"Language.Defaults\"\n          })\n        }), \" provided by the language subclass.\"]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-init\",\n      children: [_jsx(_components.h2, {\n        id: \"init\",\n        tag: \"method\",\n        children: \"Tokenizer.__init__ \"\n      }), _jsxs(_components.p, {\n        children: [\"Create a \", _jsx(InlineCode, {\n          children: \"Tokenizer\"\n        }), \" to create \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" objects given unicode text. For examples of\\nhow to construct a custom tokenizer with different tokenization rules, see the\\n\", _jsx(_components.a, {\n          href: \"https://spacy.io/usage/linguistic-features#native-tokenizers\",\n          children: \"usage documentation\"\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"# Construction 1\\nfrom spacy.tokenizer import Tokenizer\\nfrom spacy.lang.en import English\\nnlp = English()\\n# Create a blank Tokenizer with just the English vocab\\ntokenizer = Tokenizer(nlp.vocab)\\n\\n# Construction 2\\nfrom spacy.lang.en import English\\nnlp = English()\\n# Create a Tokenizer with the default settings for English\\n# including punctuation rules and exceptions\\ntokenizer = nlp.tokenizer\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"vocab\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A storage container for lexical types. \", _jsx(_components.del, {\n                children: \"Vocab\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"rules\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Exceptions and special-cases for the tokenizer. \", _jsx(_components.del, {\n                children: \"Optional[Dict[str, List[Dict[int, str]]]]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"prefix_search\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A function matching the signature of \", _jsx(InlineCode, {\n                children: \"re.compile(string).search\"\n              }), \" to match prefixes. \", _jsx(_components.del, {\n                children: \"Optional[Callable[[str], Optional[Match]]]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"suffix_search\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A function matching the signature of \", _jsx(InlineCode, {\n                children: \"re.compile(string).search\"\n              }), \" to match suffixes. \", _jsx(_components.del, {\n                children: \"Optional[Callable[[str], Optional[Match]]]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"infix_finditer\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A function matching the signature of \", _jsx(InlineCode, {\n                children: \"re.compile(string).finditer\"\n              }), \" to find infixes. \", _jsx(_components.del, {\n                children: \"Optional[Callable[[str], Iterator[Match]]]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"token_match\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A function matching the signature of \", _jsx(InlineCode, {\n                children: \"re.compile(string).match\"\n              }), \" to find token matches. \", _jsx(_components.del, {\n                children: \"Optional[Callable[[str], Optional[Match]]]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"url_match\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A function matching the signature of \", _jsx(InlineCode, {\n                children: \"re.compile(string).match\"\n              }), \" to find token matches after considering prefixes and suffixes. \", _jsx(_components.del, {\n                children: \"Optional[Callable[[str], Optional[Match]]]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"faster_heuristics\"\n              }), \" \", _jsx(Tag, {\n                variant: \"new\",\n                children: \"3.3.0\"\n              })]\n            }), _jsxs(_components.td, {\n              children: [\"Whether to restrict the final \", _jsx(InlineCode, {\n                children: \"Matcher\"\n              }), \"-based pass for rules to those containing affixes or space. Defaults to \", _jsx(InlineCode, {\n                children: \"True\"\n              }), \". \", _jsx(_components.del, {\n                children: \"bool\"\n              })]\n            })]\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-call\",\n      children: [_jsx(_components.h2, {\n        id: \"call\",\n        tag: \"method\",\n        children: \"Tokenizer.__call__ \"\n      }), _jsx(_components.p, {\n        children: \"Tokenize a string.\"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"tokens = tokenizer(\\\"This is a sentence\\\")\\nassert len(tokens) == 4\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"string\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The string to tokenize. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.strong, {\n                children: \"RETURNS\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A container for linguistic annotations. \", _jsx(_components.del, {\n                children: \"Doc\"\n              })]\n            })]\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-pipe\",\n      children: [_jsx(_components.h2, {\n        id: \"pipe\",\n        tag: \"method\",\n        children: \"Tokenizer.pipe \"\n      }), _jsx(_components.p, {\n        children: \"Tokenize a stream of texts.\"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"texts = [\\\"One document.\\\", \\\"...\\\", \\\"Lots of documents\\\"]\\nfor doc in tokenizer.pipe(texts, batch_size=50):\\n    pass\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"texts\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A sequence of unicode texts. \", _jsx(_components.del, {\n                children: \"Iterable[str]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"batch_size\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The number of texts to accumulate in an internal buffer. Defaults to \", _jsx(InlineCode, {\n                children: \"1000\"\n              }), \". \", _jsx(_components.del, {\n                children: \"int\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.strong, {\n                children: \"YIELDS\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The tokenized \", _jsx(InlineCode, {\n                children: \"Doc\"\n              }), \" objects, in order. \", _jsx(_components.del, {\n                children: \"Doc\"\n              })]\n            })]\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-find_infix\",\n      children: [_jsx(_components.h2, {\n        id: \"find_infix\",\n        tag: \"method\",\n        children: \"Tokenizer.find_infix \"\n      }), _jsx(_components.p, {\n        children: \"Find internal split points of the string.\"\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"string\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The string to split. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.strong, {\n                children: \"RETURNS\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A list of \", _jsx(InlineCode, {\n                children: \"re.MatchObject\"\n              }), \" objects that have \", _jsx(InlineCode, {\n                children: \".start()\"\n              }), \" and \", _jsx(InlineCode, {\n                children: \".end()\"\n              }), \" methods, denoting the placement of internal segment separators, e.g. hyphens. \", _jsx(_components.del, {\n                children: \"List[Match]\"\n              })]\n            })]\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-find_prefix\",\n      children: [_jsx(_components.h2, {\n        id: \"find_prefix\",\n        tag: \"method\",\n        children: \"Tokenizer.find_prefix \"\n      }), _jsxs(_components.p, {\n        children: [\"Find the length of a prefix that should be segmented from the string, or \", _jsx(InlineCode, {\n          children: \"None\"\n        }), \"\\nif no prefix rules match.\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"string\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The string to segment. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.strong, {\n                children: \"RETURNS\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The length of the prefix if present, otherwise \", _jsx(InlineCode, {\n                children: \"None\"\n              }), \". \", _jsx(_components.del, {\n                children: \"Optional[int]\"\n              })]\n            })]\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-find_suffix\",\n      children: [_jsx(_components.h2, {\n        id: \"find_suffix\",\n        tag: \"method\",\n        children: \"Tokenizer.find_suffix \"\n      }), _jsxs(_components.p, {\n        children: [\"Find the length of a suffix that should be segmented from the string, or \", _jsx(InlineCode, {\n          children: \"None\"\n        }), \"\\nif no suffix rules match.\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"string\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The string to segment. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.strong, {\n                children: \"RETURNS\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The length of the suffix if present, otherwise \", _jsx(InlineCode, {\n                children: \"None\"\n              }), \". \", _jsx(_components.del, {\n                children: \"Optional[int]\"\n              })]\n            })]\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-add_special_case\",\n      children: [_jsx(_components.h2, {\n        id: \"add_special_case\",\n        tag: \"method\",\n        children: \"Tokenizer.add_special_case \"\n      }), _jsxs(_components.p, {\n        children: [\"Add a special-case tokenization rule. This mechanism is also used to add custom\\ntokenizer exceptions to the language data. See the usage guide on the\\n\", _jsx(_components.a, {\n          href: \"/usage/linguistic-features#language-data\",\n          children: \"languages data\"\n        }), \" and\\n\", _jsx(_components.a, {\n          href: \"/usage/linguistic-features#special-cases\",\n          children: \"tokenizer special cases\"\n        }), \" for more\\ndetails and examples.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"from spacy.attrs import ORTH, NORM\\ncase = [{ORTH: \\\"do\\\"}, {ORTH: \\\"n't\\\", NORM: \\\"not\\\"}]\\ntokenizer.add_special_case(\\\"don't\\\", case)\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"string\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The string to specially tokenize. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"token_attrs\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A sequence of dicts, where each dict describes a token and its attributes. The \", _jsx(InlineCode, {\n                children: \"ORTH\"\n              }), \" fields of the attributes must exactly match the string when they are concatenated. \", _jsx(_components.del, {\n                children: \"Iterable[Dict[int, str]]\"\n              })]\n            })]\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-explain\",\n      children: [_jsx(_components.h2, {\n        id: \"explain\",\n        tag: \"method\",\n        children: \"Tokenizer.explain \"\n      }), _jsxs(_components.p, {\n        children: [\"Tokenize a string with a slow debugging tokenizer that provides information\\nabout which tokenizer rule or pattern was matched for each token. The tokens\\nproduced are identical to \", _jsx(InlineCode, {\n          children: \"Tokenizer.__call__\"\n        }), \" except for whitespace tokens.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"tok_exp = nlp.tokenizer.explain(\\\"(don't)\\\")\\nassert [t[0] for t in tok_exp] == [\\\"PREFIX\\\", \\\"SPECIAL-1\\\", \\\"SPECIAL-2\\\", \\\"SUFFIX\\\"]\\nassert [t[1] for t in tok_exp] == [\\\"(\\\", \\\"do\\\", \\\"n't\\\", \\\")\\\"]\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"string\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The string to tokenize with the debugging tokenizer. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.strong, {\n                children: \"RETURNS\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A list of \", _jsx(InlineCode, {\n                children: \"(pattern_string, token_string)\"\n              }), \" tuples. \", _jsx(_components.del, {\n                children: \"List[Tuple[str, str]]\"\n              })]\n            })]\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-to_disk\",\n      children: [_jsx(_components.h2, {\n        id: \"to_disk\",\n        tag: \"method\",\n        children: \"Tokenizer.to_disk \"\n      }), _jsx(_components.p, {\n        children: \"Serialize the tokenizer to disk.\"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"tokenizer = Tokenizer(nlp.vocab)\\ntokenizer.to_disk(\\\"/path/to/tokenizer\\\")\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"path\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A path to a directory, which will be created if it doesnâ€™t exist. Paths may be either strings or \", _jsx(InlineCode, {\n                children: \"Path\"\n              }), \"-like objects. \", _jsx(_components.del, {\n                children: \"Union[str, Path]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.em, {\n                children: \"keyword-only\"\n              })\n            }), _jsx(_components.td, {})]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"exclude\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"String names of \", _jsx(_components.a, {\n                href: \"#serialization-fields\",\n                children: \"serialization fields\"\n              }), \" to exclude. \", _jsx(_components.del, {\n                children: \"Iterable[str]\"\n              })]\n            })]\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-from_disk\",\n      children: [_jsx(_components.h2, {\n        id: \"from_disk\",\n        tag: \"method\",\n        children: \"Tokenizer.from_disk \"\n      }), _jsx(_components.p, {\n        children: \"Load the tokenizer from disk. Modifies the object in place and returns it.\"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"tokenizer = Tokenizer(nlp.vocab)\\ntokenizer.from_disk(\\\"/path/to/tokenizer\\\")\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"path\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A path to a directory. Paths may be either strings or \", _jsx(InlineCode, {\n                children: \"Path\"\n              }), \"-like objects. \", _jsx(_components.del, {\n                children: \"Union[str, Path]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.em, {\n                children: \"keyword-only\"\n              })\n            }), _jsx(_components.td, {})]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"exclude\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"String names of \", _jsx(_components.a, {\n                href: \"#serialization-fields\",\n                children: \"serialization fields\"\n              }), \" to exclude. \", _jsx(_components.del, {\n                children: \"Iterable[str]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.strong, {\n                children: \"RETURNS\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The modified \", _jsx(InlineCode, {\n                children: \"Tokenizer\"\n              }), \" object. \", _jsx(_components.del, {\n                children: \"Tokenizer\"\n              })]\n            })]\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-to_bytes\",\n      children: [_jsx(_components.h2, {\n        id: \"to_bytes\",\n        tag: \"method\",\n        children: \"Tokenizer.to_bytes \"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"tokenizer = tokenizer(nlp.vocab)\\ntokenizer_bytes = tokenizer.to_bytes()\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"Serialize the tokenizer to a bytestring.\"\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.em, {\n                children: \"keyword-only\"\n              })\n            }), _jsx(_components.td, {})]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"exclude\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"String names of \", _jsx(_components.a, {\n                href: \"#serialization-fields\",\n                children: \"serialization fields\"\n              }), \" to exclude. \", _jsx(_components.del, {\n                children: \"Iterable[str]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.strong, {\n                children: \"RETURNS\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The serialized form of the \", _jsx(InlineCode, {\n                children: \"Tokenizer\"\n              }), \" object. \", _jsx(_components.del, {\n                children: \"bytes\"\n              })]\n            })]\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-from_bytes\",\n      children: [_jsx(_components.h2, {\n        id: \"from_bytes\",\n        tag: \"method\",\n        children: \"Tokenizer.from_bytes \"\n      }), _jsx(_components.p, {\n        children: \"Load the tokenizer from a bytestring. Modifies the object in place and returns\\nit.\"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"tokenizer_bytes = tokenizer.to_bytes()\\ntokenizer = Tokenizer(nlp.vocab)\\ntokenizer.from_bytes(tokenizer_bytes)\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"bytes_data\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The data to load from. \", _jsx(_components.del, {\n                children: \"bytes\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.em, {\n                children: \"keyword-only\"\n              })\n            }), _jsx(_components.td, {})]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"exclude\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"String names of \", _jsx(_components.a, {\n                href: \"#serialization-fields\",\n                children: \"serialization fields\"\n              }), \" to exclude. \", _jsx(_components.del, {\n                children: \"Iterable[str]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.strong, {\n                children: \"RETURNS\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The \", _jsx(InlineCode, {\n                children: \"Tokenizer\"\n              }), \" object. \", _jsx(_components.del, {\n                children: \"Tokenizer\"\n              })]\n            })]\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-attributes\",\n      children: [_jsx(_components.h2, {\n        id: \"attributes\",\n        children: \"Attributes \"\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"vocab\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The vocab object of the parent \", _jsx(InlineCode, {\n                children: \"Doc\"\n              }), \". \", _jsx(_components.del, {\n                children: \"Vocab\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"prefix_search\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A function to find segment boundaries from the start of a string. Returns the length of the segment, or \", _jsx(InlineCode, {\n                children: \"None\"\n              }), \". \", _jsx(_components.del, {\n                children: \"Optional[Callable[[str], Optional[Match]]]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"suffix_search\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A function to find segment boundaries from the end of a string. Returns the length of the segment, or \", _jsx(InlineCode, {\n                children: \"None\"\n              }), \". \", _jsx(_components.del, {\n                children: \"Optional[Callable[[str], Optional[Match]]]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"infix_finditer\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A function to find internal segment separators, e.g. hyphens. Returns a (possibly empty) sequence of \", _jsx(InlineCode, {\n                children: \"re.MatchObject\"\n              }), \" objects. \", _jsx(_components.del, {\n                children: \"Optional[Callable[[str], Iterator[Match]]]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"token_match\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A function matching the signature of \", _jsx(InlineCode, {\n                children: \"re.compile(string).match\"\n              }), \" to find token matches. Returns an \", _jsx(InlineCode, {\n                children: \"re.MatchObject\"\n              }), \" or \", _jsx(InlineCode, {\n                children: \"None\"\n              }), \". \", _jsx(_components.del, {\n                children: \"Optional[Callable[[str], Optional[Match]]]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"rules\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A dictionary of tokenizer exceptions and special cases. \", _jsx(_components.del, {\n                children: \"Optional[Dict[str, List[Dict[int, str]]]]\"\n              })]\n            })]\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-serialization-fields\",\n      children: [_jsx(_components.h2, {\n        id: \"serialization-fields\",\n        children: \"Serialization fields \"\n      }), _jsxs(_components.p, {\n        children: [\"During serialization, spaCy will export several data fields used to restore\\ndifferent aspects of the object. If needed, you can exclude them from\\nserialization by passing in the string names via the \", _jsx(InlineCode, {\n          children: \"exclude\"\n        }), \" argument.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"data = tokenizer.to_bytes(exclude=[\\\"vocab\\\", \\\"exceptions\\\"])\\ntokenizer.from_disk(\\\"./data\\\", exclude=[\\\"token_match\\\"])\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"vocab\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The shared \", _jsx(_components.a, {\n                href: \"/api/vocab\",\n                children: _jsx(InlineCode, {\n                  children: \"Vocab\"\n                })\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"prefix_search\"\n              })\n            }), _jsx(_components.td, {\n              children: \"The prefix rules.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"suffix_search\"\n              })\n            }), _jsx(_components.td, {\n              children: \"The suffix rules.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"infix_finditer\"\n              })\n            }), _jsx(_components.td, {\n              children: \"The infix rules.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"token_match\"\n              })\n            }), _jsx(_components.td, {\n              children: \"The token match expression.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"exceptions\"\n              })\n            }), _jsx(_components.td, {\n              children: \"The tokenizer exception rules.\"\n            })]\n          })]\n        })]\n      })]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{"title":"Tokenizer","teaser":"Segment text into words, punctuations marks, etc.","tag":"class","source":"spacy/tokenizer.pyx"},"scope":{}},"sectionTitle":"API Documentation","theme":"green","section":"api","apiDetails":{"stringName":null,"baseClass":null,"trainable":null},"isIndex":false,"next":null},"__N_SSG":true},"page":"/[...listPathPage]","query":{"listPathPage":["api","tokenizer"]},"buildId":"Ugre-usgT1EZhnSeYcBR9","isFallback":false,"dynamicIds":[728],"gsp":true,"scriptLoader":[]}</script></body></html>