{"pageProps":{"title":"Linguistic Features","next":{"slug":"/usage/rule-based-matching","title":"Rule-based Matching"},"menu":[["POS Tagging","pos-tagging"],["Morphology","morphology"],["Lemmatization","lemmatization"],["Dependency Parse","dependency-parse"],["Named Entities","named-entities"],["Entity Linking","entity-linking"],["Tokenization","tokenization"],["Merging & Splitting","retokenization"],["Sentence Segmentation","sbd"],["Mappings & Exceptions","mappings-exceptions"],["Vectors & Similarity","vectors-similarity"],["Language Data","language-data"]],"slug":"/usage/linguistic-features","mdx":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    section: \"section\",\n    p: \"p\",\n    a: \"a\",\n    h2: \"h2\",\n    strong: \"strong\",\n    table: \"table\",\n    thead: \"thead\",\n    tr: \"tr\",\n    th: \"th\",\n    tbody: \"tbody\",\n    td: \"td\",\n    blockquote: \"blockquote\",\n    h4: \"h4\",\n    ol: \"ol\",\n    li: \"li\",\n    pre: \"pre\",\n    code: \"code\",\n    h3: \"h3\",\n    em: \"em\",\n    ul: \"ul\",\n    hr: \"hr\",\n    img: \"img\"\n  }, _provideComponents(), props.components), {InlineCode, PosDeps101, Infobox, Tag, Iframe, NER101, Tokenization101, Accordion, Vectors101, LanguageData101} = _components;\n  if (!Accordion) _missingMdxReference(\"Accordion\", true);\n  if (!Iframe) _missingMdxReference(\"Iframe\", true);\n  if (!Infobox) _missingMdxReference(\"Infobox\", true);\n  if (!InlineCode) _missingMdxReference(\"InlineCode\", true);\n  if (!LanguageData101) _missingMdxReference(\"LanguageData101\", true);\n  if (!NER101) _missingMdxReference(\"NER101\", true);\n  if (!PosDeps101) _missingMdxReference(\"PosDeps101\", true);\n  if (!Tag) _missingMdxReference(\"Tag\", true);\n  if (!Tokenization101) _missingMdxReference(\"Tokenization101\", true);\n  if (!Vectors101) _missingMdxReference(\"Vectors101\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.section, {\n      children: _jsxs(_components.p, {\n        children: [\"Processing raw text intelligently is difficult: most words are rare, and itâ€™s\\ncommon for words that look completely different to mean almost the same thing.\\nThe same words in a different order can mean something completely different.\\nEven splitting text into useful word-like units can be difficult in many\\nlanguages. While itâ€™s possible to solve some problems starting from only the raw\\ncharacters, itâ€™s usually better to use linguistic knowledge to add useful\\ninformation. Thatâ€™s exactly what spaCy is designed to do: you put in raw text,\\nand get back a \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" object, that comes with a variety of\\nannotations.\"]\n      })\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-pos-tagging\",\n      children: [_jsx(_components.h2, {\n        id: \"pos-tagging\",\n        model: \"tagger, parser\",\n        children: \"Part-of-speech tagging \"\n      }), _jsx(PosDeps101, {}), _jsx(Infobox, {\n        title: \"Part-of-speech tag scheme\",\n        emoji: \"ðŸ“–\",\n        children: _jsxs(_components.p, {\n          children: [\"For a list of the fine-grained and coarse-grained part-of-speech tags assigned\\nby spaCyâ€™s models across different languages, see the label schemes documented\\nin the \", _jsx(_components.a, {\n            href: \"/models\",\n            children: \"models directory\"\n          }), \".\"]\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-morphology\",\n      children: [_jsx(_components.h2, {\n        id: \"morphology\",\n        children: \"Morphology \"\n      }), _jsxs(_components.p, {\n        children: [\"Inflectional morphology is the process by which a root form of a word is\\nmodified by adding prefixes or suffixes that specify its grammatical function\\nbut do not change its part-of-speech. We say that a \", _jsx(_components.strong, {\n          children: \"lemma\"\n        }), \" (root form) is\\n\", _jsx(_components.strong, {\n          children: \"inflected\"\n        }), \" (modified/combined) with one or more \", _jsx(_components.strong, {\n          children: \"morphological features\"\n        }), \" to\\ncreate a surface form. Here are some examples:\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Context\"\n            }), _jsx(_components.th, {\n              children: \"Surface\"\n            }), _jsx(_components.th, {\n              children: \"Lemma\"\n            }), _jsx(_components.th, {\n              children: \"POS\"\n            }), _jsx(_components.th, {\n              children: \"Morphological Features\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"I was reading the paper\"\n            }), _jsx(_components.td, {\n              children: \"reading\"\n            }), _jsx(_components.td, {\n              children: \"read\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VerbForm=Ger\"\n              })\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"I donâ€™t watch the news, I read the paper\"\n            }), _jsx(_components.td, {\n              children: \"read\"\n            }), _jsx(_components.td, {\n              children: \"read\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"VerbForm=Fin\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"Mood=Ind\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"Tense=Pres\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"I read the paper yesterday\"\n            }), _jsx(_components.td, {\n              children: \"read\"\n            }), _jsx(_components.td, {\n              children: \"read\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"VerbForm=Fin\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"Mood=Ind\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"Tense=Past\"\n              })]\n            })]\n          })]\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"Morphological features are stored in the\\n\", _jsx(_components.a, {\n          href: \"/api/morphology#morphanalysis\",\n          children: _jsx(InlineCode, {\n            children: \"MorphAnalysis\"\n          })\n        }), \" under \", _jsx(InlineCode, {\n          children: \"Token.morph\"\n        }), \", which\\nallows you to access individual morphological features.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"ðŸ“ Things to try\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"Change â€œIâ€ to â€œSheâ€. You should see that the morphological features change\\nand express that itâ€™s a pronoun in the third person.\"\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Inspect \", _jsx(InlineCode, {\n              children: \"token.morph\"\n            }), \" for the other tokens.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nprint(\\\"Pipeline:\\\", nlp.pipe_names)\\ndoc = nlp(\\\"I was reading the paper.\\\")\\ntoken = doc[0]  # 'I'\\nprint(token.morph)  # 'Case=Nom|Number=Sing|Person=1|PronType=Prs'\\nprint(token.morph.get(\\\"PronType\\\"))  # ['Prs']\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"morphologizer\",\n        version: \"3\",\n        model: \"morphologizer\",\n        children: \"Statistical morphology \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCyâ€™s statistical \", _jsx(_components.a, {\n          href: \"/api/morphologizer\",\n          children: _jsx(InlineCode, {\n            children: \"Morphologizer\"\n          })\n        }), \" component assigns the\\nmorphological features and coarse-grained part-of-speech tags as \", _jsx(InlineCode, {\n          children: \"Token.morph\"\n        }), \"\\nand \", _jsx(InlineCode, {\n          children: \"Token.pos\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"de_core_news_sm\\\")\\ndoc = nlp(\\\"Wo bist du?\\\") # English: 'Where are you?'\\nprint(doc[2].morph)  # 'Case=Nom|Number=Sing|Person=2|PronType=Prs'\\nprint(doc[2].pos_) # 'PRON'\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"rule-based-morphology\",\n        children: \"Rule-based morphology \"\n      }), _jsxs(_components.p, {\n        children: [\"For languages with relatively simple morphological systems like English, spaCy\\ncan assign morphological features through a rule-based approach, which uses the\\n\", _jsx(_components.strong, {\n          children: \"token text\"\n        }), \" and \", _jsx(_components.strong, {\n          children: \"fine-grained part-of-speech tags\"\n        }), \" to produce\\ncoarse-grained part-of-speech tags and morphological features.\"]\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"The part-of-speech tagger assigns each token a \", _jsx(_components.strong, {\n            children: \"fine-grained part-of-speech\\ntag\"\n          }), \". In the API, these tags are known as \", _jsx(InlineCode, {\n            children: \"Token.tag\"\n          }), \". They express the\\npart-of-speech (e.g. verb) and some amount of morphological information, e.g.\\nthat the verb is past tense (e.g. \", _jsx(InlineCode, {\n            children: \"VBD\"\n          }), \" for a past tense verb in the Penn\\nTreebank) .\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"For words whose coarse-grained POS is not set by a prior process, a\\n\", _jsx(_components.a, {\n            href: \"#mappings-exceptions\",\n            children: \"mapping table\"\n          }), \" maps the fine-grained tags to a\\ncoarse-grained POS tags and morphological features.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Where are you?\\\")\\nprint(doc[2].morph)  # 'Case=Nom|Person=2|PronType=Prs'\\nprint(doc[2].pos_)  # 'PRON'\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-lemmatization\",\n      children: [_jsx(_components.h2, {\n        id: \"lemmatization\",\n        model: \"lemmatizer\",\n        version: \"3\",\n        children: \"Lemmatization \"\n      }), _jsx(_components.p, {\n        children: \"spaCy provides two pipeline components for lemmatization:\"\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"The \", _jsx(_components.a, {\n            href: \"/api/lemmatizer\",\n            children: _jsx(InlineCode, {\n              children: \"Lemmatizer\"\n            })\n          }), \" component provides lookup and rule-based\\nlemmatization methods in a configurable component. An individual language can\\nextend the \", _jsx(InlineCode, {\n            children: \"Lemmatizer\"\n          }), \" as part of its \", _jsx(_components.a, {\n            href: \"#language-data\",\n            children: \"language data\"\n          }), \".\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"The \", _jsx(_components.a, {\n            href: \"/api/edittreelemmatizer\",\n            children: _jsx(InlineCode, {\n              children: \"EditTreeLemmatizer\"\n            })\n          }), \"\\n\", _jsx(Tag, {\n            variant: \"new\",\n            children: \"3.3\"\n          }), \" component provides a trainable lemmatizer.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\n# English pipelines include a rule-based lemmatizer\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nlemmatizer = nlp.get_pipe(\\\"lemmatizer\\\")\\nprint(lemmatizer.mode)  # 'rule'\\n\\ndoc = nlp(\\\"I was reading the paper.\\\")\\nprint([token.lemma_ for token in doc])\\n# ['I', 'be', 'read', 'the', 'paper', '.']\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Changed in v3.0\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Unlike spaCy v2, spaCy v3 models do \", _jsx(_components.em, {\n            children: \"not\"\n          }), \" provide lemmas by default or switch\\nautomatically between lookup and rule-based lemmas depending on whether a tagger\\nis in the pipeline. To have lemmas in a \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \", the pipeline needs to include a\\n\", _jsx(_components.a, {\n            href: \"/api/lemmatizer\",\n            children: _jsx(InlineCode, {\n              children: \"Lemmatizer\"\n            })\n          }), \" component. The lemmatizer component is\\nconfigured to use a single mode such as \", _jsx(InlineCode, {\n            children: \"\\\"lookup\\\"\"\n          }), \" or \", _jsx(InlineCode, {\n            children: \"\\\"rule\\\"\"\n          }), \" on\\ninitialization. The \", _jsx(InlineCode, {\n            children: \"\\\"rule\\\"\"\n          }), \" mode requires \", _jsx(InlineCode, {\n            children: \"Token.pos\"\n          }), \" to be set by a previous\\ncomponent.\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The data for spaCyâ€™s lemmatizers is distributed in the package\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spacy-lookups-data\",\n          children: _jsx(InlineCode, {\n            children: \"spacy-lookups-data\"\n          })\n        }), \". The\\nprovided trained pipelines already include all the required tables, but if you\\nare creating new pipelines, youâ€™ll probably want to install \", _jsx(InlineCode, {\n          children: \"spacy-lookups-data\"\n        }), \"\\nto provide the data when the lemmatizer is initialized.\"]\n      }), _jsx(_components.h3, {\n        id: \"lemmatizer-lookup\",\n        children: \"Lookup lemmatizer \"\n      }), _jsxs(_components.p, {\n        children: [\"For pipelines without a tagger or morphologizer, a lookup lemmatizer can be\\nadded to the pipeline as long as a lookup table is provided, typically through\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spacy-lookups-data\",\n          children: _jsx(InlineCode, {\n            children: \"spacy-lookups-data\"\n          })\n        }), \". The\\nlookup lemmatizer looks up the token surface form in the lookup table without\\nreference to the tokenâ€™s part-of-speech or context.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# pip install -U spacy[lookups]\\nimport spacy\\n\\nnlp = spacy.blank(\\\"sv\\\")\\nnlp.add_pipe(\\\"lemmatizer\\\", config={\\\"mode\\\": \\\"lookup\\\"})\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"lemmatizer-rule\",\n        children: \"Rule-based lemmatizer \"\n      }), _jsxs(_components.p, {\n        children: [\"When training pipelines that include a component that assigns part-of-speech\\ntags (a morphologizer or a tagger with a \", _jsx(_components.a, {\n          href: \"#mappings-exceptions\",\n          children: \"POS mapping\"\n        }), \"), a\\nrule-based lemmatizer can be added using rule tables from\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spacy-lookups-data\",\n          children: _jsx(InlineCode, {\n            children: \"spacy-lookups-data\"\n          })\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# pip install -U spacy[lookups]\\nimport spacy\\n\\nnlp = spacy.blank(\\\"de\\\")\\n# Morphologizer (note: model is not yet trained!)\\nnlp.add_pipe(\\\"morphologizer\\\")\\n# Rule-based lemmatizer\\nnlp.add_pipe(\\\"lemmatizer\\\", config={\\\"mode\\\": \\\"rule\\\"})\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The rule-based deterministic lemmatizer maps the surface form to a lemma in\\nlight of the previously assigned coarse-grained part-of-speech and morphological\\ninformation, without consulting the context of the token. The rule-based\\nlemmatizer also accepts list-based exception files. For English, these are\\nacquired from \", _jsx(_components.a, {\n          href: \"https://wordnet.princeton.edu/\",\n          children: \"WordNet\"\n        }), \".\"]\n      }), _jsx(_components.h3, {\n        children: \"Trainable lemmatizer\"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/edittreelemmatizer\",\n          children: _jsx(InlineCode, {\n            children: \"EditTreeLemmatizer\"\n          })\n        }), \" can learn form-to-lemma\\ntransformations from a training corpus that includes lemma annotations. This\\nremoves the need to write language-specific rules and can (in many cases)\\nprovide higher accuracies than lookup and rule-based lemmatizers.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"import spacy\\n\\nnlp = spacy.blank(\\\"de\\\")\\nnlp.add_pipe(\\\"trainable_lemmatizer\\\", name=\\\"lemmatizer\\\")\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-dependency-parse\",\n      children: [_jsx(_components.h2, {\n        id: \"dependency-parse\",\n        model: \"parser\",\n        children: \"Dependency Parsing \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy features a fast and accurate syntactic dependency parser, and has a rich\\nAPI for navigating the tree. The parser also powers the sentence boundary\\ndetection, and lets you iterate over base noun phrases, or â€œchunksâ€. You can\\ncheck whether a \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" object has been parsed by calling\\n\", _jsx(InlineCode, {\n          children: \"doc.has_annotation(\\\"DEP\\\")\"\n        }), \", which checks whether the attribute \", _jsx(InlineCode, {\n          children: \"Token.dep\"\n        }), \" has\\nbeen set returns a boolean value. If the result is \", _jsx(InlineCode, {\n          children: \"False\"\n        }), \", the default sentence\\niterator will raise an exception.\"]\n      }), _jsx(Infobox, {\n        title: \"Dependency label scheme\",\n        emoji: \"ðŸ“–\",\n        children: _jsxs(_components.p, {\n          children: [\"For a list of the syntactic dependency labels assigned by spaCyâ€™s models across\\ndifferent languages, see the label schemes documented in the\\n\", _jsx(_components.a, {\n            href: \"/models\",\n            children: \"models directory\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"noun-chunks\",\n        children: \"Noun chunks \"\n      }), _jsxs(_components.p, {\n        children: [\"Noun chunks are â€œbase noun phrasesâ€ â€“ flat phrases that have a noun as their\\nhead. You can think of noun chunks as a noun plus the words describing the noun\\nâ€“ for example, â€œthe lavish green grassâ€ or â€œthe worldâ€™s largest tech fundâ€. To\\nget the noun chunks in a document, simply iterate over\\n\", _jsx(_components.a, {\n          href: \"/api/doc#noun_chunks\",\n          children: _jsx(InlineCode, {\n            children: \"Doc.noun_chunks\"\n          })\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Autonomous cars shift insurance liability toward manufacturers\\\")\\nfor chunk in doc.noun_chunks:\\n    print(chunk.text, chunk.root.text, chunk.root.dep_,\\n            chunk.root.head.text)\\n\"\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Text:\"\n            }), \" The original noun chunk text.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Root text:\"\n            }), \" The original text of the word connecting the noun chunk to\\nthe rest of the parse.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Root dep:\"\n            }), \" Dependency relation connecting the root to its head.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Root head text:\"\n            }), \" The text of the root tokenâ€™s head.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Text\"\n            }), _jsx(_components.th, {\n              children: \"root.text\"\n            }), _jsx(_components.th, {\n              children: \"root.dep_\"\n            }), _jsx(_components.th, {\n              children: \"root.head.text\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Autonomous cars\"\n            }), _jsx(_components.td, {\n              children: \"cars\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nsubj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"shift\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"insurance liability\"\n            }), _jsx(_components.td, {\n              children: \"liability\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"dobj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"shift\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"manufacturers\"\n            }), _jsx(_components.td, {\n              children: \"manufacturers\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"pobj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"toward\"\n            })]\n          })]\n        })]\n      }), _jsx(_components.h3, {\n        id: \"navigating\",\n        children: \"Navigating the parse tree \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy uses the terms \", _jsx(_components.strong, {\n          children: \"head\"\n        }), \" and \", _jsx(_components.strong, {\n          children: \"child\"\n        }), \" to describe the words \", _jsx(_components.strong, {\n          children: \"connected by\\na single arc\"\n        }), \" in the dependency tree. The term \", _jsx(_components.strong, {\n          children: \"dep\"\n        }), \" is used for the arc\\nlabel, which describes the type of syntactic relation that connects the child to\\nthe head. As with other attributes, the value of \", _jsx(InlineCode, {\n          children: \".dep\"\n        }), \" is a hash value. You can\\nget the string value with \", _jsx(InlineCode, {\n          children: \".dep_\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Autonomous cars shift insurance liability toward manufacturers\\\")\\nfor token in doc:\\n    print(token.text, token.dep_, token.head.text, token.head.pos_,\\n            [child for child in token.children])\\n\"\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Text:\"\n            }), \" The original token text.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Dep:\"\n            }), \" The syntactic relation connecting child to head.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Head text:\"\n            }), \" The original text of the token head.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Head POS:\"\n            }), \" The part-of-speech tag of the token head.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Children:\"\n            }), \" The immediate syntactic dependents of the token.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Text\"\n            }), _jsx(_components.th, {\n              children: \"Dep\"\n            }), _jsx(_components.th, {\n              children: \"Head text\"\n            }), _jsx(_components.th, {\n              children: \"Head POS\"\n            }), _jsx(_components.th, {\n              children: \"Children\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Autonomous\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"amod\"\n              })\n            }), _jsx(_components.td, {\n              children: \"cars\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NOUN\"\n              })\n            }), _jsx(_components.td, {})]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"cars\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nsubj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"shift\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Autonomous\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"shift\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"ROOT\"\n              })\n            }), _jsx(_components.td, {\n              children: \"shift\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsx(_components.td, {\n              children: \"cars, liability, toward\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"insurance\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"compound\"\n              })\n            }), _jsx(_components.td, {\n              children: \"liability\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NOUN\"\n              })\n            }), _jsx(_components.td, {})]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"liability\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"dobj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"shift\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsx(_components.td, {\n              children: \"insurance\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"toward\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"prep\"\n              })\n            }), _jsx(_components.td, {\n              children: \"shift\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NOUN\"\n              })\n            }), _jsx(_components.td, {\n              children: \"manufacturers\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"manufacturers\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"pobj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"toward\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"ADP\"\n              })\n            }), _jsx(_components.td, {})]\n          })]\n        })]\n      }), _jsx(Iframe, {\n        title: \"displaCy visualization of dependencies and entities 2\",\n        src: \"/images/displacy-long2.html\",\n        height: 450\n      }), _jsxs(_components.p, {\n        children: [\"Because the syntactic relations form a tree, every word has \", _jsx(_components.strong, {\n          children: \"exactly one\\nhead\"\n        }), \". You can therefore iterate over the arcs in the tree by iterating over\\nthe words in the sentence. This is usually the best way to match an arc of\\ninterest â€“ from below:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.symbols import nsubj, VERB\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Autonomous cars shift insurance liability toward manufacturers\\\")\\n\\n# Finding a verb with a subject from below â€” good\\nverbs = set()\\nfor possible_subject in doc:\\n    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\\n        verbs.add(possible_subject.head)\\nprint(verbs)\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"If you try to match from above, youâ€™ll have to iterate twice. Once for the head,\\nand then again through the children:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# Finding a verb with a subject from above â€” less good\\nverbs = []\\nfor possible_verb in doc:\\n    if possible_verb.pos == VERB:\\n        for possible_subject in possible_verb.children:\\n            if possible_subject.dep == nsubj:\\n                verbs.append(possible_verb)\\n                break\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"To iterate through the children, use the \", _jsx(InlineCode, {\n          children: \"token.children\"\n        }), \" attribute, which\\nprovides a sequence of \", _jsx(_components.a, {\n          href: \"/api/token\",\n          children: _jsx(InlineCode, {\n            children: \"Token\"\n          })\n        }), \" objects.\"]\n      }), _jsx(_components.h4, {\n        id: \"navigating-around\",\n        children: \"Iterating around the local tree \"\n      }), _jsxs(_components.p, {\n        children: [\"A few more convenience attributes are provided for iterating around the local\\ntree from the token. \", _jsx(_components.a, {\n          href: \"/api/token#lefts\",\n          children: _jsx(InlineCode, {\n            children: \"Token.lefts\"\n          })\n        }), \" and\\n\", _jsx(_components.a, {\n          href: \"/api/token#rights\",\n          children: _jsx(InlineCode, {\n            children: \"Token.rights\"\n          })\n        }), \" attributes provide sequences of syntactic\\nchildren that occur before and after the token. Both sequences are in sentence\\norder. There are also two integer-typed attributes,\\n\", _jsx(_components.a, {\n          href: \"/api/token#n_lefts\",\n          children: _jsx(InlineCode, {\n            children: \"Token.n_lefts\"\n          })\n        }), \" and\\n\", _jsx(_components.a, {\n          href: \"/api/token#n_rights\",\n          children: _jsx(InlineCode, {\n            children: \"Token.n_rights\"\n          })\n        }), \" that give the number of left and right\\nchildren.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"bright red apples on the tree\\\")\\nprint([token.text for token in doc[2].lefts])  # ['bright', 'red']\\nprint([token.text for token in doc[2].rights])  # ['on']\\nprint(doc[2].n_lefts)  # 2\\nprint(doc[2].n_rights)  # 1\\n\"\n        })\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"de_core_news_sm\\\")\\ndoc = nlp(\\\"schÃ¶ne rote Ã„pfel auf dem Baum\\\")\\nprint([token.text for token in doc[2].lefts])  # ['schÃ¶ne', 'rote']\\nprint([token.text for token in doc[2].rights])  # ['auf']\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"You can get a whole phrase by its syntactic head using the\\n\", _jsx(_components.a, {\n          href: \"/api/token#subtree\",\n          children: _jsx(InlineCode, {\n            children: \"Token.subtree\"\n          })\n        }), \" attribute. This returns an ordered\\nsequence of tokens. You can walk up the tree with the\\n\", _jsx(_components.a, {\n          href: \"/api/token#ancestors\",\n          children: _jsx(InlineCode, {\n            children: \"Token.ancestors\"\n          })\n        }), \" attribute, and check dominance with\\n\", _jsx(_components.a, {\n          href: \"/api/token#is_ancestor\",\n          children: _jsx(InlineCode, {\n            children: \"Token.is_ancestor\"\n          })\n        })]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Projective vs. non-projective\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"For the \", _jsx(_components.a, {\n            href: \"/models/en\",\n            children: \"default English pipelines\"\n          }), \", the parse tree is\\n\", _jsx(_components.strong, {\n            children: \"projective\"\n          }), \", which means that there are no crossing brackets. The tokens\\nreturned by \", _jsx(InlineCode, {\n            children: \".subtree\"\n          }), \" are therefore guaranteed to be contiguous. This is not\\ntrue for the German pipelines, which have many\\n\", _jsx(_components.a, {\n            href: \"https://explosion.ai/blog/german-model#word-order\",\n            children: \"non-projective dependencies\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Credit and mortgage account holders must submit their requests\\\")\\n\\nroot = [token for token in doc if token.head == token][0]\\nsubject = list(root.lefts)[0]\\nfor descendant in subject.subtree:\\n    assert subject is descendant or subject.is_ancestor(descendant)\\n    print(descendant.text, descendant.dep_, descendant.n_lefts,\\n            descendant.n_rights,\\n            [ancestor.text for ancestor in descendant.ancestors])\\n\"\n        })\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Text\"\n            }), _jsx(_components.th, {\n              children: \"Dep\"\n            }), _jsx(_components.th, {\n              children: \"n_lefts\"\n            }), _jsx(_components.th, {\n              children: \"n_rights\"\n            }), _jsx(_components.th, {\n              children: \"ancestors\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Credit\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nmod\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"2\"\n              })\n            }), _jsx(_components.td, {\n              children: \"holders, submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"and\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"cc\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: \"holders, submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"mortgage\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"compound\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: \"account, Credit, holders, submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"account\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"conj\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"1\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Credit, holders, submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"holders\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nsubj\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"1\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"0\"\n              })\n            }), _jsx(_components.td, {\n              children: \"submit\"\n            })]\n          })]\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"Finally, the \", _jsx(InlineCode, {\n          children: \".left_edge\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \".right_edge\"\n        }), \" attributes can be especially useful,\\nbecause they give you the first and last token of the subtree. This is the\\neasiest way to create a \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" object for a syntactic phrase. Note that\\n\", _jsx(InlineCode, {\n          children: \".right_edge\"\n        }), \" gives a token \", _jsx(_components.strong, {\n          children: \"within\"\n        }), \" the subtree â€“ so if you use it as the\\nend-point of a range, donâ€™t forget to \", _jsx(InlineCode, {\n          children: \"+1\"\n        }), \"!\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Credit and mortgage account holders must submit their requests\\\")\\nspan = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]\\nwith doc.retokenize() as retokenizer:\\n    retokenizer.merge(span)\\nfor token in doc:\\n    print(token.text, token.pos_, token.dep_, token.head.text)\\n\"\n        })\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Text\"\n            }), _jsx(_components.th, {\n              children: \"POS\"\n            }), _jsx(_components.th, {\n              children: \"Dep\"\n            }), _jsx(_components.th, {\n              children: \"Head text\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Credit and mortgage account holders\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NOUN\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"nsubj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"must\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"aux\"\n              })\n            }), _jsx(_components.td, {\n              children: \"submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"submit\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"VERB\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"ROOT\"\n              })\n            }), _jsx(_components.td, {\n              children: \"submit\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"their\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"ADJ\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"poss\"\n              })\n            }), _jsx(_components.td, {\n              children: \"requests\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"requests\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NOUN\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"dobj\"\n              })\n            }), _jsx(_components.td, {\n              children: \"submit\"\n            })]\n          })]\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"The dependency parse can be a useful tool for \", _jsx(_components.strong, {\n          children: \"information extraction\"\n        }), \",\\nespecially when combined with other predictions like\\n\", _jsx(_components.a, {\n          href: \"#named-entities\",\n          children: \"named entities\"\n        }), \". The following example extracts money and\\ncurrency values, i.e. entities labeled as \", _jsx(InlineCode, {\n          children: \"MONEY\"\n        }), \", and then uses the dependency\\nparse to find the noun phrase they are referring to â€“ for example \", _jsx(InlineCode, {\n          children: \"\\\"Net income\\\"\"\n        }), \"\\nâ†’ \", _jsx(InlineCode, {\n          children: \"\\\"$9.4 million\\\"\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\n# Merge noun phrases and entities for easier analysis\\nnlp.add_pipe(\\\"merge_entities\\\")\\nnlp.add_pipe(\\\"merge_noun_chunks\\\")\\n\\nTEXTS = [\\n    \\\"Net income was $9.4 million compared to the prior year of $2.7 million.\\\",\\n    \\\"Revenue exceeded twelve billion dollars, with a loss of $1b.\\\",\\n]\\nfor doc in nlp.pipe(TEXTS):\\n    for token in doc:\\n        if token.ent_type_ == \\\"MONEY\\\":\\n            # We have an attribute and direct object, so check for subject\\n            if token.dep_ in (\\\"attr\\\", \\\"dobj\\\"):\\n                subj = [w for w in token.head.lefts if w.dep_ == \\\"nsubj\\\"]\\n                if subj:\\n                    print(subj[0], \\\"-->\\\", token)\\n            # We have a prepositional object with a preposition\\n            elif token.dep_ == \\\"pobj\\\" and token.head.dep_ == \\\"prep\\\":\\n                print(token.head.head, \\\"-->\\\", token)\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Combining models and rules\",\n        emoji: \"ðŸ“–\",\n        children: _jsxs(_components.p, {\n          children: [\"For more examples of how to write rule-based information extraction logic that\\ntakes advantage of the modelâ€™s predictions produced by the different components,\\nsee the usage guide on\\n\", _jsx(_components.a, {\n            href: \"/usage/rule-based-matching#models-rules\",\n            children: \"combining models and rules\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"displacy\",\n        children: \"Visualizing dependencies \"\n      }), _jsxs(_components.p, {\n        children: [\"The best way to understand spaCyâ€™s dependency parser is interactively. To make\\nthis easier, spaCy comes with a visualization module. You can pass a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" or a\\nlist of \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" objects to displaCy and run\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#displacy.serve\",\n          children: _jsx(InlineCode, {\n            children: \"displacy.serve\"\n          })\n        }), \" to run the web server, or\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#displacy.render\",\n          children: _jsx(InlineCode, {\n            children: \"displacy.render\"\n          })\n        }), \" to generate the raw markup.\\nIf you want to know how to write rules that hook into some type of syntactic\\nconstruction, just plug the sentence into the visualizer and see how spaCy\\nannotates it.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy import displacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Autonomous cars shift insurance liability toward manufacturers\\\")\\n# Since this is an interactive Jupyter environment, we can use displacy.render here\\ndisplacy.render(doc, style='dep')\\n\"\n        })\n      }), _jsx(Infobox, {\n        children: _jsxs(_components.p, {\n          children: [\"For more details and examples, see the\\n\", _jsx(_components.a, {\n            href: \"/usage/visualizers\",\n            children: \"usage guide on visualizing spaCy\"\n          }), \". You can also test\\ndisplaCy in our \", _jsx(_components.a, {\n            href: \"https://explosion.ai/demos/displacy\",\n            children: \"online demo\"\n          }), \"..\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"disabling\",\n        children: \"Disabling the parser \"\n      }), _jsxs(_components.p, {\n        children: [\"In the \", _jsx(_components.a, {\n          href: \"/models\",\n          children: \"trained pipelines\"\n        }), \" provided by spaCy, the parser is loaded and\\nenabled by default as part of the\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines\",\n          children: \"standard processing pipeline\"\n        }), \". If you donâ€™t need\\nany of the syntactic information, you should disable the parser. Disabling the\\nparser will make spaCy load and run much faster. If you want to load the parser,\\nbut need to disable it for specific documents, you can also control its use on\\nthe \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object. For more details, see the usage guide on\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines/#disabling\",\n          children: \"disabling pipeline components\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"nlp = spacy.load(\\\"en_core_web_sm\\\", disable=[\\\"parser\\\"])\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-named-entities\",\n      children: [_jsx(_components.h2, {\n        id: \"named-entities\",\n        children: \"Named Entity Recognition \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy features an extremely fast statistical entity recognition system, that\\nassigns labels to contiguous spans of tokens. The default\\n\", _jsx(_components.a, {\n          href: \"/models\",\n          children: \"trained pipelines\"\n        }), \" can identify a variety of named and numeric\\nentities, including companies, locations, organizations and products. You can\\nadd arbitrary classes to the entity recognition system, and update the model\\nwith new examples.\"]\n      }), _jsx(_components.h3, {\n        id: \"named-entities-101\",\n        children: \"Named Entity Recognition 101 \"\n      }), _jsx(NER101, {}), _jsx(_components.h3, {\n        id: \"accessing-ner\",\n        children: \"Accessing entity annotations and labels \"\n      }), _jsxs(_components.p, {\n        children: [\"The standard way to access entity annotations is the \", _jsx(_components.a, {\n          href: \"/api/doc#ents\",\n          children: _jsx(InlineCode, {\n            children: \"doc.ents\"\n          })\n        }), \"\\nproperty, which produces a sequence of \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \" objects. The entity\\ntype is accessible either as a hash value or as a string, using the attributes\\n\", _jsx(InlineCode, {\n          children: \"ent.label\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"ent.label_\"\n        }), \". The \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" object acts as a sequence of tokens, so\\nyou can iterate over the entity or index into it. You can also get the text form\\nof the whole entity, as though it were a single token.\"]\n      }), _jsxs(_components.p, {\n        children: [\"You can also access token entity annotations using the\\n\", _jsx(_components.a, {\n          href: \"/api/token#attributes\",\n          children: _jsx(InlineCode, {\n            children: \"token.ent_iob\"\n          })\n        }), \" and\\n\", _jsx(_components.a, {\n          href: \"/api/token#attributes\",\n          children: _jsx(InlineCode, {\n            children: \"token.ent_type\"\n          })\n        }), \" attributes. \", _jsx(InlineCode, {\n          children: \"token.ent_iob\"\n        }), \" indicates\\nwhether an entity starts, continues or ends on the tag. If no entity type is set\\non a token, it will return an empty string.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"IOB Scheme\"\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"I\"\n            }), \" â€“ Token is \", _jsx(_components.strong, {\n              children: \"inside\"\n            }), \" an entity.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"O\"\n            }), \" â€“ Token is \", _jsx(_components.strong, {\n              children: \"outside\"\n            }), \" an entity.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"B\"\n            }), \" â€“ Token is the \", _jsx(_components.strong, {\n              children: \"beginning\"\n            }), \" of an entity.\"]\n          }), \"\\n\"]\n        }), \"\\n\", _jsx(_components.h4, {\n          children: \"BILUO Scheme\"\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"B\"\n            }), \" â€“ Token is the \", _jsx(_components.strong, {\n              children: \"beginning\"\n            }), \" of a multi-token entity.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"I\"\n            }), \" â€“ Token is \", _jsx(_components.strong, {\n              children: \"inside\"\n            }), \" a multi-token entity.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"L\"\n            }), \" â€“ Token is the \", _jsx(_components.strong, {\n              children: \"last\"\n            }), \" token of a multi-token entity.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"U\"\n            }), \" â€“ Token is a single-token \", _jsx(_components.strong, {\n              children: \"unit\"\n            }), \" entity.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"O\"\n            }), \" â€“ Token is \", _jsx(_components.strong, {\n              children: \"outside\"\n            }), \" an entity.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"San Francisco considers banning sidewalk delivery robots\\\")\\n\\n# document level\\nents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\\nprint(ents)\\n\\n# token level\\nent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]\\nent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]\\nprint(ent_san)  # ['San', 'B', 'GPE']\\nprint(ent_francisco)  # ['Francisco', 'I', 'GPE']\\n\"\n        })\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Text\"\n            }), _jsx(_components.th, {\n              children: \"ent_iob\"\n            }), _jsx(_components.th, {\n              children: \"ent_iob_\"\n            }), _jsx(_components.th, {\n              children: \"ent_type_\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"San\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"3\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"B\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"GPE\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"beginning of an entity\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Francisco\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"1\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"I\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"GPE\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"inside an entity\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"considers\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"2\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"O\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"outside an entity\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"banning\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"2\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"O\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"outside an entity\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"sidewalk\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"2\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"O\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"outside an entity\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"delivery\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"2\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"O\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"outside an entity\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"robots\"\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"2\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"O\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: \"outside an entity\"\n            })]\n          })]\n        })]\n      }), _jsx(_components.h3, {\n        id: \"setting-entities\",\n        children: \"Setting entity annotations \"\n      }), _jsxs(_components.p, {\n        children: [\"To ensure that the sequence of token annotations remains consistent, you have to\\nset entity annotations \", _jsx(_components.strong, {\n          children: \"at the document level\"\n        }), \". However, you canâ€™t write\\ndirectly to the \", _jsx(InlineCode, {\n          children: \"token.ent_iob\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"token.ent_type\"\n        }), \" attributes, so the easiest\\nway to set entities is to use the \", _jsx(_components.a, {\n          href: \"/api/doc#set_ents\",\n          children: _jsx(InlineCode, {\n            children: \"doc.set_ents\"\n          })\n        }), \" function\\nand create the new entity as a \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.tokens import Span\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"fb is hiring a new vice president of global policy\\\")\\nents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\\nprint('Before', ents)\\n# The model didn't recognize \\\"fb\\\" as an entity :(\\n\\n# Create a span for the new entity\\nfb_ent = Span(doc, 0, 1, label=\\\"ORG\\\")\\norig_ents = list(doc.ents)\\n\\n# Option 1: Modify the provided entity spans, leaving the rest unmodified\\ndoc.set_ents([fb_ent], default=\\\"unmodified\\\")\\n\\n# Option 2: Assign a complete list of ents to doc.ents\\ndoc.ents = orig_ents + [fb_ent]\\n\\nents = [(e.text, e.start, e.end, e.label_) for e in doc.ents]\\nprint('After', ents)\\n# [('fb', 0, 1, 'ORG')] ðŸŽ‰\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Keep in mind that \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" is initialized with the start and end \", _jsx(_components.strong, {\n          children: \"token\"\n        }), \"\\nindices, not the character offsets. To create a span from character offsets, use\\n\", _jsx(_components.a, {\n          href: \"/api/doc#char_span\",\n          children: _jsx(InlineCode, {\n            children: \"Doc.char_span\"\n          })\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"fb_ent = doc.char_span(0, 2, label=\\\"ORG\\\")\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"setting-from-array\",\n        children: \"Setting entity annotations from array \"\n      }), _jsxs(_components.p, {\n        children: [\"You can also assign entity annotations using the\\n\", _jsx(_components.a, {\n          href: \"/api/doc#from_array\",\n          children: _jsx(InlineCode, {\n            children: \"doc.from_array\"\n          })\n        }), \" method. To do this, you should include\\nboth the \", _jsx(InlineCode, {\n          children: \"ENT_TYPE\"\n        }), \" and the \", _jsx(InlineCode, {\n          children: \"ENT_IOB\"\n        }), \" attributes in the array youâ€™re importing\\nfrom.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import numpy\\nimport spacy\\nfrom spacy.attrs import ENT_IOB, ENT_TYPE\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp.make_doc(\\\"London is a big city in the United Kingdom.\\\")\\nprint(\\\"Before\\\", doc.ents)  # []\\n\\nheader = [ENT_IOB, ENT_TYPE]\\nattr_array = numpy.zeros((len(doc), len(header)), dtype=\\\"uint64\\\")\\nattr_array[0, 0] = 3  # B\\nattr_array[0, 1] = doc.vocab.strings[\\\"GPE\\\"]\\ndoc.from_array(header, attr_array)\\nprint(\\\"After\\\", doc.ents)  # [London]\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"setting-cython\",\n        children: \"Setting entity annotations in Cython \"\n      }), _jsxs(_components.p, {\n        children: [\"Finally, you can always write to the underlying struct if you compile a\\n\", _jsx(_components.a, {\n          href: \"http://cython.org/\",\n          children: \"Cython\"\n        }), \" function. This is easy to do, and allows you to\\nwrite efficient native code.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# cython: infer_types=True\\nfrom spacy.typedefs cimport attr_t\\nfrom spacy.tokens.doc cimport Doc\\n\\ncpdef set_entity(Doc doc, int start, int end, attr_t ent_type):\\n    for i in range(start, end):\\n        doc.c[i].ent_type = ent_type\\n    doc.c[start].ent_iob = 3\\n    for i in range(start+1, end):\\n        doc.c[i].ent_iob = 2\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Obviously, if you write directly to the array of \", _jsx(InlineCode, {\n          children: \"TokenC*\"\n        }), \" structs, youâ€™ll have\\nresponsibility for ensuring that the data is left in a consistent state.\"]\n      }), _jsx(_components.h3, {\n        id: \"entity-types\",\n        children: \"Built-in entity types \"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Tip: Understanding entity types\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"You can also use \", _jsx(InlineCode, {\n            children: \"spacy.explain()\"\n          }), \" to get the description for the string\\nrepresentation of an entity label. For example, \", _jsx(InlineCode, {\n            children: \"spacy.explain(\\\"LANGUAGE\\\")\"\n          }), \"\\nwill return â€œany named languageâ€.\"]\n        }), \"\\n\"]\n      }), _jsx(Infobox, {\n        title: \"Annotation scheme\",\n        children: _jsxs(_components.p, {\n          children: [\"For details on the entity types available in spaCyâ€™s trained pipelines, see the\\nâ€œlabel schemeâ€ sections of the individual models in the\\n\", _jsx(_components.a, {\n            href: \"/models\",\n            children: \"models directory\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"displacy\",\n        children: \"Visualizing named entities \"\n      }), _jsxs(_components.p, {\n        children: [\"The\\n\", _jsxs(_components.a, {\n          href: \"https://explosion.ai/demos/displacy-ent\",\n          children: [\"displaCy \", _jsx(\"sup\", {\n            children: \"ENT\"\n          }), \" visualizer\"]\n        }), \"\\nlets you explore an entity recognition modelâ€™s behavior interactively. If youâ€™re\\ntraining a model, itâ€™s very useful to run the visualization yourself. To help\\nyou do that, spaCy comes with a visualization module. You can pass a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" or a\\nlist of \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" objects to displaCy and run\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#displacy.serve\",\n          children: _jsx(InlineCode, {\n            children: \"displacy.serve\"\n          })\n        }), \" to run the web server, or\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#displacy.render\",\n          children: _jsx(InlineCode, {\n            children: \"displacy.render\"\n          })\n        }), \" to generate the raw markup.\"]\n      }), _jsxs(_components.p, {\n        children: [\"For more details and examples, see the\\n\", _jsx(_components.a, {\n          href: \"/usage/visualizers\",\n          children: \"usage guide on visualizing spaCy\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Named Entity example\",\n          children: \"import spacy\\nfrom spacy import displacy\\n\\ntext = \\\"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\\\"\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(text)\\ndisplacy.serve(doc, style=\\\"ent\\\")\\n\"\n        })\n      }), _jsx(Iframe, {\n        title: \"displaCy visualizer for entities\",\n        src: \"/images/displacy-ent2.html\",\n        height: 180\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-entity-linking\",\n      children: [_jsx(_components.h2, {\n        id: \"entity-linking\",\n        children: \"Entity Linking \"\n      }), _jsxs(_components.p, {\n        children: [\"To ground the named entities into the â€œreal worldâ€, spaCy provides functionality\\nto perform entity linking, which resolves a textual entity to a unique\\nidentifier from a knowledge base (KB). You can create your own\\n\", _jsx(_components.a, {\n          href: \"/api/kb\",\n          children: _jsx(InlineCode, {\n            children: \"KnowledgeBase\"\n          })\n        }), \" and \", _jsx(_components.a, {\n          href: \"/usage/training\",\n          children: \"train\"\n        }), \" a new\\n\", _jsx(_components.a, {\n          href: \"/api/entitylinker\",\n          children: _jsx(InlineCode, {\n            children: \"EntityLinker\"\n          })\n        }), \" using that custom knowledge base.\"]\n      }), _jsx(_components.h3, {\n        id: \"entity-linking-accessing\",\n        model: \"entity linking\",\n        children: \"Accessing entity identifiers \"\n      }), _jsxs(_components.p, {\n        children: [\"The annotated KB identifier is accessible as either a hash value or as a string,\\nusing the attributes \", _jsx(InlineCode, {\n          children: \"ent.kb_id\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"ent.kb_id_\"\n        }), \" of a \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \"\\nobject, or the \", _jsx(InlineCode, {\n          children: \"ent_kb_id\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"ent_kb_id_\"\n        }), \" attributes of a\\n\", _jsx(_components.a, {\n          href: \"/api/token\",\n          children: _jsx(InlineCode, {\n            children: \"Token\"\n          })\n        }), \" object.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"my_custom_el_pipeline\\\")\\ndoc = nlp(\\\"Ada Lovelace was born in London\\\")\\n\\n# Document level\\nents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]\\nprint(ents)  # [('Ada Lovelace', 'PERSON', 'Q7259'), ('London', 'GPE', 'Q84')]\\n\\n# Token level\\nent_ada_0 = [doc[0].text, doc[0].ent_type_, doc[0].ent_kb_id_]\\nent_ada_1 = [doc[1].text, doc[1].ent_type_, doc[1].ent_kb_id_]\\nent_london_5 = [doc[5].text, doc[5].ent_type_, doc[5].ent_kb_id_]\\nprint(ent_ada_0)  # ['Ada', 'PERSON', 'Q7259']\\nprint(ent_ada_1)  # ['Lovelace', 'PERSON', 'Q7259']\\nprint(ent_london_5)  # ['London', 'GPE', 'Q84']\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-tokenization\",\n      children: [_jsx(_components.h2, {\n        id: \"tokenization\",\n        children: \"Tokenization \"\n      }), _jsxs(_components.p, {\n        children: [\"Tokenization is the task of splitting a text into meaningful segments, called\\n\", _jsx(_components.em, {\n          children: \"tokens\"\n        }), \". The input to the tokenizer is a unicode text, and the output is a\\n\", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" object. To construct a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object, you need a\\n\", _jsx(_components.a, {\n          href: \"/api/vocab\",\n          children: _jsx(InlineCode, {\n            children: \"Vocab\"\n          })\n        }), \" instance, a sequence of \", _jsx(InlineCode, {\n          children: \"word\"\n        }), \" strings, and optionally a\\nsequence of \", _jsx(InlineCode, {\n          children: \"spaces\"\n        }), \" booleans, which allow you to maintain alignment of the\\ntokens into the original string.\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"spaCyâ€™s tokenization is \", _jsx(_components.strong, {\n            children: \"non-destructive\"\n          }), \", which means that youâ€™ll always be\\nable to reconstruct the original input from the tokenized output. Whitespace\\ninformation is preserved in the tokens and no information is added or removed\\nduring tokenization. This is kind of a core principle of spaCyâ€™s \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \" object:\\n\", _jsx(InlineCode, {\n            children: \"doc.text == input_text\"\n          }), \" should always hold true.\"]\n        })\n      }), _jsx(Tokenization101, {}), _jsxs(Accordion, {\n        title: \"Algorithm details: How spaCy's tokenizer works\",\n        id: \"how-tokenizer-works\",\n        spaced: true,\n        children: [_jsx(_components.p, {\n          children: \"spaCy introduces a novel tokenization algorithm that gives a better balance\\nbetween performance, ease of definition and ease of alignment into the original\\nstring.\"\n        }), _jsx(_components.p, {\n          children: \"After consuming a prefix or suffix, we consult the special cases again. We want\\nthe special cases to handle things like â€œdonâ€™tâ€ in English, and we want the same\\nrule to work for â€œ(donâ€™t)!â€œ. We do this by splitting off the open bracket, then\\nthe exclamation, then the closed bracket, and finally matching the special case.\\nHereâ€™s an implementation of the algorithm in Python optimized for readability\\nrather than performance:\"\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"def tokenizer_pseudo_code(\\n    text,\\n    special_cases,\\n    prefix_search,\\n    suffix_search,\\n    infix_finditer,\\n    token_match,\\n    url_match\\n):\\n    tokens = []\\n    for substring in text.split():\\n        suffixes = []\\n        while substring:\\n            if substring in special_cases:\\n                tokens.extend(special_cases[substring])\\n                substring = \\\"\\\"\\n                continue\\n            while prefix_search(substring) or suffix_search(substring):\\n                if token_match(substring):\\n                    tokens.append(substring)\\n                    substring = \\\"\\\"\\n                    break\\n                if substring in special_cases:\\n                    tokens.extend(special_cases[substring])\\n                    substring = \\\"\\\"\\n                    break\\n                if prefix_search(substring):\\n                    split = prefix_search(substring).end()\\n                    tokens.append(substring[:split])\\n                    substring = substring[split:]\\n                    if substring in special_cases:\\n                        continue\\n                if suffix_search(substring):\\n                    split = suffix_search(substring).start()\\n                    suffixes.append(substring[split:])\\n                    substring = substring[:split]\\n            if token_match(substring):\\n                tokens.append(substring)\\n                substring = \\\"\\\"\\n            elif url_match(substring):\\n                tokens.append(substring)\\n                substring = \\\"\\\"\\n            elif substring in special_cases:\\n                tokens.extend(special_cases[substring])\\n                substring = \\\"\\\"\\n            elif list(infix_finditer(substring)):\\n                infixes = infix_finditer(substring)\\n                offset = 0\\n                for match in infixes:\\n                    if offset == 0 and match.start() == 0:\\n                        continue\\n                    tokens.append(substring[offset : match.start()])\\n                    tokens.append(substring[match.start() : match.end()])\\n                    offset = match.end()\\n                if substring[offset:]:\\n                    tokens.append(substring[offset:])\\n                substring = \\\"\\\"\\n            elif substring:\\n                tokens.append(substring)\\n                substring = \\\"\\\"\\n        tokens.extend(reversed(suffixes))\\n    for match in matcher(special_cases, text):\\n        tokens.replace(match, special_cases[match])\\n    return tokens\\n\"\n          })\n        }), _jsx(_components.p, {\n          children: \"The algorithm can be summarized as follows:\"\n        }), _jsxs(_components.ol, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"Iterate over space-separated substrings.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Check whether we have an explicitly defined special case for this substring.\\nIf we do, use it.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Look for a token match. If there is a match, stop processing and keep this\\ntoken.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Check whether we have an explicitly defined special case for this substring.\\nIf we do, use it.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Otherwise, try to consume one prefix. If we consumed a prefix, go back to #3,\\nso that the token match and special cases always get priority.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"If we didnâ€™t consume a prefix, try to consume a suffix and then go back to\\n#3.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"If we canâ€™t consume a prefix or a suffix, look for a URL match.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"If thereâ€™s no URL match, then look for a special case.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Look for â€œinfixesâ€ â€“ stuff like hyphens etc. and split the substring into\\ntokens on all infixes.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Once we canâ€™t consume any more of the string, handle it as a single token.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Make a final pass over the text to check for special cases that include\\nspaces or that were missed due to the incremental processing of affixes.\"\n          }), \"\\n\"]\n        })]\n      }), _jsxs(_components.p, {\n        children: [_jsx(_components.strong, {\n          children: \"Global\"\n        }), \" and \", _jsx(_components.strong, {\n          children: \"language-specific\"\n        }), \" tokenizer data is supplied via the language\\ndata in \", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spaCy/tree/master/spacy/lang\",\n          children: _jsx(InlineCode, {\n            children: \"spacy/lang\"\n          })\n        }), \". The tokenizer exceptions\\ndefine special cases like â€œdonâ€™tâ€ in English, which needs to be split into two\\ntokens: \", _jsx(InlineCode, {\n          children: \"{ORTH: \\\"do\\\"}\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"{ORTH: \\\"n't\\\", NORM: \\\"not\\\"}\"\n        }), \". The prefixes, suffixes\\nand infixes mostly define punctuation rules â€“ for example, when to split off\\nperiods (at the end of a sentence), and when to leave tokens containing periods\\nintact (abbreviations like â€œU.S.â€).\"]\n      }), _jsx(Accordion, {\n        title: \"Should I change the language data or add custom tokenizer rules?\",\n        id: \"lang-data-vs-tokenizer\",\n        children: _jsxs(_components.p, {\n          children: [\"Tokenization rules that are specific to one language, but can be \", _jsx(_components.strong, {\n            children: \"generalized\\nacross that language\"\n          }), \", should ideally live in the language data in\\n\", _jsx(_components.a, {\n            href: \"https://github.com/explosion/spaCy/tree/master/spacy/lang\",\n            children: _jsx(InlineCode, {\n              children: \"spacy/lang\"\n            })\n          }), \" â€“Â we always appreciate pull requests!\\nAnything thatâ€™s specific to a domain or text type â€“ like financial trading\\nabbreviations or Bavarian youth slang â€“ should be added as a special case rule\\nto your tokenizer instance. If youâ€™re dealing with a lot of customizations, it\\nmight make sense to create an entirely custom subclass.\"]\n        })\n      }), _jsx(_components.hr, {}), _jsx(_components.h3, {\n        id: \"special-cases\",\n        children: \"Adding special case tokenization rules \"\n      }), _jsxs(_components.p, {\n        children: [\"Most domains have at least some idiosyncrasies that require custom tokenization\\nrules. This could be very certain expressions, or abbreviations only used in\\nthis specific field. Hereâ€™s how to add a special case rule to an existing\\n\", _jsx(_components.a, {\n          href: \"/api/tokenizer\",\n          children: _jsx(InlineCode, {\n            children: \"Tokenizer\"\n          })\n        }), \" instance:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.symbols import ORTH\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"gimme that\\\")  # phrase to tokenize\\nprint([w.text for w in doc])  # ['gimme', 'that']\\n\\n# Add special case rule\\nspecial_case = [{ORTH: \\\"gim\\\"}, {ORTH: \\\"me\\\"}]\\nnlp.tokenizer.add_special_case(\\\"gimme\\\", special_case)\\n\\n# Check new tokenization\\nprint([w.text for w in nlp(\\\"gimme that\\\")])  # ['gim', 'me', 'that']\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"The special case doesnâ€™t have to match an entire whitespace-delimited substring.\\nThe tokenizer will incrementally split off punctuation, and keep looking up the\\nremaining substring. The special case rules also have precedence over the\\npunctuation splitting.\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"assert \\\"gimme\\\" not in [w.text for w in nlp(\\\"gimme!\\\")]\\nassert \\\"gimme\\\" not in [w.text for w in nlp('(\\\"...gimme...?\\\")')]\\n\\nnlp.tokenizer.add_special_case(\\\"...gimme...?\\\", [{\\\"ORTH\\\": \\\"...gimme...?\\\"}])\\nassert len(nlp(\\\"...gimme...?\\\")) == 1\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"tokenizer-debug\",\n        version: \"2.2.3\",\n        children: \"Debugging the tokenizer \"\n      }), _jsxs(_components.p, {\n        children: [\"A working implementation of the pseudo-code above is available for debugging as\\n\", _jsx(_components.a, {\n          href: \"/api/tokenizer#explain\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.tokenizer.explain(text)\"\n          })\n        }), \". It returns a list of\\ntuples showing which tokenizer rule or pattern was matched for each token. The\\ntokens produced are identical to \", _jsx(InlineCode, {\n          children: \"nlp.tokenizer()\"\n        }), \" except for whitespace tokens:\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Expected output\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            lang: \"none\",\n            children: \"\\\"      PREFIX\\nLet    SPECIAL-1\\n's     SPECIAL-2\\ngo     TOKEN\\n!      SUFFIX\\n\\\"      SUFFIX\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\n\\nnlp = English()\\ntext = '''\\\"Let's go!\\\"'''\\ndoc = nlp(text)\\ntok_exp = nlp.tokenizer.explain(text)\\nassert [t.text for t in doc if not t.is_space] == [t[1] for t in tok_exp]\\nfor t in tok_exp:\\n    print(t[1], \\\"\\\\\\\\t\\\", t[0])\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"native-tokenizers\",\n        children: \"Customizing spaCyâ€™s Tokenizer class \"\n      }), _jsx(_components.p, {\n        children: \"Letâ€™s imagine you wanted to create a tokenizer for a new language or specific\\ndomain. There are six things you may need to define:\"\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"A dictionary of \", _jsx(_components.strong, {\n            children: \"special cases\"\n          }), \". This handles things like contractions,\\nunits of measurement, emoticons, certain abbreviations, etc.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"A function \", _jsx(InlineCode, {\n            children: \"prefix_search\"\n          }), \", to handle \", _jsx(_components.strong, {\n            children: \"preceding punctuation\"\n          }), \", such as open\\nquotes, open brackets, etc.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"A function \", _jsx(InlineCode, {\n            children: \"suffix_search\"\n          }), \", to handle \", _jsx(_components.strong, {\n            children: \"succeeding punctuation\"\n          }), \", such as\\ncommas, periods, close quotes, etc.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"A function \", _jsx(InlineCode, {\n            children: \"infix_finditer\"\n          }), \", to handle non-whitespace separators, such as\\nhyphens etc.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"An optional boolean function \", _jsx(InlineCode, {\n            children: \"token_match\"\n          }), \" matching strings that should never\\nbe split, overriding the infix rules. Useful for things like numbers.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"An optional boolean function \", _jsx(InlineCode, {\n            children: \"url_match\"\n          }), \", which is similar to \", _jsx(InlineCode, {\n            children: \"token_match\"\n          }), \"\\nexcept that prefixes and suffixes are removed before applying the match.\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"You shouldnâ€™t usually need to create a \", _jsx(InlineCode, {\n          children: \"Tokenizer\"\n        }), \" subclass. Standard usage is\\nto use \", _jsx(InlineCode, {\n          children: \"re.compile()\"\n        }), \" to build a regular expression object, and pass its\\n\", _jsx(InlineCode, {\n          children: \".search()\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \".finditer()\"\n        }), \" methods:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import re\\nimport spacy\\nfrom spacy.tokenizer import Tokenizer\\n\\nspecial_cases = {\\\":)\\\": [{\\\"ORTH\\\": \\\":)\\\"}]}\\nprefix_re = re.compile(r'''^[\\\\\\\\[\\\\\\\\(\\\"']''')\\nsuffix_re = re.compile(r'''[\\\\\\\\]\\\\\\\\)\\\"']$''')\\ninfix_re = re.compile(r'''[-~]''')\\nsimple_url_re = re.compile(r'''^https?://''')\\n\\ndef custom_tokenizer(nlp):\\n    return Tokenizer(nlp.vocab, rules=special_cases,\\n                                prefix_search=prefix_re.search,\\n                                suffix_search=suffix_re.search,\\n                                infix_finditer=infix_re.finditer,\\n                                url_match=simple_url_re.match)\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nnlp.tokenizer = custom_tokenizer(nlp)\\ndoc = nlp(\\\"hello-world. :)\\\")\\nprint([t.text for t in doc]) # ['hello', '-', 'world.', ':)']\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"If you need to subclass the tokenizer instead, the relevant methods to\\nspecialize are \", _jsx(InlineCode, {\n          children: \"find_prefix\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"find_suffix\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"find_infix\"\n        }), \".\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"When customizing the prefix, suffix and infix handling, remember that youâ€™re\\npassing in \", _jsx(_components.strong, {\n            children: \"functions\"\n          }), \" for spaCy to execute, e.g. \", _jsx(InlineCode, {\n            children: \"prefix_re.search\"\n          }), \" â€“ not\\njust the regular expressions. This means that your functions also need to define\\nhow the rules should be applied. For example, if youâ€™re adding your own prefix\\nrules, you need to make sure theyâ€™re only applied to characters at the\\n\", _jsx(_components.strong, {\n            children: \"beginning of a token\"\n          }), \", e.g. by adding \", _jsx(InlineCode, {\n            children: \"^\"\n          }), \". Similarly, suffix rules should\\nonly be applied at the \", _jsx(_components.strong, {\n            children: \"end of a token\"\n          }), \", so your expression should end with a\\n\", _jsx(InlineCode, {\n            children: \"$\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h4, {\n        id: \"native-tokenizer-additions\",\n        children: \"Modifying existing rule sets \"\n      }), _jsxs(_components.p, {\n        children: [\"In many situations, you donâ€™t necessarily need entirely custom rules. Sometimes\\nyou just want to add another character to the prefixes, suffixes or infixes. The\\ndefault prefix, suffix and infix rules are available via the \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" objectâ€™s\\n\", _jsx(InlineCode, {\n          children: \"Defaults\"\n        }), \" and the \", _jsx(InlineCode, {\n          children: \"Tokenizer\"\n        }), \" attributes such as\\n\", _jsx(_components.a, {\n          href: \"/api/tokenizer#attributes\",\n          children: _jsx(InlineCode, {\n            children: \"Tokenizer.suffix_search\"\n          })\n        }), \" are writable, so you can\\noverwrite them with compiled regular expression objects using modified default\\nrules. spaCy ships with utility functions to help you compile the regular\\nexpressions â€“ for example,\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#util.compile_suffix_regex\",\n          children: _jsx(InlineCode, {\n            children: \"compile_suffix_regex\"\n          })\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"suffixes = nlp.Defaults.suffixes + [r'''-+$''',]\\nsuffix_regex = spacy.util.compile_suffix_regex(suffixes)\\nnlp.tokenizer.suffix_search = suffix_regex.search\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"Similarly, you can remove a character from the default suffixes:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"suffixes = list(nlp.Defaults.suffixes)\\nsuffixes.remove(\\\"\\\\\\\\\\\\\\\\[\\\")\\nsuffix_regex = spacy.util.compile_suffix_regex(suffixes)\\nnlp.tokenizer.suffix_search = suffix_regex.search\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"Tokenizer.suffix_search\"\n        }), \" attribute should be a function which takes a\\nunicode string and returns a \", _jsx(_components.strong, {\n          children: \"regex match object\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"None\"\n        }), \". Usually we use\\nthe \", _jsx(InlineCode, {\n          children: \".search\"\n        }), \" attribute of a compiled regex object, but you can use some other\\nfunction that behaves the same way.\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"If youâ€™ve loaded a trained pipeline, writing to the\\n\", _jsx(_components.a, {\n            href: \"/api/language#defaults\",\n            children: _jsx(InlineCode, {\n              children: \"nlp.Defaults\"\n            })\n          }), \" or \", _jsx(InlineCode, {\n            children: \"English.Defaults\"\n          }), \" directly wonâ€™t\\nwork, since the regular expressions are read from the pipeline data and will be\\ncompiled when you load it. If you modify \", _jsx(InlineCode, {\n            children: \"nlp.Defaults\"\n          }), \", youâ€™ll only see the\\neffect if you call \", _jsx(_components.a, {\n            href: \"/api/top-level#spacy.blank\",\n            children: _jsx(InlineCode, {\n              children: \"spacy.blank\"\n            })\n          }), \". If you want to\\nmodify the tokenizer loaded from a trained pipeline, you should modify\\n\", _jsx(InlineCode, {\n            children: \"nlp.tokenizer\"\n          }), \" directly. If youâ€™re training your own pipeline, you can register\\n\", _jsx(_components.a, {\n            href: \"/usage/training/#custom-code-nlp-callbacks\",\n            children: \"callbacks\"\n          }), \" to modify the \", _jsx(InlineCode, {\n            children: \"nlp\"\n          }), \"\\nobject before training.\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The prefix, infix and suffix rule sets include not only individual characters\\nbut also detailed regular expressions that take the surrounding context into\\naccount. For example, there is a regular expression that treats a hyphen between\\nletters as an infix. If you do not want the tokenizer to split on hyphens\\nbetween letters, you can modify the existing infix definition from\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spaCy/tree/master/spacy/lang/punctuation.py\",\n          children: _jsx(InlineCode, {\n            children: \"lang/punctuation.py\"\n          })\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\\nfrom spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\\nfrom spacy.util import compile_infix_regex\\n\\n# Default tokenizer\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"mother-in-law\\\")\\nprint([t.text for t in doc]) # ['mother', '-', 'in', '-', 'law']\\n\\n# Modify tokenizer infix patterns\\ninfixes = (\\n    LIST_ELLIPSES\\n    + LIST_ICONS\\n    + [\\n        r\\\"(?<=[0-9])[+\\\\\\\\-\\\\\\\\*^](?=[0-9-])\\\",\\n        r\\\"(?<=[{al}{q}])\\\\\\\\.(?=[{au}{q}])\\\".format(\\n            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\\n        ),\\n        r\\\"(?<=[{a}]),(?=[{a}])\\\".format(a=ALPHA),\\n        # âœ… Commented out regex that splits on hyphens between letters:\\n        # r\\\"(?<=[{a}])(?:{h})(?=[{a}])\\\".format(a=ALPHA, h=HYPHENS),\\n        r\\\"(?<=[{a}0-9])[:<>=/](?=[{a}])\\\".format(a=ALPHA),\\n    ]\\n)\\n\\ninfix_re = compile_infix_regex(infixes)\\nnlp.tokenizer.infix_finditer = infix_re.finditer\\ndoc = nlp(\\\"mother-in-law\\\")\\nprint([t.text for t in doc]) # ['mother-in-law']\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"For an overview of the default regular expressions, see\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spaCy/tree/master/spacy/lang/punctuation.py\",\n          children: _jsx(InlineCode, {\n            children: \"lang/punctuation.py\"\n          })\n        }), \" and\\nlanguage-specific definitions such as\\n\", _jsx(_components.a, {\n          href: \"https://github.com/explosion/spaCy/tree/master/spacy/lang/de/punctuation.py\",\n          children: _jsx(InlineCode, {\n            children: \"lang/de/punctuation.py\"\n          })\n        }), \" for\\nGerman.\"]\n      }), _jsx(_components.h3, {\n        id: \"custom-tokenizer\",\n        children: \"Hooking a custom tokenizer into the pipeline \"\n      }), _jsxs(_components.p, {\n        children: [\"The tokenizer is the first component of the processing pipeline and the only one\\nthat canâ€™t be replaced by writing to \", _jsx(InlineCode, {\n          children: \"nlp.pipeline\"\n        }), \". This is because it has a\\ndifferent signature from all the other components: it takes a text and returns a\\n\", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \", whereas all other components expect to already receive a\\ntokenized \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \".\"]\n      }), _jsx(_components.img, {\n        src: \"/images/pipeline.svg\",\n        alt: \"The processing pipeline\"\n      }), _jsxs(_components.p, {\n        children: [\"To overwrite the existing tokenizer, you need to replace \", _jsx(InlineCode, {\n          children: \"nlp.tokenizer\"\n        }), \" with a\\ncustom function that takes a text and returns a \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Creating a Doc\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"Constructing a \", _jsx(_components.a, {\n            href: \"/api/doc\",\n            children: _jsx(InlineCode, {\n              children: \"Doc\"\n            })\n          }), \" object manually requires at least two\\narguments: the shared \", _jsx(InlineCode, {\n            children: \"Vocab\"\n          }), \" and a list of words. Optionally, you can pass in\\na list of \", _jsx(InlineCode, {\n            children: \"spaces\"\n          }), \" values indicating whether the token at this position is\\nfollowed by a space (default \", _jsx(InlineCode, {\n            children: \"True\"\n          }), \"). See the section on\\n\", _jsx(_components.a, {\n            href: \"#own-annotations\",\n            children: \"pre-tokenized text\"\n          }), \" for more info.\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"words = [\\\"Let\\\", \\\"'s\\\", \\\"go\\\", \\\"!\\\"]\\nspaces = [False, True, False, False]\\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"nlp = spacy.blank(\\\"en\\\")\\nnlp.tokenizer = my_tokenizer\\n\"\n        })\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Argument\"\n            }), _jsx(_components.th, {\n              children: \"Type\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"text\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"str\"\n              })\n            }), _jsx(_components.td, {\n              children: \"The raw text to tokenize.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.strong, {\n                children: \"RETURNS\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"/api/doc\",\n                children: _jsx(InlineCode, {\n                  children: \"Doc\"\n                })\n              })\n            }), _jsx(_components.td, {\n              children: \"The tokenized document.\"\n            })]\n          })]\n        })]\n      }), _jsx(_components.h4, {\n        id: \"custom-tokenizer-example\",\n        children: \"Example 1: Basic whitespace tokenizer \"\n      }), _jsxs(_components.p, {\n        children: [\"Hereâ€™s an example of the most basic whitespace tokenizer. It takes the shared\\nvocab, so it can construct \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" objects. When itâ€™s called on a text, it returns\\na \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object consisting of the text split on single space characters. We can\\nthen overwrite the \", _jsx(InlineCode, {\n          children: \"nlp.tokenizer\"\n        }), \" attribute with an instance of our custom\\ntokenizer.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.tokens import Doc\\n\\nclass WhitespaceTokenizer:\\n    def __init__(self, vocab):\\n        self.vocab = vocab\\n\\n    def __call__(self, text):\\n        words = text.split(\\\" \\\")\\n        spaces = [True] * len(words)\\n        # Avoid zero-length tokens\\n        for i, word in enumerate(words):\\n            if word == \\\"\\\":\\n                words[i] = \\\" \\\"\\n                spaces[i] = False\\n        # Remove the final trailing space\\n        if words[-1] == \\\" \\\":\\n            words = words[0:-1]\\n            spaces = spaces[0:-1]\\n        else:\\n           spaces[-1] = False\\n\\n        return Doc(self.vocab, words=words, spaces=spaces)\\n\\nnlp = spacy.blank(\\\"en\\\")\\nnlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\\ndoc = nlp(\\\"What's happened to me? he thought. It wasn't a dream.\\\")\\nprint([token.text for token in doc])\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"custom-tokenizer-example2\",\n        children: \"Example 2: Third-party tokenizers (BERT word pieces) \"\n      }), _jsxs(_components.p, {\n        children: [\"You can use the same approach to plug in any other third-party tokenizers. Your\\ncustom callable just needs to return a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object with the tokens produced by\\nyour tokenizer. In this example, the wrapper uses the \", _jsx(_components.strong, {\n          children: \"BERT word piece\\ntokenizer\"\n        }), \", provided by the\\n\", _jsx(_components.a, {\n          href: \"https://github.com/huggingface/tokenizers\",\n          children: _jsx(InlineCode, {\n            children: \"tokenizers\"\n          })\n        }), \" library. The tokens\\navailable in the \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object returned by spaCy now match the exact word pieces\\nproduced by the tokenizer.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"ðŸ’¡ Tip: spacy-transformers\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"If youâ€™re working with transformer models like BERT, check out the\\n\", _jsx(_components.a, {\n            href: \"https://github.com/explosion/spacy-transformers\",\n            children: _jsx(InlineCode, {\n              children: \"spacy-transformers\"\n            })\n          }), \"\\nextension package and \", _jsx(_components.a, {\n            href: \"/usage/embeddings-transformers\",\n            children: \"documentation\"\n          }), \". It\\nincludes a pipeline component for using pretrained transformer weights and\\n\", _jsx(_components.strong, {\n            children: \"training transformer models\"\n          }), \" in spaCy, as well as helpful utilities for\\naligning word pieces to linguistic tokenization.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Custom BERT word piece tokenizer\",\n          children: \"from tokenizers import BertWordPieceTokenizer\\nfrom spacy.tokens import Doc\\nimport spacy\\n\\nclass BertTokenizer:\\n    def __init__(self, vocab, vocab_file, lowercase=True):\\n        self.vocab = vocab\\n        self._tokenizer = BertWordPieceTokenizer(vocab_file, lowercase=lowercase)\\n\\n    def __call__(self, text):\\n        tokens = self._tokenizer.encode(text)\\n        words = []\\n        spaces = []\\n        for i, (text, (start, end)) in enumerate(zip(tokens.tokens, tokens.offsets)):\\n            words.append(text)\\n            if i < len(tokens.tokens) - 1:\\n                # If next start != current end we assume a space in between\\n                next_start, next_end = tokens.offsets[i + 1]\\n                spaces.append(next_start > end)\\n            else:\\n                spaces.append(True)\\n        return Doc(self.vocab, words=words, spaces=spaces)\\n\\nnlp = spacy.blank(\\\"en\\\")\\nnlp.tokenizer = BertTokenizer(nlp.vocab, \\\"bert-base-uncased-vocab.txt\\\")\\ndoc = nlp(\\\"Justin Drew Bieber is a Canadian singer, songwriter, and actor.\\\")\\nprint(doc.text, [token.text for token in doc])\\n# [CLS]justin drew bi##eber is a canadian singer, songwriter, and actor.[SEP]\\n# ['[CLS]', 'justin', 'drew', 'bi', '##eber', 'is', 'a', 'canadian', 'singer',\\n#  ',', 'songwriter', ',', 'and', 'actor', '.', '[SEP]']\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Important note on tokenization and models\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Keep in mind that your modelsâ€™ results may be less accurate if the tokenization\\nduring training differs from the tokenization at runtime. So if you modify a\\ntrained pipelineâ€™s tokenization afterwards, it may produce very different\\npredictions. You should therefore train your pipeline with the \", _jsx(_components.strong, {\n            children: \"same\\ntokenizer\"\n          }), \" it will be using at runtime. See the docs on\\n\", _jsx(_components.a, {\n            href: \"#custom-tokenizer-training\",\n            children: \"training with custom tokenization\"\n          }), \" for details.\"]\n        })\n      }), _jsx(_components.h4, {\n        id: \"custom-tokenizer-training\",\n        version: \"3\",\n        children: \"Training with custom tokenization \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCyâ€™s \", _jsx(_components.a, {\n          href: \"/usage/training#config\",\n          children: \"training config\"\n        }), \" describes the settings,\\nhyperparameters, pipeline and tokenizer used for constructing and training the\\npipeline. The \", _jsx(InlineCode, {\n          children: \"[nlp.tokenizer]\"\n        }), \" block refers to a \", _jsx(_components.strong, {\n          children: \"registered function\"\n        }), \" that\\ntakes the \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object and returns a tokenizer. Here, weâ€™re registering a\\nfunction called \", _jsx(InlineCode, {\n          children: \"whitespace_tokenizer\"\n        }), \" in the\\n\", _jsxs(_components.a, {\n          href: \"/api/top-level#registry\",\n          children: [_jsx(InlineCode, {\n            children: \"@tokenizers\"\n          }), \" registry\"]\n        }), \". To make sure spaCy knows how\\nto construct your tokenizer during training, you can pass in your Python file by\\nsetting \", _jsx(InlineCode, {\n          children: \"--code functions.py\"\n        }), \" when you run \", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[nlp.tokenizer]\\n@tokenizers = \\\"whitespace_tokenizer\\\"\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"functions.py\",\n          highlight: \"1\",\n          children: \"@spacy.registry.tokenizers(\\\"whitespace_tokenizer\\\")\\ndef create_whitespace_tokenizer():\\n    def create_tokenizer(nlp):\\n        return WhitespaceTokenizer(nlp.vocab)\\n\\n    return create_tokenizer\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Registered functions can also take arguments that are then passed in from the\\nconfig. This allows you to quickly change and keep track of different settings.\\nHere, the registered function called \", _jsx(InlineCode, {\n          children: \"bert_word_piece_tokenizer\"\n        }), \" takes two\\narguments: the path to a vocabulary file and whether to lowercase the text. The\\nPython type hints \", _jsx(InlineCode, {\n          children: \"str\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"bool\"\n        }), \" ensure that the received values have the\\ncorrect type.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"config.cfg\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[nlp.tokenizer]\\n@tokenizers = \\\"bert_word_piece_tokenizer\\\"\\nvocab_file = \\\"bert-base-uncased-vocab.txt\\\"\\nlowercase = true\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"functions.py\",\n          highlight: \"1\",\n          children: \"@spacy.registry.tokenizers(\\\"bert_word_piece_tokenizer\\\")\\ndef create_whitespace_tokenizer(vocab_file: str, lowercase: bool):\\n    def create_tokenizer(nlp):\\n        return BertWordPieceTokenizer(nlp.vocab, vocab_file, lowercase)\\n\\n    return create_tokenizer\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"To avoid hard-coding local paths into your config file, you can also set the\\nvocab path on the CLI by using the \", _jsx(InlineCode, {\n          children: \"--nlp.tokenizer.vocab_file\"\n        }), \"\\n\", _jsx(_components.a, {\n          href: \"/usage/training#config-overrides\",\n          children: \"override\"\n        }), \" when you run\\n\", _jsx(_components.a, {\n          href: \"/api/cli#train\",\n          children: _jsx(InlineCode, {\n            children: \"spacy train\"\n          })\n        }), \". For more details on using registered functions,\\nsee the docs in \", _jsx(_components.a, {\n          href: \"/usage/training#custom-code\",\n          children: \"training with custom code\"\n        }), \".\"]\n      }), _jsx(Infobox, {\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Remember that a registered function should always be a function that spaCy\\n\", _jsx(_components.strong, {\n            children: \"calls to create something\"\n          }), \", not the â€œsomethingâ€ itself. In this case, it\\n\", _jsx(_components.strong, {\n            children: \"creates a function\"\n          }), \" that takes the \", _jsx(InlineCode, {\n            children: \"nlp\"\n          }), \" object and returns a callable that\\ntakes a text and returns a \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h4, {\n        id: \"own-annotations\",\n        children: \"Using pre-tokenized text \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy generally assumes by default that your data is \", _jsx(_components.strong, {\n          children: \"raw text\"\n        }), \". However,\\nsometimes your data is partially annotated, e.g. with pre-existing tokenization,\\npart-of-speech tags, etc. The most common situation is that you have\\n\", _jsx(_components.strong, {\n          children: \"pre-defined tokenization\"\n        }), \". If you have a list of strings, you can create a\\n\", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" object directly. Optionally, you can also specify a list of\\nboolean values, indicating whether each word is followed by a space.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"âœï¸ Things to try\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"Change a boolean value in the list of \", _jsx(InlineCode, {\n              children: \"spaces\"\n            }), \". You should see it reflected\\nin the \", _jsx(InlineCode, {\n              children: \"doc.text\"\n            }), \" and whether the token is followed by a space.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Remove \", _jsx(InlineCode, {\n              children: \"spaces=spaces\"\n            }), \" from the \", _jsx(InlineCode, {\n              children: \"Doc\"\n            }), \". You should see that every token is\\nnow followed by a space.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Copy-paste a random sentence from the internet and manually construct a\\n\", _jsx(InlineCode, {\n              children: \"Doc\"\n            }), \" with \", _jsx(InlineCode, {\n              children: \"words\"\n            }), \" and \", _jsx(InlineCode, {\n              children: \"spaces\"\n            }), \" so that the \", _jsx(InlineCode, {\n              children: \"doc.text\"\n            }), \" matches the original\\ninput text.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.tokens import Doc\\n\\nnlp = spacy.blank(\\\"en\\\")\\nwords = [\\\"Hello\\\", \\\",\\\", \\\"world\\\", \\\"!\\\"]\\nspaces = [False, True, False, False]\\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\\nprint(doc.text)\\nprint([(t.text, t.text_with_ws, t.whitespace_) for t in doc])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"If provided, the spaces list must be the \", _jsx(_components.strong, {\n          children: \"same length\"\n        }), \" as the words list. The\\nspaces list affects the \", _jsx(InlineCode, {\n          children: \"doc.text\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"span.text\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"token.idx\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"span.start_char\"\n        }), \"\\nand \", _jsx(InlineCode, {\n          children: \"span.end_char\"\n        }), \" attributes. If you donâ€™t provide a \", _jsx(InlineCode, {\n          children: \"spaces\"\n        }), \" sequence, spaCy\\nwill assume that all words are followed by a space. Once you have a\\n\", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" object, you can write to its attributes to set the\\npart-of-speech tags, syntactic dependencies, named entities and other\\nattributes.\"]\n      }), _jsx(_components.h4, {\n        id: \"aligning-tokenization\",\n        children: \"Aligning tokenization \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCyâ€™s tokenization is non-destructive and uses language-specific rules\\noptimized for compatibility with treebank annotations. Other tools and resources\\ncan sometimes tokenize things differently â€“ for example, \", _jsx(InlineCode, {\n          children: \"\\\"I'm\\\"\"\n        }), \" â†’\\n\", _jsx(InlineCode, {\n          children: \"[\\\"I\\\", \\\"'\\\", \\\"m\\\"]\"\n        }), \" instead of \", _jsx(InlineCode, {\n          children: \"[\\\"I\\\", \\\"'m\\\"]\"\n        }), \".\"]\n      }), _jsxs(_components.p, {\n        children: [\"In situations like that, you often want to align the tokenization so that you\\ncan merge annotations from different sources together, or take vectors predicted\\nby a\\n\", _jsx(_components.a, {\n          href: \"https://github.com/huggingface/pytorch-transformers\",\n          children: \"pretrained BERT model\"\n        }), \" and\\napply them to spaCy tokens. spaCyâ€™s \", _jsx(_components.a, {\n          href: \"/api/example#alignment-object\",\n          children: _jsx(InlineCode, {\n            children: \"Alignment\"\n          })\n        }), \"\\nobject allows the one-to-one mappings of token indices in both directions as\\nwell as taking into account indices where multiple tokens align to one single\\ntoken.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"âœï¸ Things to try\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"Change the capitalization in one of the token lists â€“ for example,\\n\", _jsx(InlineCode, {\n              children: \"\\\"obama\\\"\"\n            }), \" to \", _jsx(InlineCode, {\n              children: \"\\\"Obama\\\"\"\n            }), \". Youâ€™ll see that the alignment is case-insensitive.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Change \", _jsx(InlineCode, {\n              children: \"\\\"podcasts\\\"\"\n            }), \" in \", _jsx(InlineCode, {\n              children: \"other_tokens\"\n            }), \" to \", _jsx(InlineCode, {\n              children: \"\\\"pod\\\", \\\"casts\\\"\"\n            }), \". You should see\\nthat there are now two tokens of length 2 in \", _jsx(InlineCode, {\n              children: \"y2x\"\n            }), \", one corresponding to\\nâ€œâ€˜sâ€, and one to â€œpodcastsâ€.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Make \", _jsx(InlineCode, {\n              children: \"other_tokens\"\n            }), \" and \", _jsx(InlineCode, {\n              children: \"spacy_tokens\"\n            }), \" identical. Youâ€™ll see that all\\ntokens now correspond 1-to-1.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.training import Alignment\\n\\nother_tokens = [\\\"i\\\", \\\"listened\\\", \\\"to\\\", \\\"obama\\\", \\\"'\\\", \\\"s\\\", \\\"podcasts\\\", \\\".\\\"]\\nspacy_tokens = [\\\"i\\\", \\\"listened\\\", \\\"to\\\", \\\"obama\\\", \\\"'s\\\", \\\"podcasts\\\", \\\".\\\"]\\nalign = Alignment.from_strings(other_tokens, spacy_tokens)\\nprint(f\\\"a -> b, lengths: {align.x2y.lengths}\\\")  # array([1, 1, 1, 1, 1, 1, 1, 1])\\nprint(f\\\"a -> b, mapping: {align.x2y.data}\\\")  # array([0, 1, 2, 3, 4, 4, 5, 6]) : two tokens both refer to \\\"'s\\\"\\nprint(f\\\"b -> a, lengths: {align.y2x.lengths}\\\")  # array([1, 1, 1, 1, 2, 1, 1])   : the token \\\"'s\\\" refers to two tokens\\nprint(f\\\"b -> a, mappings: {align.y2x.data}\\\")  # array([0, 1, 2, 3, 4, 5, 6, 7])\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"Here are some insights from the alignment information generated in the example\\nabove:\"\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"The one-to-one mappings for the first four tokens are identical, which means\\nthey map to each other. This makes sense because theyâ€™re also identical in the\\ninput: \", _jsx(InlineCode, {\n            children: \"\\\"i\\\"\"\n          }), \", \", _jsx(InlineCode, {\n            children: \"\\\"listened\\\"\"\n          }), \", \", _jsx(InlineCode, {\n            children: \"\\\"to\\\"\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"\\\"obama\\\"\"\n          }), \".\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"The value of \", _jsx(InlineCode, {\n            children: \"x2y.data[6]\"\n          }), \" is \", _jsx(InlineCode, {\n            children: \"5\"\n          }), \", which means that \", _jsx(InlineCode, {\n            children: \"other_tokens[6]\"\n          }), \"\\n(\", _jsx(InlineCode, {\n            children: \"\\\"podcasts\\\"\"\n          }), \") aligns to \", _jsx(InlineCode, {\n            children: \"spacy_tokens[5]\"\n          }), \" (also \", _jsx(InlineCode, {\n            children: \"\\\"podcasts\\\"\"\n          }), \").\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(InlineCode, {\n            children: \"x2y.data[4]\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"x2y.data[5]\"\n          }), \" are both \", _jsx(InlineCode, {\n            children: \"4\"\n          }), \", which means that both tokens 4\\nand 5 of \", _jsx(InlineCode, {\n            children: \"other_tokens\"\n          }), \" (\", _jsx(InlineCode, {\n            children: \"\\\"'\\\"\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"\\\"s\\\"\"\n          }), \") align to token 4 of \", _jsx(InlineCode, {\n            children: \"spacy_tokens\"\n          }), \"\\n(\", _jsx(InlineCode, {\n            children: \"\\\"'s\\\"\"\n          }), \").\"]\n        }), \"\\n\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"The current implementation of the alignment algorithm assumes that both\\ntokenizations add up to the same string. For example, youâ€™ll be able to align\\n\", _jsx(InlineCode, {\n            children: \"[\\\"I\\\", \\\"'\\\", \\\"m\\\"]\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"[\\\"I\\\", \\\"'m\\\"]\"\n          }), \", which both add up to \", _jsx(InlineCode, {\n            children: \"\\\"I'm\\\"\"\n          }), \", but not\\n\", _jsx(InlineCode, {\n            children: \"[\\\"I\\\", \\\"'m\\\"]\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"[\\\"I\\\", \\\"am\\\"]\"\n          }), \".\"]\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-retokenization\",\n      children: [_jsx(_components.h2, {\n        id: \"retokenization\",\n        version: \"2.1\",\n        children: \"Merging and splitting \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/doc#retokenize\",\n          children: _jsx(InlineCode, {\n            children: \"Doc.retokenize\"\n          })\n        }), \" context manager lets you merge and\\nsplit tokens. Modifications to the tokenization are stored and performed all at\\nonce when the context manager exits. To merge several tokens into one single\\ntoken, pass a \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" to \", _jsx(_components.a, {\n          href: \"/api/doc#retokenizer.merge\",\n          children: _jsx(InlineCode, {\n            children: \"retokenizer.merge\"\n          })\n        }), \". An\\noptional dictionary of \", _jsx(InlineCode, {\n          children: \"attrs\"\n        }), \" lets you set attributes that will be assigned to\\nthe merged token â€“ for example, the lemma, part-of-speech tag or entity type. By\\ndefault, the merged token will receive the same attributes as the merged spanâ€™s\\nroot.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"âœï¸ Things to try\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"Inspect the \", _jsx(InlineCode, {\n              children: \"token.lemma_\"\n            }), \" attribute with and without setting the \", _jsx(InlineCode, {\n              children: \"attrs\"\n            }), \".\\nYouâ€™ll see that the lemma defaults to â€œNewâ€, the lemma of the spanâ€™s root.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Overwrite other attributes like the \", _jsx(InlineCode, {\n              children: \"\\\"ENT_TYPE\\\"\"\n            }), \". Since â€œNew Yorkâ€ is also\\nrecognized as a named entity, this change will also be reflected in the\\n\", _jsx(InlineCode, {\n              children: \"doc.ents\"\n            }), \".\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"I live in New York\\\")\\nprint(\\\"Before:\\\", [token.text for token in doc])\\n\\nwith doc.retokenize() as retokenizer:\\n    retokenizer.merge(doc[3:5], attrs={\\\"LEMMA\\\": \\\"new york\\\"})\\nprint(\\\"After:\\\", [token.text for token in doc])\\n\"\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Tip: merging entities and noun phrases\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"If you need to merge named entities or noun chunks, check out the built-in\\n\", _jsx(_components.a, {\n            href: \"/api/pipeline-functions#merge_entities\",\n            children: _jsx(InlineCode, {\n              children: \"merge_entities\"\n            })\n          }), \" and\\n\", _jsx(_components.a, {\n            href: \"/api/pipeline-functions#merge_noun_chunks\",\n            children: _jsx(InlineCode, {\n              children: \"merge_noun_chunks\"\n            })\n          }), \" pipeline\\ncomponents. When added to your pipeline using \", _jsx(InlineCode, {\n            children: \"nlp.add_pipe\"\n          }), \", theyâ€™ll take\\ncare of merging the spans automatically.\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"If an attribute in the \", _jsx(InlineCode, {\n          children: \"attrs\"\n        }), \" is a context-dependent token attribute, it will\\nbe applied to the underlying \", _jsx(_components.a, {\n          href: \"/api/token\",\n          children: _jsx(InlineCode, {\n            children: \"Token\"\n          })\n        }), \". For example \", _jsx(InlineCode, {\n          children: \"LEMMA\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"POS\"\n        }), \"\\nor \", _jsx(InlineCode, {\n          children: \"DEP\"\n        }), \" only apply to a word in context, so theyâ€™re token attributes. If an\\nattribute is a context-independent lexical attribute, it will be applied to the\\nunderlying \", _jsx(_components.a, {\n          href: \"/api/lexeme\",\n          children: _jsx(InlineCode, {\n            children: \"Lexeme\"\n          })\n        }), \", the entry in the vocabulary. For example,\\n\", _jsx(InlineCode, {\n          children: \"LOWER\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"IS_STOP\"\n        }), \" apply to all words of the same spelling, regardless of the\\ncontext.\"]\n      }), _jsxs(Infobox, {\n        variant: \"warning\",\n        title: \"Note on merging overlapping spans\",\n        children: [_jsxs(_components.p, {\n          children: [\"If youâ€™re trying to merge spans that overlap, spaCy will raise an error because\\nitâ€™s unclear how the result should look. Depending on the application, you may\\nwant to match the shortest or longest possible span, so itâ€™s up to you to filter\\nthem. If youâ€™re looking for the longest non-overlapping span, you can use the\\n\", _jsx(_components.a, {\n            href: \"/api/top-level#util.filter_spans\",\n            children: _jsx(InlineCode, {\n              children: \"util.filter_spans\"\n            })\n          }), \" helper:\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"doc = nlp(\\\"I live in Berlin Kreuzberg\\\")\\nspans = [doc[3:5], doc[3:4], doc[4:5]]\\nfiltered_spans = filter_spans(spans)\\n\"\n          })\n        })]\n      }), _jsx(_components.h3, {\n        children: \"Splitting tokens\"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/doc#retokenizer.split\",\n          children: _jsx(InlineCode, {\n            children: \"retokenizer.split\"\n          })\n        }), \" method allows splitting\\none token into two or more tokens. This can be useful for cases where\\ntokenization rules alone arenâ€™t sufficient. For example, you might want to split\\nâ€œitsâ€ into the tokens â€œitâ€ and â€œisâ€ â€“ but not the possessive pronoun â€œitsâ€. You\\ncan write rule-based logic that can find only the correct â€œitsâ€ to split, but by\\nthat time, the \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" will already be tokenized.\"]\n      }), _jsxs(_components.p, {\n        children: [\"This process of splitting a token requires more settings, because you need to\\nspecify the text of the individual tokens, optional per-token attributes and how\\nthe tokens should be attached to the existing syntax tree. This can be done by\\nsupplying a list of \", _jsx(InlineCode, {\n          children: \"heads\"\n        }), \" â€“ either the token to attach the newly split token\\nto, or a \", _jsx(InlineCode, {\n          children: \"(token, subtoken)\"\n        }), \" tuple if the newly split token should be attached\\nto another subtoken. In this case, â€œNewâ€ should be attached to â€œYorkâ€ (the\\nsecond split subtoken) and â€œYorkâ€ should be attached to â€œinâ€.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"âœï¸ Things to try\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"Assign different attributes to the subtokens and compare the result.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"Change the heads so that â€œNewâ€ is attached to â€œinâ€ and â€œYorkâ€ is attached\\nto â€œNewâ€.\"\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Split the token into three tokens instead of two â€“ for example,\\n\", _jsx(InlineCode, {\n              children: \"[\\\"New\\\", \\\"Yo\\\", \\\"rk\\\"]\"\n            }), \".\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy import displacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"I live in NewYork\\\")\\nprint(\\\"Before:\\\", [token.text for token in doc])\\ndisplacy.render(doc)  # displacy.serve if you're not in a Jupyter environment\\n\\nwith doc.retokenize() as retokenizer:\\n    heads = [(doc[3], 1), doc[2]]\\n    attrs = {\\\"POS\\\": [\\\"PROPN\\\", \\\"PROPN\\\"], \\\"DEP\\\": [\\\"pobj\\\", \\\"compound\\\"]}\\n    retokenizer.split(doc[3], [\\\"New\\\", \\\"York\\\"], heads=heads, attrs=attrs)\\nprint(\\\"After:\\\", [token.text for token in doc])\\ndisplacy.render(doc)  # displacy.serve if you're not in a Jupyter environment\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Specifying the heads as a list of \", _jsx(InlineCode, {\n          children: \"token\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"(token, subtoken)\"\n        }), \" tuples allows\\nattaching split subtokens to other subtokens, without having to keep track of\\nthe token indices after splitting.\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Token\"\n            }), _jsx(_components.th, {\n              children: \"Head\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"New\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"(doc[3], 1)\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attach this token to the second subtoken (index \", _jsx(InlineCode, {\n                children: \"1\"\n              }), \") that \", _jsx(InlineCode, {\n                children: \"doc[3]\"\n              }), \" will be split into, i.e. â€œYorkâ€.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"\\\"York\\\"\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"doc[2]\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attach this token to \", _jsx(InlineCode, {\n                children: \"doc[1]\"\n              }), \" in the original \", _jsx(InlineCode, {\n                children: \"Doc\"\n              }), \", i.e. â€œinâ€.\"]\n            })]\n          })]\n        })]\n      }), _jsx(_components.p, {\n        children: \"If you donâ€™t care about the heads (for example, if youâ€™re only running the\\ntokenizer and not the parser), you can attach each subtoken to itself:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          highlight: \"3\",\n          children: \"doc = nlp(\\\"I live in NewYorkCity\\\")\\nwith doc.retokenize() as retokenizer:\\n    heads = [(doc[3], 0), (doc[3], 1), (doc[3], 2)]\\n    retokenizer.split(doc[3], [\\\"New\\\", \\\"York\\\", \\\"City\\\"], heads=heads)\\n\"\n        })\n      }), _jsxs(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: [_jsxs(_components.p, {\n          children: [\"When splitting tokens, the subtoken texts always have to match the original\\ntoken text â€“Â or, put differently \", _jsx(InlineCode, {\n            children: \"\\\"\\\".join(subtokens) == token.text\"\n          }), \" always needs\\nto hold true. If this wasnâ€™t the case, splitting tokens could easily end up\\nproducing confusing and unexpected results that would contradict spaCyâ€™s\\nnon-destructive tokenization policy.\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-diff\",\n            lang: \"diff\",\n            children: \"doc = nlp(\\\"I live in L.A.\\\")\\nwith doc.retokenize() as retokenizer:\\n-    retokenizer.split(doc[3], [\\\"Los\\\", \\\"Angeles\\\"], heads=[(doc[3], 1), doc[2]])\\n+    retokenizer.split(doc[3], [\\\"L.\\\", \\\"A.\\\"], heads=[(doc[3], 1), doc[2]])\\n\"\n          })\n        })]\n      }), _jsx(_components.h3, {\n        id: \"retokenization-extensions\",\n        children: \"Overwriting custom extension attributes \"\n      }), _jsxs(_components.p, {\n        children: [\"If youâ€™ve registered custom\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components-attributes\",\n          children: \"extension attributes\"\n        }), \",\\nyou can overwrite them during tokenization by providing a dictionary of\\nattribute names mapped to new values as the \", _jsx(InlineCode, {\n          children: \"\\\"_\\\"\"\n        }), \" key in the \", _jsx(InlineCode, {\n          children: \"attrs\"\n        }), \". For\\nmerging, you need to provide one dictionary of attributes for the resulting\\nmerged token. For splitting, you need to provide a list of dictionaries with\\ncustom attributes, one per split subtoken.\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"To set extension attributes during retokenization, the attributes need to be\\n\", _jsx(_components.strong, {\n            children: \"registered\"\n          }), \" using the \", _jsx(_components.a, {\n            href: \"/api/token#set_extension\",\n            children: _jsx(InlineCode, {\n              children: \"Token.set_extension\"\n            })\n          }), \"\\nmethod and they need to be \", _jsx(_components.strong, {\n            children: \"writable\"\n          }), \". This means that they should either have\\na default value that can be overwritten, or a getter \", _jsx(_components.em, {\n            children: \"and\"\n          }), \" setter. Method\\nextensions or extensions with only a getter are computed dynamically, so their\\nvalues canâ€™t be overwritten. For more details, see the\\n\", _jsx(_components.a, {\n            href: \"/usage/processing-pipelines/#custom-components-attributes\",\n            children: \"extension attribute docs\"\n          }), \".\"]\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"âœï¸ Things to try\"\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"Add another custom extension â€“ maybe \", _jsx(InlineCode, {\n              children: \"\\\"music_style\\\"\"\n            }), \"? â€“ and overwrite it.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Change the extension attribute to use only a \", _jsx(InlineCode, {\n              children: \"getter\"\n            }), \" function. You should\\nsee that spaCy raises an error, because the attribute is not writable\\nanymore.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Rewrite the code to split a token with \", _jsx(InlineCode, {\n              children: \"retokenizer.split\"\n            }), \". Remember that\\nyou need to provide a list of extension attribute values as the \", _jsx(InlineCode, {\n              children: \"\\\"_\\\"\"\n            }), \"\\nproperty, one for each split subtoken.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.tokens import Token\\n\\n# Register a custom token attribute, token._.is_musician\\nToken.set_extension(\\\"is_musician\\\", default=False)\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"I like David Bowie\\\")\\nprint(\\\"Before:\\\", [(token.text, token._.is_musician) for token in doc])\\n\\nwith doc.retokenize() as retokenizer:\\n    retokenizer.merge(doc[2:4], attrs={\\\"_\\\": {\\\"is_musician\\\": True}})\\nprint(\\\"After:\\\", [(token.text, token._.is_musician) for token in doc])\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-sbd\",\n      children: [_jsx(_components.h2, {\n        id: \"sbd\",\n        children: \"Sentence Segmentation \"\n      }), _jsxs(_components.p, {\n        children: [\"A \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" objectâ€™s sentences are available via the \", _jsx(InlineCode, {\n          children: \"Doc.sents\"\n        }), \"\\nproperty. To view a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \"â€™s sentences, you can iterate over the \", _jsx(InlineCode, {\n          children: \"Doc.sents\"\n        }), \", a\\ngenerator that yields \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \" objects. You can check whether a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \"\\nhas sentence boundaries by calling\\n\", _jsx(_components.a, {\n          href: \"/api/doc#has_annotation\",\n          children: _jsx(InlineCode, {\n            children: \"Doc.has_annotation\"\n          })\n        }), \" with the attribute name\\n\", _jsx(InlineCode, {\n          children: \"\\\"SENT_START\\\"\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"This is a sentence. This is another sentence.\\\")\\nassert doc.has_annotation(\\\"SENT_START\\\")\\nfor sent in doc.sents:\\n    print(sent.text)\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"spaCy provides four alternatives for sentence segmentation:\"\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.a, {\n            href: \"#sbd-parser\",\n            children: \"Dependency parser\"\n          }), \": the statistical\\n\", _jsx(_components.a, {\n            href: \"/api/dependencyparser\",\n            children: _jsx(InlineCode, {\n              children: \"DependencyParser\"\n            })\n          }), \" provides the most accurate\\nsentence boundaries based on full dependency parses.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.a, {\n            href: \"#sbd-senter\",\n            children: \"Statistical sentence segmenter\"\n          }), \": the statistical\\n\", _jsx(_components.a, {\n            href: \"/api/sentencerecognizer\",\n            children: _jsx(InlineCode, {\n              children: \"SentenceRecognizer\"\n            })\n          }), \" is a simpler and faster\\nalternative to the parser that only sets sentence boundaries.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.a, {\n            href: \"#sbd-component\",\n            children: \"Rule-based pipeline component\"\n          }), \": the rule-based\\n\", _jsx(_components.a, {\n            href: \"/api/sentencizer\",\n            children: _jsx(InlineCode, {\n              children: \"Sentencizer\"\n            })\n          }), \" sets sentence boundaries using a\\ncustomizable list of sentence-final punctuation.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.a, {\n            href: \"#sbd-custom\",\n            children: \"Custom function\"\n          }), \": your own custom function added to the\\nprocessing pipeline can set sentence boundaries by writing to\\n\", _jsx(InlineCode, {\n            children: \"Token.is_sent_start\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), _jsx(_components.h3, {\n        id: \"sbd-parser\",\n        model: \"parser\",\n        children: \"Default: Using the dependency parse \"\n      }), _jsxs(_components.p, {\n        children: [\"Unlike other libraries, spaCy uses the dependency parse to determine sentence\\nboundaries. This is usually the most accurate approach, but it requires a\\n\", _jsx(_components.strong, {\n          children: \"trained pipeline\"\n        }), \" that provides accurate predictions. If your texts are\\ncloser to general-purpose news or web text, this should work well out-of-the-box\\nwith spaCyâ€™s provided trained pipelines. For social media or conversational text\\nthat doesnâ€™t follow the same rules, your application may benefit from a custom\\ntrained or rule-based component.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"This is a sentence. This is another sentence.\\\")\\nfor sent in doc.sents:\\n    print(sent.text)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"spaCyâ€™s dependency parser respects already set boundaries, so you can preprocess\\nyour \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" using custom components \", _jsx(_components.em, {\n          children: \"before\"\n        }), \" itâ€™s parsed. Depending on your text,\\nthis may also improve parse accuracy, since the parser is constrained to predict\\nparses consistent with the sentence boundaries.\"]\n      }), _jsx(_components.h3, {\n        id: \"sbd-senter\",\n        model: \"senter\",\n        version: \"3\",\n        children: \"Statistical sentence segmenter \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/sentencerecognizer\",\n          children: _jsx(InlineCode, {\n            children: \"SentenceRecognizer\"\n          })\n        }), \" is a simple statistical\\ncomponent that only provides sentence boundaries. Along with being faster and\\nsmaller than the parser, its primary advantage is that itâ€™s easier to train\\nbecause it only requires annotated sentence boundaries rather than full\\ndependency parses. spaCyâ€™s \", _jsx(_components.a, {\n          href: \"/models\",\n          children: \"trained pipelines\"\n        }), \" include both a parser\\nand a trained sentence segmenter, which is\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#disabling\",\n          children: \"disabled\"\n        }), \" by default. If you only need\\nsentence boundaries and no parser, you can use the \", _jsx(InlineCode, {\n          children: \"exclude\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"disable\"\n        }), \"\\nargument on \", _jsx(_components.a, {\n          href: \"/api/top-level#spacy.load\",\n          children: _jsx(InlineCode, {\n            children: \"spacy.load\"\n          })\n        }), \" to load the pipeline\\nwithout the parser and then enable the sentence recognizer explicitly with\\n\", _jsx(_components.a, {\n          href: \"/api/language#enable_pipe\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.enable_pipe\"\n          })\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"senter vs. parser\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"The recall for the \", _jsx(InlineCode, {\n            children: \"senter\"\n          }), \" is typically slightly lower than for the parser,\\nwhich is better at predicting sentence boundaries when punctuation is not\\npresent.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\", exclude=[\\\"parser\\\"])\\nnlp.enable_pipe(\\\"senter\\\")\\ndoc = nlp(\\\"This is a sentence. This is another sentence.\\\")\\nfor sent in doc.sents:\\n    print(sent.text)\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"sbd-component\",\n        children: \"Rule-based pipeline component \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/sentencizer\",\n          children: _jsx(InlineCode, {\n            children: \"Sentencizer\"\n          })\n        }), \" component is a\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines\",\n          children: \"pipeline component\"\n        }), \" that splits sentences on\\npunctuation like \", _jsx(InlineCode, {\n          children: \".\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"!\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"?\"\n        }), \". You can plug it into your pipeline if you only\\nneed sentence boundaries without dependency parses.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.lang.en import English\\n\\nnlp = English()  # just the language with no pipeline\\nnlp.add_pipe(\\\"sentencizer\\\")\\ndoc = nlp(\\\"This is a sentence. This is another sentence.\\\")\\nfor sent in doc.sents:\\n    print(sent.text)\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"sbd-custom\",\n        children: \"Custom rule-based strategy \"\n      }), _jsxs(_components.p, {\n        children: [\"If you want to implement your own strategy that differs from the default\\nrule-based approach of splitting on sentences, you can also create a\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components\",\n          children: \"custom pipeline component\"\n        }), \" that\\ntakes a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object and sets the \", _jsx(InlineCode, {\n          children: \"Token.is_sent_start\"\n        }), \" attribute on each\\nindividual token. If set to \", _jsx(InlineCode, {\n          children: \"False\"\n        }), \", the token is explicitly marked as \", _jsx(_components.em, {\n          children: \"not\"\n        }), \" the\\nstart of a sentence. If set to \", _jsx(InlineCode, {\n          children: \"None\"\n        }), \" (default), itâ€™s treated as a missing value\\nand can still be overwritten by the parser.\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"To prevent inconsistent state, you can only set boundaries \", _jsx(_components.strong, {\n            children: \"before\"\n          }), \" a document\\nis parsed (and \", _jsx(InlineCode, {\n            children: \"doc.has_annotation(\\\"DEP\\\")\"\n          }), \" is \", _jsx(InlineCode, {\n            children: \"False\"\n          }), \"). To ensure that your\\ncomponent is added in the right place, you can set \", _jsx(InlineCode, {\n            children: \"before='parser'\"\n          }), \" or\\n\", _jsx(InlineCode, {\n            children: \"first=True\"\n          }), \" when adding it to the pipeline using\\n\", _jsx(_components.a, {\n            href: \"/api/language#add_pipe\",\n            children: _jsx(InlineCode, {\n              children: \"nlp.add_pipe\"\n            })\n          }), \".\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Hereâ€™s an example of a component that implements a pre-processing rule for\\nsplitting on \", _jsx(InlineCode, {\n          children: \"\\\"...\\\"\"\n        }), \" tokens. The component is added before the parser, which is\\nthen used to further segment the text. Thatâ€™s possible, because \", _jsx(InlineCode, {\n          children: \"is_sent_start\"\n        }), \"\\nis only set to \", _jsx(InlineCode, {\n          children: \"True\"\n        }), \" for some of the tokens â€“ all others still specify \", _jsx(InlineCode, {\n          children: \"None\"\n        }), \"\\nfor unset sentence boundaries. This approach can be useful if you want to\\nimplement \", _jsx(_components.strong, {\n          children: \"additional\"\n        }), \" rules specific to your data, while still being able to\\ntake advantage of dependency-based sentence segmentation.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.language import Language\\nimport spacy\\n\\ntext = \\\"this is a sentence...hello...and another sentence.\\\"\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(text)\\nprint(\\\"Before:\\\", [sent.text for sent in doc.sents])\\n\\n@Language.component(\\\"set_custom_boundaries\\\")\\ndef set_custom_boundaries(doc):\\n    for token in doc[:-1]:\\n        if token.text == \\\"...\\\":\\n            doc[token.i + 1].is_sent_start = True\\n    return doc\\n\\nnlp.add_pipe(\\\"set_custom_boundaries\\\", before=\\\"parser\\\")\\ndoc = nlp(text)\\nprint(\\\"After:\\\", [sent.text for sent in doc.sents])\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-mappings-exceptions\",\n      children: [_jsx(_components.h2, {\n        id: \"mappings-exceptions\",\n        version: \"3\",\n        children: \"Mappings & Exceptions \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/attributeruler\",\n          children: _jsx(InlineCode, {\n            children: \"AttributeRuler\"\n          })\n        }), \" manages \", _jsx(_components.strong, {\n          children: \"rule-based mappings and\\nexceptions\"\n        }), \" for all token-level attributes. As the number of\\n\", _jsx(_components.a, {\n          href: \"/api/#architecture-pipeline\",\n          children: \"pipeline components\"\n        }), \" has grown from spaCy v2 to\\nv3, handling rules and exceptions in each component individually has become\\nimpractical, so the \", _jsx(InlineCode, {\n          children: \"AttributeRuler\"\n        }), \" provides a single component with a unified\\npattern format for all token attribute mappings and exceptions.\"]\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"AttributeRuler\"\n        }), \" uses\\n\", _jsxs(_components.a, {\n          href: \"/usage/rule-based-matching#adding-patterns\",\n          children: [_jsx(InlineCode, {\n            children: \"Matcher\"\n          }), \" patterns\"]\n        }), \" to identify\\ntokens and then assigns them the provided attributes. If needed, the\\n\", _jsx(_components.a, {\n          href: \"/api/matcher\",\n          children: _jsx(InlineCode, {\n            children: \"Matcher\"\n          })\n        }), \" patterns can include context around the target token.\\nFor example, the attribute ruler can:\"]\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"provide exceptions for any \", _jsx(_components.strong, {\n            children: \"token attributes\"\n          })]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"map \", _jsx(_components.strong, {\n            children: \"fine-grained tags\"\n          }), \" to \", _jsx(_components.strong, {\n            children: \"coarse-grained tags\"\n          }), \" for languages without\\nstatistical morphologizers (replacing the v2.x \", _jsx(InlineCode, {\n            children: \"tag_map\"\n          }), \" in the\\n\", _jsx(_components.a, {\n            href: \"#language-data\",\n            children: \"language data\"\n          }), \")\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"map token \", _jsx(_components.strong, {\n            children: \"surface form + fine-grained tags\"\n          }), \" to \", _jsx(_components.strong, {\n            children: \"morphological features\"\n          }), \"\\n(replacing the v2.x \", _jsx(InlineCode, {\n            children: \"morph_rules\"\n          }), \" in the \", _jsx(_components.a, {\n            href: \"#language-data\",\n            children: \"language data\"\n          }), \")\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"specify the \", _jsx(_components.strong, {\n            children: \"tags for space tokens\"\n          }), \" (replacing hard-coded behavior in the\\ntagger)\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"The following example shows how the tag and POS \", _jsx(InlineCode, {\n          children: \"NNP\"\n        }), \"/\", _jsx(InlineCode, {\n          children: \"PROPN\"\n        }), \" can be specified\\nfor the phrase \", _jsx(InlineCode, {\n          children: \"\\\"The Who\\\"\"\n        }), \", overriding the tags provided by the statistical\\ntagger and the POS tag map.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ntext = \\\"I saw The Who perform. Who did you see?\\\"\\ndoc1 = nlp(text)\\nprint(doc1[2].tag_, doc1[2].pos_)  # DT DET\\nprint(doc1[3].tag_, doc1[3].pos_)  # WP PRON\\n\\n# Add attribute ruler with exception for \\\"The Who\\\" as NNP/PROPN NNP/PROPN\\nruler = nlp.get_pipe(\\\"attribute_ruler\\\")\\n# Pattern to match \\\"The Who\\\"\\npatterns = [[{\\\"LOWER\\\": \\\"the\\\"}, {\\\"TEXT\\\": \\\"Who\\\"}]]\\n# The attributes to assign to the matched token\\nattrs = {\\\"TAG\\\": \\\"NNP\\\", \\\"POS\\\": \\\"PROPN\\\"}\\n# Add rules to the attribute ruler\\nruler.add(patterns=patterns, attrs=attrs, index=0)  # \\\"The\\\" in \\\"The Who\\\"\\nruler.add(patterns=patterns, attrs=attrs, index=1)  # \\\"Who\\\" in \\\"The Who\\\"\\n\\ndoc2 = nlp(text)\\nprint(doc2[2].tag_, doc2[2].pos_)  # NNP PROPN\\nprint(doc2[3].tag_, doc2[3].pos_)  # NNP PROPN\\n# The second \\\"Who\\\" remains unmodified\\nprint(doc2[5].tag_, doc2[5].pos_)  # WP PRON\\n\"\n        })\n      }), _jsx(Infobox, {\n        variant: \"warning\",\n        title: \"Migrating from spaCy v2.x\",\n        children: _jsxs(_components.p, {\n          children: [\"The \", _jsx(_components.a, {\n            href: \"/api/attributeruler\",\n            children: _jsx(InlineCode, {\n              children: \"AttributeRuler\"\n            })\n          }), \" can import a \", _jsx(_components.strong, {\n            children: \"tag map and morph\\nrules\"\n          }), \" in the v2.x format via its built-in methods or when the component is\\ninitialized before training. See the\\n\", _jsx(_components.a, {\n            href: \"/usage/v3#migrating-training-mappings-exceptions\",\n            children: \"migration guide\"\n          }), \" for details.\"]\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-vectors-similarity\",\n      children: [_jsx(_components.h2, {\n        id: \"vectors-similarity\",\n        children: \"Word vectors and semantic similarity \"\n      }), _jsx(Vectors101, {}), _jsx(_components.h3, {\n        id: \"adding-vectors\",\n        children: \"Adding word vectors \"\n      }), _jsxs(_components.p, {\n        children: [\"Custom word vectors can be trained using a number of open-source libraries, such\\nas \", _jsx(_components.a, {\n          href: \"https://radimrehurek.com/gensim\",\n          children: \"Gensim\"\n        }), \", \", _jsx(_components.a, {\n          href: \"https://fasttext.cc\",\n          children: \"FastText\"\n        }), \",\\nor Tomas Mikolovâ€™s original\\n\", _jsx(_components.a, {\n          href: \"https://code.google.com/archive/p/word2vec/\",\n          children: \"Word2vec implementation\"\n        }), \". Most\\nword vector libraries output an easy-to-read text-based format, where each line\\nconsists of the word followed by its vector. For everyday use, we want to\\nconvert the vectors into a binary format that loads faster and takes up less\\nspace on disk. The easiest way to do this is the\\n\", _jsx(_components.a, {\n          href: \"/api/cli#init-vectors\",\n          children: _jsx(InlineCode, {\n            children: \"init vectors\"\n          })\n        }), \" command-line utility. This will output a\\nblank spaCy pipeline in the directory \", _jsx(InlineCode, {\n          children: \"/tmp/la_vectors_wiki_lg\"\n        }), \", giving you\\naccess to some nice Latin vectors. You can then pass the directory path to\\n\", _jsx(_components.a, {\n          href: \"/api/top-level#spacy.load\",\n          children: _jsx(InlineCode, {\n            children: \"spacy.load\"\n          })\n        }), \" or use it in the\\n\", _jsx(_components.a, {\n          href: \"/api/data-formats#config-initialize\",\n          children: _jsx(InlineCode, {\n            children: \"[initialize]\"\n          })\n        }), \" of your config when you\\n\", _jsx(_components.a, {\n          href: \"/usage/training\",\n          children: \"train\"\n        }), \" a model.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Usage example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"nlp_latin = spacy.load(\\\"/tmp/la_vectors_wiki_lg\\\")\\ndoc1 = nlp_latin(\\\"Caecilius est in horto\\\")\\ndoc2 = nlp_latin(\\\"servus est in atrio\\\")\\ndoc1.similarity(doc2)\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-bash\",\n          lang: \"bash\",\n          children: \"$ wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.la.300.vec.gz\\n$ python -m spacy init vectors en cc.la.300.vec.gz /tmp/la_vectors_wiki_lg\\n\"\n        })\n      }), _jsxs(Accordion, {\n        title: \"How to optimize vector coverage\",\n        id: \"custom-vectors-coverage\",\n        spaced: true,\n        children: [_jsxs(_components.p, {\n          children: [\"To help you strike a good balance between coverage and memory usage, spaCyâ€™s\\n\", _jsx(_components.a, {\n            href: \"/api/vectors\",\n            children: _jsx(InlineCode, {\n              children: \"Vectors\"\n            })\n          }), \" class lets you map \", _jsx(_components.strong, {\n            children: \"multiple keys\"\n          }), \" to the \", _jsx(_components.strong, {\n            children: \"same\\nrow\"\n          }), \" of the table. If youâ€™re using the\\n\", _jsx(_components.a, {\n            href: \"/api/cli#init-vectors\",\n            children: _jsx(InlineCode, {\n              children: \"spacy init vectors\"\n            })\n          }), \" command to create a vocabulary,\\npruning the vectors will be taken care of automatically if you set the \", _jsx(InlineCode, {\n            children: \"--prune\"\n          }), \"\\nflag. You can also do it manually in the following steps:\"]\n        }), _jsxs(_components.ol, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"Start with a \", _jsx(_components.strong, {\n              children: \"word vectors package\"\n            }), \" that covers a huge vocabulary. For\\ninstance, the \", _jsx(_components.a, {\n              href: \"/models/en#en_core_web_lg\",\n              children: _jsx(InlineCode, {\n                children: \"en_core_web_lg\"\n              })\n            }), \" package provides\\n300-dimensional GloVe vectors for 685k terms of English.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"If your vocabulary has values set for the \", _jsx(InlineCode, {\n              children: \"Lexeme.prob\"\n            }), \" attribute, the\\nlexemes will be sorted by descending probability to determine which vectors\\nto prune. Otherwise, lexemes will be sorted by their order in the \", _jsx(InlineCode, {\n              children: \"Vocab\"\n            }), \".\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"Call \", _jsx(_components.a, {\n              href: \"/api/vocab#prune_vectors\",\n              children: _jsx(InlineCode, {\n                children: \"Vocab.prune_vectors\"\n              })\n            }), \" with the number of\\nvectors you want to keep.\"]\n          }), \"\\n\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"nlp = spacy.load(\\\"en_core_web_lg\\\")\\nn_vectors = 105000  # number of vectors to keep\\nremoved_words = nlp.vocab.prune_vectors(n_vectors)\\n\\nassert len(nlp.vocab.vectors) <= n_vectors  # unique vectors have been pruned\\nassert nlp.vocab.vectors.n_keys > n_vectors  # but not the total entries\\n\"\n          })\n        }), _jsxs(_components.p, {\n          children: [_jsx(_components.a, {\n            href: \"/api/vocab#prune_vectors\",\n            children: _jsx(InlineCode, {\n              children: \"Vocab.prune_vectors\"\n            })\n          }), \" reduces the current vector\\ntable to a given number of unique entries, and returns a dictionary containing\\nthe removed words, mapped to \", _jsx(InlineCode, {\n            children: \"(string, score)\"\n          }), \" tuples, where \", _jsx(InlineCode, {\n            children: \"string\"\n          }), \" is the\\nentry the removed word was mapped to and \", _jsx(InlineCode, {\n            children: \"score\"\n          }), \" the similarity score between\\nthe two words.\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            title: \"Removed words\",\n            children: \"{\\n    \\\"Shore\\\": (\\\"coast\\\", 0.732257),\\n    \\\"Precautionary\\\": (\\\"caution\\\", 0.490973),\\n    \\\"hopelessness\\\": (\\\"sadness\\\", 0.742366),\\n    \\\"Continous\\\": (\\\"continuous\\\", 0.732549),\\n    \\\"Disemboweled\\\": (\\\"corpse\\\", 0.499432),\\n    \\\"biostatistician\\\": (\\\"scientist\\\", 0.339724),\\n    \\\"somewheres\\\": (\\\"somewheres\\\", 0.402736),\\n    \\\"observing\\\": (\\\"observe\\\", 0.823096),\\n    \\\"Leaving\\\": (\\\"leaving\\\", 1.0),\\n}\\n\"\n          })\n        }), _jsxs(_components.p, {\n          children: [\"In the example above, the vector for â€œShoreâ€ was removed and remapped to the\\nvector of â€œcoastâ€, which is deemed about 73% similar. â€œLeavingâ€ was remapped to\\nthe vector of â€œleavingâ€, which is identical. If youâ€™re using the\\n\", _jsx(_components.a, {\n            href: \"/api/cli#init-vectors\",\n            children: _jsx(InlineCode, {\n              children: \"init vectors\"\n            })\n          }), \" command, you can set the \", _jsx(InlineCode, {\n            children: \"--prune\"\n          }), \"\\noption to easily reduce the size of the vectors as you add them to a spaCy\\npipeline:\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-bash\",\n            lang: \"bash\",\n            children: \"$ python -m spacy init vectors en la.300d.vec.tgz /tmp/la_vectors_web_md --prune 10000\\n\"\n          })\n        }), _jsx(_components.p, {\n          children: \"This will create a blank spaCy pipeline with vectors for the first 10,000 words\\nin the vectors. All other words in the vectors are mapped to the closest vector\\namong those retained.\"\n        })]\n      }), _jsx(_components.h3, {\n        id: \"adding-individual-vectors\",\n        children: \"Adding vectors individually \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"vector\"\n        }), \" attribute is a \", _jsx(_components.strong, {\n          children: \"read-only\"\n        }), \" numpy or cupy array (depending on\\nwhether youâ€™ve configured spaCy to use GPU memory), with dtype \", _jsx(InlineCode, {\n          children: \"float32\"\n        }), \". The\\narray is read-only so that spaCy can avoid unnecessary copy operations where\\npossible. You can modify the vectors via the \", _jsx(_components.a, {\n          href: \"/api/vocab\",\n          children: _jsx(InlineCode, {\n            children: \"Vocab\"\n          })\n        }), \" or\\n\", _jsx(_components.a, {\n          href: \"/api/vectors\",\n          children: _jsx(InlineCode, {\n            children: \"Vectors\"\n          })\n        }), \" table. Using the\\n\", _jsx(_components.a, {\n          href: \"/api/vocab#set_vector\",\n          children: _jsx(InlineCode, {\n            children: \"Vocab.set_vector\"\n          })\n        }), \" method is often the easiest approach\\nif you have vectors in an arbitrary format, as you can read in the vectors with\\nyour own logic, and just set them with a simple loop. This method is likely to\\nbe slower than approaches that work with the whole vectors table at once, but\\nitâ€™s a great approach for once-off conversions before you save out your \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \"\\nobject to disk.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Adding vectors\",\n          children: \"from spacy.vocab import Vocab\\n\\nvector_data = {\\n    \\\"dog\\\": numpy.random.uniform(-1, 1, (300,)),\\n    \\\"cat\\\": numpy.random.uniform(-1, 1, (300,)),\\n    \\\"orange\\\": numpy.random.uniform(-1, 1, (300,))\\n}\\nvocab = Vocab()\\nfor word, vector in vector_data.items():\\n    vocab.set_vector(word, vector)\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-language-data\",\n      children: [_jsx(_components.h2, {\n        id: \"language-data\",\n        children: \"Language Data \"\n      }), _jsx(LanguageData101, {}), _jsx(_components.h3, {\n        id: \"language-subclass\",\n        children: \"Creating a custom language subclass \"\n      }), _jsxs(_components.p, {\n        children: [\"If you want to customize multiple components of the language data or add support\\nfor a custom language or domain-specific â€œdialectâ€, you can also implement your\\nown language subclass. The subclass should define two attributes: the \", _jsx(InlineCode, {\n          children: \"lang\"\n        }), \"\\n(unique language code) and the \", _jsx(InlineCode, {\n          children: \"Defaults\"\n        }), \" defining the language data. For an\\noverview of the available attributes that can be overwritten, see the\\n\", _jsx(_components.a, {\n          href: \"/api/language#defaults\",\n          children: _jsx(InlineCode, {\n            children: \"Language.Defaults\"\n          })\n        }), \" documentation.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\n\\nclass CustomEnglishDefaults(English.Defaults):\\n    stop_words = set([\\\"custom\\\", \\\"stop\\\"])\\n\\nclass CustomEnglish(English):\\n    lang = \\\"custom_en\\\"\\n    Defaults = CustomEnglishDefaults\\n\\nnlp1 = English()\\nnlp2 = CustomEnglish()\\n\\nprint(nlp1.lang, [token.is_stop for token in nlp1(\\\"custom stop\\\")])\\nprint(nlp2.lang, [token.is_stop for token in nlp2(\\\"custom stop\\\")])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/top-level#registry\",\n          children: _jsx(InlineCode, {\n            children: \"@spacy.registry.languages\"\n          })\n        }), \" decorator lets you\\nregister a custom language class and assign it a string name. This means that\\nyou can call \", _jsx(_components.a, {\n          href: \"/api/top-level#spacy.blank\",\n          children: _jsx(InlineCode, {\n            children: \"spacy.blank\"\n          })\n        }), \" with your custom\\nlanguage name, and even train pipelines with it and refer to it in your\\n\", _jsx(_components.a, {\n          href: \"/usage/training#config\",\n          children: \"training config\"\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Config usage\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"After registering your custom language class using the \", _jsx(InlineCode, {\n            children: \"languages\"\n          }), \" registry,\\nyou can refer to it in your \", _jsx(_components.a, {\n            href: \"/usage/training#config\",\n            children: \"training config\"\n          }), \". This\\nmeans spaCy will train your pipeline using the custom subclass.\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-ini\",\n            lang: \"ini\",\n            children: \"[nlp]\\nlang = \\\"custom_en\\\"\\n\"\n          })\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"In order to resolve \", _jsx(InlineCode, {\n            children: \"\\\"custom_en\\\"\"\n          }), \" to your subclass, the registered function\\nneeds to be available during training. You can load a Python file containing\\nthe code using the \", _jsx(InlineCode, {\n            children: \"--code\"\n          }), \" argument:\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-bash\",\n            lang: \"bash\",\n            children: \"python -m spacy train config.cfg --code code.py\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Registering a custom language\",\n          highlight: \"7,12-13\",\n          children: \"import spacy\\nfrom spacy.lang.en import English\\n\\nclass CustomEnglishDefaults(English.Defaults):\\n    stop_words = set([\\\"custom\\\", \\\"stop\\\"])\\n\\n@spacy.registry.languages(\\\"custom_en\\\")\\nclass CustomEnglish(English):\\n    lang = \\\"custom_en\\\"\\n    Defaults = CustomEnglishDefaults\\n\\n# This now works! ðŸŽ‰\\nnlp = spacy.blank(\\\"custom_en\\\")\\n\"\n        })\n      })]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{"title":"Linguistic Features","next":"/usage/rule-based-matching","menu":[["POS Tagging","pos-tagging"],["Morphology","morphology"],["Lemmatization","lemmatization"],["Dependency Parse","dependency-parse"],["Named Entities","named-entities"],["Entity Linking","entity-linking"],["Tokenization","tokenization"],["Merging & Splitting","retokenization"],["Sentence Segmentation","sbd"],["Mappings & Exceptions","mappings-exceptions"],["Vectors & Similarity","vectors-similarity"],["Language Data","language-data"]]},"scope":{}},"sectionTitle":"Usage Documentation","theme":"blue","section":"usage","apiDetails":{"stringName":null,"baseClass":null,"trainable":null},"isIndex":false},"__N_SSG":true}