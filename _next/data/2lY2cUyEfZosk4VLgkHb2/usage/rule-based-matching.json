{"pageProps":{"title":"Rule-based matching","teaser":"Find phrases and tokens, and match entities","menu":[["Token Matcher","matcher"],["Phrase Matcher","phrasematcher"],["Dependency Matcher","dependencymatcher"],["Entity Ruler","entityruler"],["Span Ruler","spanruler"],["Models & Rules","models-rules"]],"slug":"/usage/rule-based-matching","mdx":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\n/*TODO: double-check that this still works if the ruler is added to the pipeline on creation, and include suggestion if needed*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    section: \"section\",\n    p: \"p\",\n    strong: \"strong\",\n    em: \"em\",\n    a: \"a\",\n    h2: \"h2\",\n    h3: \"h3\",\n    ol: \"ol\",\n    li: \"li\",\n    pre: \"pre\",\n    code: \"code\",\n    h4: \"h4\",\n    table: \"table\",\n    thead: \"thead\",\n    tr: \"tr\",\n    th: \"th\",\n    tbody: \"tbody\",\n    td: \"td\",\n    del: \"del\",\n    hr: \"hr\",\n    blockquote: \"blockquote\",\n    h5: \"h5\",\n    ul: \"ul\",\n    img: \"img\"\n  }, _provideComponents(), props.components), {InlineCode, Accordion, Infobox, Image, Iframe} = _components;\n  if (!Accordion) _missingMdxReference(\"Accordion\", true);\n  if (!Iframe) _missingMdxReference(\"Iframe\", true);\n  if (!Image) _missingMdxReference(\"Image\", true);\n  if (!Infobox) _missingMdxReference(\"Infobox\", true);\n  if (!InlineCode) _missingMdxReference(\"InlineCode\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.section, {\n      children: [_jsxs(_components.p, {\n        children: [\"Compared to using regular expressions on raw text, spaCy’s rule-based matcher\\nengines and components not only let you find the words and phrases you’re\\nlooking for – they also give you access to the tokens within the document and\\ntheir relationships. This means you can easily access and analyze the\\nsurrounding tokens, merge spans into single tokens or add entries to the named\\nentities in \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \".\"]\n      }), _jsxs(Accordion, {\n        title: \"Should I use rules or train a model?\",\n        id: \"rules-vs-model\",\n        children: [_jsx(_components.p, {\n          children: \"For complex tasks, it’s usually better to train a statistical entity recognition\\nmodel. However, statistical models require training data, so for many\\nsituations, rule-based approaches are more practical. This is especially true at\\nthe start of a project: you can use a rule-based approach as part of a data\\ncollection process, to help you “bootstrap” a statistical model.\"\n        }), _jsxs(_components.p, {\n          children: [\"Training a model is useful if you have some examples and you want your system to\\nbe able to \", _jsx(_components.strong, {\n            children: \"generalize\"\n          }), \" based on those examples. It works especially well if\\nthere are clues in the \", _jsx(_components.em, {\n            children: \"local context\"\n          }), \". For instance, if you’re trying to detect\\nperson or company names, your application may benefit from a statistical named\\nentity recognition model.\"]\n        }), _jsxs(_components.p, {\n          children: [\"Rule-based systems are a good choice if there’s a more or less \", _jsx(_components.strong, {\n            children: \"finite number\"\n          }), \"\\nof examples that you want to find in the data, or if there’s a very \", _jsx(_components.strong, {\n            children: \"clear,\\nstructured pattern\"\n          }), \" you can express with token rules or regular expressions.\\nFor instance, country names, IP addresses or URLs are things you might be able\\nto handle well with a purely rule-based approach.\"]\n        }), _jsxs(_components.p, {\n          children: [\"You can also combine both approaches and improve a statistical model with rules\\nto handle very specific cases and boost accuracy. For details, see the section\\non \", _jsx(_components.a, {\n            href: \"#entityruler\",\n            children: \"rule-based entity recognition\"\n          }), \".\"]\n        })]\n      }), _jsxs(Accordion, {\n        title: \"When should I use the token matcher vs. the phrase matcher?\",\n        id: \"matcher-vs-phrase-matcher\",\n        children: [_jsxs(_components.p, {\n          children: [\"The \", _jsx(InlineCode, {\n            children: \"PhraseMatcher\"\n          }), \" is useful if you already have a large terminology list or\\ngazetteer consisting of single or multi-token phrases that you want to find\\nexact instances of in your data. As of spaCy v2.1.0, you can also match on the\\n\", _jsx(InlineCode, {\n            children: \"LOWER\"\n          }), \" attribute for fast and case-insensitive matching.\"]\n        }), _jsxs(_components.p, {\n          children: [\"The \", _jsx(InlineCode, {\n            children: \"Matcher\"\n          }), \" isn’t as blazing fast as the \", _jsx(InlineCode, {\n            children: \"PhraseMatcher\"\n          }), \", since it compares\\nacross individual token attributes. However, it allows you to write very\\nabstract representations of the tokens you’re looking for, using lexical\\nattributes, linguistic features predicted by the model, operators, set\\nmembership and rich comparison. For example, you can find a noun, followed by a\\nverb with the lemma “love” or “like”, followed by an optional determiner and\\nanother token that’s at least 10 characters long.\"]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-matcher\",\n      children: [_jsx(_components.h2, {\n        id: \"matcher\",\n        children: \"Token-based matching \"\n      }), _jsxs(_components.p, {\n        children: [\"spaCy features a rule-matching engine, the \", _jsx(_components.a, {\n          href: \"/api/matcher\",\n          children: _jsx(InlineCode, {\n            children: \"Matcher\"\n          })\n        }), \", that\\noperates over tokens, similar to regular expressions. The rules can refer to\\ntoken annotations (e.g. the token \", _jsx(InlineCode, {\n          children: \"text\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"tag_\"\n        }), \", and flags like \", _jsx(InlineCode, {\n          children: \"IS_PUNCT\"\n        }), \").\\nThe rule matcher also lets you pass in a custom callback to act on matches – for\\nexample, to merge entities and apply custom labels. You can also associate\\npatterns with entity IDs, to allow some basic entity linking or disambiguation.\\nTo match large terminology lists, you can use the\\n\", _jsx(_components.a, {\n          href: \"/api/phrasematcher\",\n          children: _jsx(InlineCode, {\n            children: \"PhraseMatcher\"\n          })\n        }), \", which accepts \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" objects as match\\npatterns.\"]\n      }), _jsx(_components.h3, {\n        id: \"adding-patterns\",\n        children: \"Adding patterns \"\n      }), _jsx(_components.p, {\n        children: \"Let’s say we want to enable spaCy to find a combination of three tokens:\"\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"A token whose \", _jsx(_components.strong, {\n            children: \"lowercase form matches “hello”\"\n          }), \", e.g. “Hello” or “HELLO”.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"A token whose \", _jsxs(_components.strong, {\n            children: [_jsx(InlineCode, {\n              children: \"is_punct\"\n            }), \" flag is set to \", _jsx(InlineCode, {\n              children: \"True\"\n            })]\n          }), \", i.e. any punctuation.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"A token whose \", _jsx(_components.strong, {\n            children: \"lowercase form matches “world”\"\n          }), \", e.g. “World” or “WORLD”.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"[{\\\"LOWER\\\": \\\"hello\\\"}, {\\\"IS_PUNCT\\\": True}, {\\\"LOWER\\\": \\\"world\\\"}]\\n\"\n        })\n      }), _jsxs(Infobox, {\n        title: \"Important note\",\n        variant: \"danger\",\n        children: [_jsxs(_components.p, {\n          children: [\"When writing patterns, keep in mind that \", _jsx(_components.strong, {\n            children: \"each dictionary\"\n          }), \" represents \", _jsx(_components.strong, {\n            children: \"one\\ntoken\"\n          }), \". If spaCy’s tokenization doesn’t match the tokens defined in a pattern,\\nthe pattern is not going to produce any results. When developing complex\\npatterns, make sure to check examples against spaCy’s tokenization:\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"doc = nlp(\\\"A complex-example,!\\\")\\nprint([token.text for token in doc])\\n\"\n          })\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"First, we initialize the \", _jsx(InlineCode, {\n          children: \"Matcher\"\n        }), \" with a vocab. The matcher must always share\\nthe same vocab with the documents it will operate on. We can now call\\n\", _jsx(_components.a, {\n          href: \"/api/matcher#add\",\n          children: _jsx(InlineCode, {\n            children: \"matcher.add()\"\n          })\n        }), \" with an ID and a list of patterns.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import Matcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = Matcher(nlp.vocab)\\n# Add match ID \\\"HelloWorld\\\" with no callback and one pattern\\npattern = [{\\\"LOWER\\\": \\\"hello\\\"}, {\\\"IS_PUNCT\\\": True}, {\\\"LOWER\\\": \\\"world\\\"}]\\nmatcher.add(\\\"HelloWorld\\\", [pattern])\\n\\ndoc = nlp(\\\"Hello, world! Hello world!\\\")\\nmatches = matcher(doc)\\nfor match_id, start, end in matches:\\n    string_id = nlp.vocab.strings[match_id]  # Get string representation\\n    span = doc[start:end]  # The matched span\\n    print(match_id, string_id, start, end, span.text)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The matcher returns a list of \", _jsx(InlineCode, {\n          children: \"(match_id, start, end)\"\n        }), \" tuples – in this case,\\n\", _jsx(InlineCode, {\n          children: \"[('15578876784678163569', 0, 3)]\"\n        }), \", which maps to the span \", _jsx(InlineCode, {\n          children: \"doc[0:3]\"\n        }), \" of our\\noriginal document. The \", _jsx(InlineCode, {\n          children: \"match_id\"\n        }), \" is the \", _jsx(_components.a, {\n          href: \"/usage/spacy-101#vocab\",\n          children: \"hash value\"\n        }), \" of\\nthe string ID “HelloWorld”. To get the string value, you can look up the ID in\\nthe \", _jsx(_components.a, {\n          href: \"/api/stringstore\",\n          children: _jsx(InlineCode, {\n            children: \"StringStore\"\n          })\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"for match_id, start, end in matches:\\n    string_id = nlp.vocab.strings[match_id]  # 'HelloWorld'\\n    span = doc[start:end]                    # The matched span\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"Optionally, we could also choose to add more than one pattern, for example to\\nalso match sequences without punctuation between “hello” and “world”:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"patterns = [\\n    [{\\\"LOWER\\\": \\\"hello\\\"}, {\\\"IS_PUNCT\\\": True}, {\\\"LOWER\\\": \\\"world\\\"}],\\n    [{\\\"LOWER\\\": \\\"hello\\\"}, {\\\"LOWER\\\": \\\"world\\\"}]\\n]\\nmatcher.add(\\\"HelloWorld\\\", patterns)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"By default, the matcher will only return the matches and \", _jsx(_components.strong, {\n          children: \"not do anything\\nelse\"\n        }), \", like merge entities or assign labels. This is all up to you and can be\\ndefined individually for each pattern, by passing in a callback function as the\\n\", _jsx(InlineCode, {\n          children: \"on_match\"\n        }), \" argument on \", _jsx(InlineCode, {\n          children: \"add()\"\n        }), \". This is useful, because it lets you write\\nentirely custom and \", _jsx(_components.strong, {\n          children: \"pattern-specific logic\"\n        }), \". For example, you might want to\\nmerge \", _jsx(_components.em, {\n          children: \"some\"\n        }), \" patterns into one token, while adding entity labels for other\\npattern types. You shouldn’t have to create different matchers for each of those\\nprocesses.\"]\n      }), _jsx(_components.h4, {\n        id: \"adding-patterns-attributes\",\n        children: \"Available token attributes \"\n      }), _jsxs(_components.p, {\n        children: [\"The available token pattern keys correspond to a number of\\n\", _jsxs(_components.a, {\n          href: \"/api/token#attributes\",\n          children: [_jsx(InlineCode, {\n            children: \"Token\"\n          }), \" attributes\"]\n        }), \". The supported attributes for\\nrule-based matching are:\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Attribute\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"ORTH\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The exact verbatim text of a token. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"TEXT\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The exact verbatim text of a token. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NORM\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The normalized form of the token text. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"LOWER\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The lowercase form of the token text. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"LENGTH\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The length of the token text. \", _jsx(_components.del, {\n                children: \"int\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"IS_ALPHA\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"IS_ASCII\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"IS_DIGIT\"\n              })]\n            }), _jsxs(_components.td, {\n              children: [\"Token text consists of alphabetic characters, ASCII characters, digits. \", _jsx(_components.del, {\n                children: \"bool\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"IS_LOWER\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"IS_UPPER\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"IS_TITLE\"\n              })]\n            }), _jsxs(_components.td, {\n              children: [\"Token text is in lowercase, uppercase, titlecase. \", _jsx(_components.del, {\n                children: \"bool\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"IS_PUNCT\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"IS_SPACE\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"IS_STOP\"\n              })]\n            }), _jsxs(_components.td, {\n              children: [\"Token is punctuation, whitespace, stop word. \", _jsx(_components.del, {\n                children: \"bool\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"IS_SENT_START\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Token is start of sentence. \", _jsx(_components.del, {\n                children: \"bool\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"LIKE_NUM\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"LIKE_URL\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"LIKE_EMAIL\"\n              })]\n            }), _jsxs(_components.td, {\n              children: [\"Token text resembles a number, URL, email. \", _jsx(_components.del, {\n                children: \"bool\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"SPACY\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Token has a trailing space. \", _jsx(_components.del, {\n                children: \"bool\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"POS\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"TAG\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"MORPH\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"DEP\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"LEMMA\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"SHAPE\"\n              })]\n            }), _jsxs(_components.td, {\n              children: [\"The token’s simple and extended part-of-speech tag, morphological analysis, dependency label, lemma, shape. Note that the values of these attributes are case-sensitive. For a list of available part-of-speech tags and dependency labels, see the \", _jsx(_components.a, {\n                href: \"/api/annotation\",\n                children: \"Annotation Specifications\"\n              }), \". \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"ENT_TYPE\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The token’s entity label. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"_\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Properties in \", _jsx(_components.a, {\n                href: \"/usage/processing-pipelines#custom-components-attributes\",\n                children: \"custom extension attributes\"\n              }), \". \", _jsx(_components.del, {\n                children: \"Dict[str, Any]\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"OP\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(_components.a, {\n                href: \"#quantifiers\",\n                children: \"Operator or quantifier\"\n              }), \" to determine how often to match a token pattern. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          })]\n        })]\n      }), _jsx(Accordion, {\n        title: \"Does it matter if the attribute names are uppercase or lowercase?\",\n        children: _jsxs(_components.p, {\n          children: [\"No, it shouldn’t. spaCy will normalize the names internally and\\n\", _jsx(InlineCode, {\n            children: \"{\\\"LOWER\\\": \\\"text\\\"}\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"{\\\"lower\\\": \\\"text\\\"}\"\n          }), \" will both produce the same result.\\nUsing the uppercase version is mostly a convention to make it clear that the\\nattributes are “special” and don’t exactly map to the token attributes like\\n\", _jsx(InlineCode, {\n            children: \"Token.lower\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"Token.lower_\"\n          }), \".\"]\n        })\n      }), _jsxs(Accordion, {\n        title: \"Why are not all token attributes supported?\",\n        children: [_jsxs(_components.p, {\n          children: [\"spaCy can’t provide access to all of the attributes because the \", _jsx(InlineCode, {\n            children: \"Matcher\"\n          }), \" loops\\nover the Cython data, not the Python objects. Inside the matcher, we’re dealing\\nwith a \", _jsxs(_components.a, {\n            href: \"/api/cython-structs#tokenc\",\n            children: [_jsx(InlineCode, {\n              children: \"TokenC\"\n            }), \" struct\"]\n          }), \" – we don’t have an instance\\nof \", _jsx(_components.a, {\n            href: \"/api/token\",\n            children: _jsx(InlineCode, {\n              children: \"Token\"\n            })\n          }), \". This means that all of the attributes that refer to\\ncomputed properties can’t be accessed.\"]\n        }), _jsxs(_components.p, {\n          children: [\"The uppercase attribute names like \", _jsx(InlineCode, {\n            children: \"LOWER\"\n          }), \" or \", _jsx(InlineCode, {\n            children: \"IS_PUNCT\"\n          }), \" refer to symbols from\\nthe \", _jsx(_components.a, {\n            href: \"https://github.com/explosion/spaCy/tree/master/spacy/attrs.pyx\",\n            children: _jsx(InlineCode, {\n              children: \"spacy.attrs\"\n            })\n          }), \" enum table. They’re passed\\ninto a function that essentially is a big case/switch statement, to figure out\\nwhich struct field to return. The same attribute identifiers are used in\\n\", _jsx(_components.a, {\n            href: \"/api/doc#to_array\",\n            children: _jsx(InlineCode, {\n              children: \"Doc.to_array\"\n            })\n          }), \", and a few other places in the code where\\nyou need to describe fields like this.\"]\n        })]\n      }), _jsx(_components.hr, {}), _jsxs(Infobox, {\n        title: \"Tip: Try the interactive matcher explorer\",\n        children: [_jsx(Image, {\n          src: \"/images/matcher-demo.jpg\",\n          href: \"https://explosion.ai/demos/matcher\",\n          alt: \"Matcher demo\"\n        }), _jsxs(_components.p, {\n          children: [\"The \", _jsx(_components.a, {\n            href: \"https://explosion.ai/demos/matcher\",\n            children: \"Matcher Explorer\"\n          }), \" lets you test the\\nrule-based \", _jsx(InlineCode, {\n            children: \"Matcher\"\n          }), \" by creating token patterns interactively and running them\\nover your text. Each token can set multiple attributes like text value,\\npart-of-speech tag or boolean flags. The token-based view lets you explore how\\nspaCy processes your text – and why your pattern matches, or why it doesn’t.\"]\n        })]\n      }), _jsx(_components.h4, {\n        id: \"adding-patterns-attributes-extended\",\n        version: \"2.1\",\n        children: \"Extended pattern syntax and attributes \"\n      }), _jsxs(_components.p, {\n        children: [\"Instead of mapping to a single value, token patterns can also map to a\\n\", _jsx(_components.strong, {\n          children: \"dictionary of properties\"\n        }), \". For example, to specify that the value of a lemma\\nshould be part of a list of values, or to set a minimum character length. The\\nfollowing rich comparison attributes are available:\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"# Matches \\\"love cats\\\" or \\\"likes flowers\\\"\\npattern1 = [{\\\"LEMMA\\\": {\\\"IN\\\": [\\\"like\\\", \\\"love\\\"]}},\\n            {\\\"POS\\\": \\\"NOUN\\\"}]\\n\\n# Matches tokens of length >= 10\\npattern2 = [{\\\"LENGTH\\\": {\\\">=\\\": 10}}]\\n\\n# Match based on morph attributes\\npattern3 = [{\\\"MORPH\\\": {\\\"IS_SUBSET\\\": [\\\"Number=Sing\\\", \\\"Gender=Neut\\\"]}}]\\n# \\\"\\\", \\\"Number=Sing\\\" and \\\"Number=Sing|Gender=Neut\\\" will match as subsets\\n# \\\"Number=Plur|Gender=Neut\\\" will not match\\n# \\\"Number=Sing|Gender=Neut|Polite=Infm\\\" will not match because it's a superset\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Attribute\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"IN\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attribute value is member of a list. \", _jsx(_components.del, {\n                children: \"Any\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"NOT_IN\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attribute value is \", _jsx(_components.em, {\n                children: \"not\"\n              }), \" member of a list. \", _jsx(_components.del, {\n                children: \"Any\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"IS_SUBSET\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attribute value (for \", _jsx(InlineCode, {\n                children: \"MORPH\"\n              }), \" or custom list attributes) is a subset of a list. \", _jsx(_components.del, {\n                children: \"Any\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"IS_SUPERSET\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attribute value (for \", _jsx(InlineCode, {\n                children: \"MORPH\"\n              }), \" or custom list attributes) is a superset of a list. \", _jsx(_components.del, {\n                children: \"Any\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"INTERSECTS\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Attribute value (for \", _jsx(InlineCode, {\n                children: \"MORPH\"\n              }), \" or custom list attributes) has a non-empty intersection with a list. \", _jsx(_components.del, {\n                children: \"Any\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"==\"\n              }), \", \", _jsx(InlineCode, {\n                children: \">=\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"<=\"\n              }), \", \", _jsx(InlineCode, {\n                children: \">\"\n              }), \", \", _jsx(InlineCode, {\n                children: \"<\"\n              })]\n            }), _jsxs(_components.td, {\n              children: [\"Attribute value is equal, greater or equal, smaller or equal, greater or smaller. \", _jsx(_components.del, {\n                children: \"Union[int, float]\"\n              })]\n            })]\n          })]\n        })]\n      }), _jsx(_components.h4, {\n        id: \"regex\",\n        version: \"2.1\",\n        children: \"Regular expressions \"\n      }), _jsx(_components.p, {\n        children: \"In some cases, only matching tokens and token attributes isn’t enough – for\\nexample, you might want to match different spellings of a word, without having\\nto add a new pattern for each spelling.\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"pattern = [{\\\"TEXT\\\": {\\\"REGEX\\\": \\\"^[Uu](\\\\\\\\.?|nited)$\\\"}},\\n           {\\\"TEXT\\\": {\\\"REGEX\\\": \\\"^[Ss](\\\\\\\\.?|tates)$\\\"}},\\n           {\\\"LOWER\\\": \\\"president\\\"}]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"REGEX\"\n        }), \" operator allows defining rules for any attribute string value,\\nincluding custom attributes. It always needs to be applied to an attribute like\\n\", _jsx(InlineCode, {\n          children: \"TEXT\"\n        }), \", \", _jsx(InlineCode, {\n          children: \"LOWER\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"TAG\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# Match different spellings of token texts\\npattern = [{\\\"TEXT\\\": {\\\"REGEX\\\": \\\"deff?in[ia]tely\\\"}}]\\n\\n# Match tokens with fine-grained POS tags starting with 'V'\\npattern = [{\\\"TAG\\\": {\\\"REGEX\\\": \\\"^V\\\"}}]\\n\\n# Match custom attribute values with regular expressions\\npattern = [{\\\"_\\\": {\\\"country\\\": {\\\"REGEX\\\": \\\"^[Uu](nited|\\\\\\\\.?) ?[Ss](tates|\\\\\\\\.?)$\\\"}}}]\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"When using the \", _jsx(InlineCode, {\n            children: \"REGEX\"\n          }), \" operator, keep in mind that it operates on \", _jsx(_components.strong, {\n            children: \"single\\ntokens\"\n          }), \", not the whole text. Each expression you provide will be matched on a\\ntoken. If you need to match on the whole text instead, see the details on\\n\", _jsx(_components.a, {\n            href: \"#regex-text\",\n            children: \"regex matching on the whole text\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h5, {\n        id: \"regex-text\",\n        children: \"Matching regular expressions on the full text \"\n      }), _jsxs(_components.p, {\n        children: [\"If your expressions apply to multiple tokens, a simple solution is to match on\\nthe \", _jsx(InlineCode, {\n          children: \"doc.text\"\n        }), \" with \", _jsx(InlineCode, {\n          children: \"re.finditer\"\n        }), \" and use the\\n\", _jsx(_components.a, {\n          href: \"/api/doc#char_span\",\n          children: _jsx(InlineCode, {\n            children: \"Doc.char_span\"\n          })\n        }), \" method to create a \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" from the\\ncharacter indices of the match. If the matched characters don’t map to one or\\nmore valid tokens, \", _jsx(InlineCode, {\n          children: \"Doc.char_span\"\n        }), \" returns \", _jsx(InlineCode, {\n          children: \"None\"\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"What’s a valid token sequence?\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"In the example, the expression will also match \", _jsx(InlineCode, {\n            children: \"\\\"US\\\"\"\n          }), \" in \", _jsx(InlineCode, {\n            children: \"\\\"USA\\\"\"\n          }), \". However,\\n\", _jsx(InlineCode, {\n            children: \"\\\"USA\\\"\"\n          }), \" is a single token and \", _jsx(InlineCode, {\n            children: \"Span\"\n          }), \" objects are \", _jsx(_components.strong, {\n            children: \"sequences of tokens\"\n          }), \". So\\n\", _jsx(InlineCode, {\n            children: \"\\\"US\\\"\"\n          }), \" cannot be its own span, because it does not end on a token boundary.\"]\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nimport re\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"The United States of America (USA) are commonly known as the United States (U.S. or US) or America.\\\")\\n\\nexpression = r\\\"[Uu](nited|\\\\\\\\.?) ?[Ss](tates|\\\\\\\\.?)\\\"\\nfor match in re.finditer(expression, doc.text):\\n    start, end = match.span()\\n    span = doc.char_span(start, end)\\n    # This is a Span object or None if match doesn't map to valid token sequence\\n    if span is not None:\\n        print(\\\"Found match:\\\", span.text)\\n\"\n        })\n      }), _jsxs(Accordion, {\n        title: \"How can I expand the match to a valid token sequence?\",\n        children: [_jsxs(_components.p, {\n          children: [\"In some cases, you might want to expand the match to the closest token\\nboundaries, so you can create a \", _jsx(InlineCode, {\n            children: \"Span\"\n          }), \" for \", _jsx(InlineCode, {\n            children: \"\\\"USA\\\"\"\n          }), \", even though only the\\nsubstring \", _jsx(InlineCode, {\n            children: \"\\\"US\\\"\"\n          }), \" is matched. You can calculate this using the character offsets\\nof the tokens in the document, available as\\n\", _jsx(_components.a, {\n            href: \"/api/token#attributes\",\n            children: _jsx(InlineCode, {\n              children: \"Token.idx\"\n            })\n          }), \". This lets you create a list of valid token\\nstart and end boundaries and leaves you with a rather basic algorithmic problem:\\nGiven a number, find the next lowest (start token) or the next highest (end\\ntoken) number that’s part of a given list of numbers. This will be the closest\\nvalid token boundary.\"]\n        }), _jsxs(_components.p, {\n          children: [\"There are many ways to do this and the most straightforward one is to create a\\ndict keyed by characters in the \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \", mapped to the token they’re part of. It’s\\neasy to write and less error-prone, and gives you a constant lookup time: you\\nonly ever need to create the dict once per \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \".\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"chars_to_tokens = {}\\nfor token in doc:\\n    for i in range(token.idx, token.idx + len(token.text)):\\n        chars_to_tokens[i] = token.i\\n\"\n          })\n        }), _jsxs(_components.p, {\n          children: [\"You can then look up character at a given position, and get the index of the\\ncorresponding token that the character is part of. Your span would then be\\n\", _jsx(InlineCode, {\n            children: \"doc[token_start:token_end]\"\n          }), \". If a character isn’t in the dict, it means it’s\\nthe (white)space tokens are split on. That hopefully shouldn’t happen, though,\\nbecause it’d mean your regex is producing matches with leading or trailing\\nwhitespace.\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            highlight: \"5-8\",\n            children: \"span = doc.char_span(start, end)\\nif span is not None:\\n    print(\\\"Found match:\\\", span.text)\\nelse:\\n    start_token = chars_to_tokens.get(start)\\n    end_token = chars_to_tokens.get(end)\\n    if start_token is not None and end_token is not None:\\n        span = doc[start_token:end_token + 1]\\n        print(\\\"Found closest match:\\\", span.text)\\n\"\n          })\n        })]\n      }), _jsx(_components.h4, {\n        id: \"fuzzy\",\n        version: \"3.5\",\n        children: \"Fuzzy matching \"\n      }), _jsx(_components.p, {\n        children: \"Fuzzy matching allows you to match tokens with alternate spellings, typos, etc.\\nwithout specifying every possible variant.\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# Matches \\\"favourite\\\", \\\"favorites\\\", \\\"gavorite\\\", \\\"theatre\\\", \\\"theatr\\\", ...\\npattern = [{\\\"TEXT\\\": {\\\"FUZZY\\\": \\\"favorite\\\"}},\\n           {\\\"TEXT\\\": {\\\"FUZZY\\\": \\\"theater\\\"}}]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"FUZZY\"\n        }), \" attribute allows fuzzy matches for any attribute string value,\\nincluding custom attributes. Just like \", _jsx(InlineCode, {\n          children: \"REGEX\"\n        }), \", it always needs to be applied to\\nan attribute like \", _jsx(InlineCode, {\n          children: \"TEXT\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"LOWER\"\n        }), \". By default \", _jsx(InlineCode, {\n          children: \"FUZZY\"\n        }), \" allows a Levenshtein\\nedit distance of at least 2 and up to 30% of the pattern string length. Using\\nthe more specific attributes \", _jsx(InlineCode, {\n          children: \"FUZZY1\"\n        }), \"..\", _jsx(InlineCode, {\n          children: \"FUZZY9\"\n        }), \" you can specify the maximum\\nallowed edit distance directly.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"# Match lowercase with fuzzy matching (allows 2 edits by default)\\npattern = [{\\\"LOWER\\\": {\\\"FUZZY\\\": \\\"definitely\\\"}}]\\n\\n# Match custom attribute values with fuzzy matching (allows 2 edits by default)\\npattern = [{\\\"_\\\": {\\\"country\\\": {\\\"FUZZY\\\": \\\"Kyrgyzstan\\\"}}}]\\n\\n# Match with exact Levenshtein edit distance limits (allows 4 edits)\\npattern = [{\\\"_\\\": {\\\"country\\\": {\\\"FUZZY4\\\": \\\"Kyrgyzstan\\\"}}}]\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"regex-fuzzy-lists\",\n        version: \"3.5\",\n        children: \"Regex and fuzzy matching with lists \"\n      }), _jsxs(_components.p, {\n        children: [\"Starting in spaCy v3.5, both \", _jsx(InlineCode, {\n          children: \"REGEX\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"FUZZY\"\n        }), \" can be combined with the\\nattributes \", _jsx(InlineCode, {\n          children: \"IN\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"NOT_IN\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"pattern = [{\\\"TEXT\\\": {\\\"FUZZY\\\": {\\\"IN\\\": [\\\"awesome\\\", \\\"cool\\\", \\\"wonderful\\\"]}}}]\\n\\npattern = [{\\\"TEXT\\\": {\\\"REGEX\\\": {\\\"NOT_IN\\\": [\\\"^awe(some)?$\\\", \\\"^wonder(ful)?\\\"]}}}]\\n\"\n        })\n      }), _jsx(_components.hr, {}), _jsx(_components.h4, {\n        id: \"quantifiers\",\n        children: \"Operators and quantifiers \"\n      }), _jsxs(_components.p, {\n        children: [\"The matcher also lets you use quantifiers, specified as the \", _jsx(InlineCode, {\n          children: \"'OP'\"\n        }), \" key.\\nQuantifiers let you define sequences of tokens to be matched, e.g. one or more\\npunctuation marks, or specify optional tokens. Note that there are no nested or\\nscoped quantifiers – instead, you can build those behaviors with \", _jsx(InlineCode, {\n          children: \"on_match\"\n        }), \"\\ncallbacks.\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"OP\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"!\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Negate the pattern, by requiring it to match exactly 0 times.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"?\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Make the pattern optional, by allowing it to match 0 or 1 times.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"+\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Require the pattern to match 1 or more times.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"*\"\n              })\n            }), _jsx(_components.td, {\n              children: \"Allow the pattern to match zero or more times.\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"{n}\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Require the pattern to match exactly \", _jsx(_components.em, {\n                children: \"n\"\n              }), \" times.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"{n,m}\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Require the pattern to match at least \", _jsx(_components.em, {\n                children: \"n\"\n              }), \" but not more than \", _jsx(_components.em, {\n                children: \"m\"\n              }), \" times.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"{n,}\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Require the pattern to match at least \", _jsx(_components.em, {\n                children: \"n\"\n              }), \" times.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"{,m}\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Require the pattern to match at most \", _jsx(_components.em, {\n                children: \"m\"\n              }), \" times.\"]\n            })]\n          })]\n        })]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Example\"\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"pattern = [{\\\"LOWER\\\": \\\"hello\\\"},\\n           {\\\"IS_PUNCT\\\": True, \\\"OP\\\": \\\"?\\\"}]\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(Infobox, {\n        title: \"Note on operator behaviour\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"In versions before v2.1.0, the semantics of the \", _jsx(InlineCode, {\n            children: \"+\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"*\"\n          }), \" operators behave\\ninconsistently. They were usually interpreted “greedily”, i.e. longer matches\\nare returned where possible. However, if you specify two \", _jsx(InlineCode, {\n            children: \"+\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"*\"\n          }), \" patterns in\\na row and their matches overlap, the first operator will behave non-greedily.\\nThis quirk in the semantics is corrected in spaCy v2.1.0.\"]\n        })\n      }), _jsx(_components.h4, {\n        id: \"adding-patterns-wildcard\",\n        version: \"2\",\n        children: \"Using wildcard token patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"While the token attributes offer many options to write highly specific patterns,\\nyou can also use an empty dictionary, \", _jsx(InlineCode, {\n          children: \"{}\"\n        }), \" as a wildcard representing \", _jsx(_components.strong, {\n          children: \"any\\ntoken\"\n        }), \". This is useful if you know the context of what you’re trying to match,\\nbut very little about the specific token and its characters. For example, let’s\\nsay you’re trying to extract people’s user names from your data. All you know is\\nthat they are listed as “User name: {username}“. The name itself may contain\\nany character, but no whitespace – so you’ll know it will be handled as one\\ntoken.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"[{\\\"ORTH\\\": \\\"User\\\"}, {\\\"ORTH\\\": \\\"name\\\"}, {\\\"ORTH\\\": \\\":\\\"}, {}]\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"pattern-validation\",\n        version: \"2.1\",\n        children: \"Validating and debugging patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"Matcher\"\n        }), \" can validate patterns against a JSON schema with the option\\n\", _jsx(InlineCode, {\n          children: \"validate=True\"\n        }), \". This is useful for debugging patterns during development, in\\nparticular for catching unsupported attributes.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import Matcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = Matcher(nlp.vocab, validate=True)\\n# Add match ID \\\"HelloWorld\\\" with unsupported attribute CASEINSENSITIVE\\npattern = [{\\\"LOWER\\\": \\\"hello\\\"}, {\\\"IS_PUNCT\\\": True}, {\\\"CASEINSENSITIVE\\\": \\\"world\\\"}]\\nmatcher.add(\\\"HelloWorld\\\", [pattern])\\n# 🚨 Raises an error:\\n# MatchPatternError: Invalid token patterns for matcher rule 'HelloWorld'\\n# Pattern 0:\\n# - [pattern -> 2 -> CASEINSENSITIVE] extra fields not permitted\\n\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"on_match\",\n        children: \"Adding on_match rules \"\n      }), _jsxs(_components.p, {\n        children: [\"To move on to a more realistic example, let’s say you’re working with a large\\ncorpus of blog articles, and you want to match all mentions of “Google I/O”\\n(which spaCy tokenizes as \", _jsx(InlineCode, {\n          children: \"['Google', 'I', '/', 'O'\"\n        }), \"]). To be safe, you only\\nmatch on the uppercase versions, avoiding matches with phrases such as “Google\\ni/o”.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\nfrom spacy.matcher import Matcher\\nfrom spacy.tokens import Span\\n\\nnlp = English()\\nmatcher = Matcher(nlp.vocab)\\n\\ndef add_event_ent(matcher, doc, i, matches):\\n    # Get the current match and create tuple of entity label, start and end.\\n    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\\n    match_id, start, end = matches[i]\\n    entity = Span(doc, start, end, label=\\\"EVENT\\\")\\n    doc.ents += (entity,)\\n    print(entity.text)\\n\\npattern = [{\\\"ORTH\\\": \\\"Google\\\"}, {\\\"ORTH\\\": \\\"I\\\"}, {\\\"ORTH\\\": \\\"/\\\"}, {\\\"ORTH\\\": \\\"O\\\"}]\\nmatcher.add(\\\"GoogleIO\\\", [pattern], on_match=add_event_ent)\\ndoc = nlp(\\\"This is a text about Google I/O\\\")\\nmatches = matcher(doc)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"A very similar logic has been implemented in the built-in\\n\", _jsx(_components.a, {\n          href: \"/api/entityruler\",\n          children: _jsx(InlineCode, {\n            children: \"EntityRuler\"\n          })\n        }), \" by the way. It also takes care of handling\\noverlapping matches, which you would otherwise have to take care of yourself.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Tip: Visualizing matches\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"When working with entities, you can use \", _jsx(_components.a, {\n            href: \"/api/top-level#displacy\",\n            children: \"displaCy\"\n          }), \" to\\nquickly generate a NER visualization from your updated \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \", which can be\\nexported as an HTML file:\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"from spacy import displacy\\nhtml = displacy.render(doc, style=\\\"ent\\\", page=True,\\n                       options={\\\"ents\\\": [\\\"EVENT\\\"]})\\n\"\n          })\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"For more info and examples, see the usage guide on\\n\", _jsx(_components.a, {\n            href: \"/usage/visualizers\",\n            children: \"visualizing spaCy\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"We can now call the matcher on our documents. The patterns will be matched in\\nthe order they occur in the text. The matcher will then iterate over the\\nmatches, look up the callback for the match ID that was matched, and invoke it.\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"doc = nlp(YOUR_TEXT_HERE)\\nmatcher(doc)\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"When the callback is invoked, it is passed four arguments: the matcher itself,\\nthe document, the position of the current match, and the total list of matches.\\nThis allows you to write callbacks that consider the entire set of matched\\nphrases, so that you can resolve overlaps and other conflicts in whatever way\\nyou prefer.\"\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Argument\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"matcher\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The matcher instance. \", _jsx(_components.del, {\n                children: \"Matcher\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"doc\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The document the matcher was used on. \", _jsx(_components.del, {\n                children: \"Doc\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"i\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"Index of the current match (\", _jsx(InlineCode, {\n                children: \"matches[i\"\n              }), \"]). \", _jsx(_components.del, {\n                children: \"int\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"matches\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A list of \", _jsx(InlineCode, {\n                children: \"(match_id, start, end)\"\n              }), \" tuples, describing the matches. A match tuple describes a span \", _jsx(InlineCode, {\n                children: \"doc[start:end\"\n              }), \"]. \", _jsx(_components.del, {\n                children: \"List[Tuple[int, int int]]\"\n              })]\n            })]\n          })]\n        })]\n      }), _jsx(_components.h3, {\n        id: \"matcher-spans\",\n        children: \"Creating spans from matches \"\n      }), _jsxs(_components.p, {\n        children: [\"Creating \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \" objects from the returned matches is a very common\\nuse case. spaCy makes this easy by giving you access to the \", _jsx(InlineCode, {\n          children: \"start\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"end\"\n        }), \"\\ntoken of each match, which you can use to construct a new span with an optional\\nlabel. As of spaCy v3.0, you can also set \", _jsx(InlineCode, {\n          children: \"as_spans=True\"\n        }), \" when calling the\\nmatcher on a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \", which will return a list of \", _jsx(_components.a, {\n          href: \"/api/span\",\n          children: _jsx(InlineCode, {\n            children: \"Span\"\n          })\n        }), \" objects\\nusing the \", _jsx(InlineCode, {\n          children: \"match_id\"\n        }), \" as the span label.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import Matcher\\nfrom spacy.tokens import Span\\n\\nnlp = spacy.blank(\\\"en\\\")\\nmatcher = Matcher(nlp.vocab)\\nmatcher.add(\\\"PERSON\\\", [[{\\\"lower\\\": \\\"barack\\\"}, {\\\"lower\\\": \\\"obama\\\"}]])\\ndoc = nlp(\\\"Barack Obama was the 44th president of the United States\\\")\\n\\n# 1. Return (match_id, start, end) tuples\\nmatches = matcher(doc)\\nfor match_id, start, end in matches:\\n    # Create the matched span and assign the match_id as a label\\n    span = Span(doc, start, end, label=match_id)\\n    print(span.text, span.label_)\\n\\n# 2. Return Span objects directly\\nmatches = matcher(doc, as_spans=True)\\nfor span in matches:\\n    print(span.text, span.label_)\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"matcher-pipeline\",\n        children: \"Using custom pipeline components \"\n      }), _jsxs(_components.p, {\n        children: [\"Let’s say your data also contains some annoying pre-processing artifacts, like\\nleftover HTML line breaks (e.g. \", _jsx(InlineCode, {\n          children: \"<br>\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"<BR/>\"\n        }), \"). To make your text easier to\\nanalyze, you want to merge those into one token and flag them, to make sure you\\ncan ignore them later. Ideally, this should all be done automatically as you\\nprocess the text. You can achieve this by adding a\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components\",\n          children: \"custom pipeline component\"\n        }), \"\\nthat’s called on each \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object, merges the leftover HTML spans and sets an\\nattribute \", _jsx(InlineCode, {\n          children: \"bad_html\"\n        }), \" on the token.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.language import Language\\nfrom spacy.matcher import Matcher\\nfrom spacy.tokens import Token\\n\\n# We're using a component factory because the component needs to be\\n# initialized with the shared vocab via the nlp object\\n@Language.factory(\\\"html_merger\\\")\\ndef create_bad_html_merger(nlp, name):\\n    return BadHTMLMerger(nlp.vocab)\\n\\nclass BadHTMLMerger:\\n    def __init__(self, vocab):\\n        patterns = [\\n            [{\\\"ORTH\\\": \\\"<\\\"}, {\\\"LOWER\\\": \\\"br\\\"}, {\\\"ORTH\\\": \\\">\\\"}],\\n            [{\\\"ORTH\\\": \\\"<\\\"}, {\\\"LOWER\\\": \\\"br/\\\"}, {\\\"ORTH\\\": \\\">\\\"}],\\n        ]\\n        # Register a new token extension to flag bad HTML\\n        Token.set_extension(\\\"bad_html\\\", default=False)\\n        self.matcher = Matcher(vocab)\\n        self.matcher.add(\\\"BAD_HTML\\\", patterns)\\n\\n    def __call__(self, doc):\\n        # This method is invoked when the component is called on a Doc\\n        matches = self.matcher(doc)\\n        spans = []  # Collect the matched spans here\\n        for match_id, start, end in matches:\\n            spans.append(doc[start:end])\\n        with doc.retokenize() as retokenizer:\\n            for span in spans:\\n                retokenizer.merge(span)\\n                for token in span:\\n                    token._.bad_html = True  # Mark token as bad HTML\\n        return doc\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nnlp.add_pipe(\\\"html_merger\\\", last=True)  # Add component to the pipeline\\ndoc = nlp(\\\"Hello<br>world! <br/> This is a test.\\\")\\nfor token in doc:\\n    print(token.text, token._.bad_html)\\n\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Instead of hard-coding the patterns into the component, you could also make it\\ntake a path to a JSON file containing the patterns. This lets you reuse the\\ncomponent with different patterns, depending on your application. When adding\\nthe component to the pipeline with \", _jsx(_components.a, {\n          href: \"/api/language#add_pipe\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.add_pipe\"\n          })\n        }), \", you\\ncan pass in the argument via the \", _jsx(InlineCode, {\n          children: \"config\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"@Language.factory(\\\"html_merger\\\", default_config={\\\"path\\\": None})\\ndef create_bad_html_merger(nlp, name, path):\\n    return BadHTMLMerger(nlp, path=path)\\n\\nnlp.add_pipe(\\\"html_merger\\\", config={\\\"path\\\": \\\"/path/to/patterns.json\\\"})\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Processing pipelines\",\n        emoji: \"📖\",\n        children: _jsxs(_components.p, {\n          children: [\"For more details and examples of how to \", _jsx(_components.strong, {\n            children: \"create custom pipeline components\"\n          }), \"\\nand \", _jsx(_components.strong, {\n            children: \"extension attributes\"\n          }), \", see the\\n\", _jsx(_components.a, {\n            href: \"/usage/processing-pipelines\",\n            children: \"usage guide\"\n          }), \".\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"example1\",\n        children: \"Example: Using linguistic annotations \"\n      }), _jsx(_components.p, {\n        children: \"Let’s say you’re analyzing user comments and you want to find out what people\\nare saying about Facebook. You want to start off by finding adjectives following\\n“Facebook is” or “Facebook was”. This is obviously a very rudimentary solution,\\nbut it’ll be fast, and a great way to get an idea for what’s in your data. Your\\npattern could look like this:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"[{\\\"LOWER\\\": \\\"facebook\\\"}, {\\\"LEMMA\\\": \\\"be\\\"}, {\\\"POS\\\": \\\"ADV\\\", \\\"OP\\\": \\\"*\\\"}, {\\\"POS\\\": \\\"ADJ\\\"}]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"This translates to a token whose lowercase form matches “facebook” (like\\nFacebook, facebook or FACEBOOK), followed by a token with the lemma “be” (for\\nexample, is, was, or ‘s), followed by an \", _jsx(_components.strong, {\n          children: \"optional\"\n        }), \" adverb, followed by an\\nadjective. Using the linguistic annotations here is especially useful, because\\nyou can tell spaCy to match “Facebook’s annoying”, but \", _jsx(_components.strong, {\n          children: \"not\"\n        }), \" “Facebook’s\\nannoying ads”. The optional adverb makes sure you won’t miss adjectives with\\nintensifiers, like “pretty awful” or “very nice”.\"]\n      }), _jsxs(_components.p, {\n        children: [\"To get a quick overview of the results, you could collect all sentences\\ncontaining a match and render them with the\\n\", _jsx(_components.a, {\n          href: \"/usage/visualizers\",\n          children: \"displaCy visualizer\"\n        }), \". In the callback function, you’ll have\\naccess to the \", _jsx(InlineCode, {\n          children: \"start\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"end\"\n        }), \" of each match, as well as the parent \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \". This\\nlets you determine the sentence containing the match, \", _jsx(InlineCode, {\n          children: \"doc[start:end].sent\"\n        }), \", and\\ncalculate the start and end of the matched span within the sentence. Using\\ndisplaCy in \", _jsx(_components.a, {\n          href: \"/usage/visualizers#manual-usage\",\n          children: \"“manual” mode\"\n        }), \" lets you pass in a\\nlist of dictionaries containing the text and entities to render.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy import displacy\\nfrom spacy.matcher import Matcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = Matcher(nlp.vocab)\\nmatched_sents = []  # Collect data of matched sentences to be visualized\\n\\ndef collect_sents(matcher, doc, i, matches):\\n    match_id, start, end = matches[i]\\n    span = doc[start:end]  # Matched span\\n    sent = span.sent  # Sentence containing matched span\\n    # Append mock entity for match in displaCy style to matched_sents\\n    # get the match span by ofsetting the start and end of the span with the\\n    # start and end of the sentence in the doc\\n    match_ents = [{\\n        \\\"start\\\": span.start_char - sent.start_char,\\n        \\\"end\\\": span.end_char - sent.start_char,\\n        \\\"label\\\": \\\"MATCH\\\",\\n    }]\\n    matched_sents.append({\\\"text\\\": sent.text, \\\"ents\\\": match_ents})\\n\\npattern = [{\\\"LOWER\\\": \\\"facebook\\\"}, {\\\"LEMMA\\\": \\\"be\\\"}, {\\\"POS\\\": \\\"ADV\\\", \\\"OP\\\": \\\"*\\\"},\\n           {\\\"POS\\\": \\\"ADJ\\\"}]\\nmatcher.add(\\\"FacebookIs\\\", [pattern], on_match=collect_sents)  # add pattern\\ndoc = nlp(\\\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\\\")\\nmatches = matcher(doc)\\n\\n# Serve visualization of sentences containing match with displaCy\\n# set manual=True to make displaCy render straight from a dictionary\\n# (if you're not running the code within a Jupyer environment, you can\\n# use displacy.serve instead)\\ndisplacy.render(matched_sents, style=\\\"ent\\\", manual=True)\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"example2\",\n        children: \"Example: Phone numbers \"\n      }), _jsxs(_components.p, {\n        children: [\"Phone numbers can have many different formats and matching them is often tricky.\\nDuring tokenization, spaCy will leave sequences of numbers intact and only split\\non whitespace and punctuation. This means that your match pattern will have to\\nlook out for number sequences of a certain length, surrounded by specific\\npunctuation – depending on the\\n\", _jsx(_components.a, {\n          href: \"https://en.wikipedia.org/wiki/National_conventions_for_writing_telephone_numbers\",\n          children: \"national conventions\"\n        }), \".\"]\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(InlineCode, {\n          children: \"IS_DIGIT\"\n        }), \" flag is not very helpful here, because it doesn’t tell us\\nanything about the length. However, you can use the \", _jsx(InlineCode, {\n          children: \"SHAPE\"\n        }), \" flag, with each \", _jsx(InlineCode, {\n          children: \"d\"\n        }), \"\\nrepresenting a digit (up to 4 digits / characters):\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"[{\\\"ORTH\\\": \\\"(\\\"}, {\\\"SHAPE\\\": \\\"ddd\\\"}, {\\\"ORTH\\\": \\\")\\\"}, {\\\"SHAPE\\\": \\\"dddd\\\"},\\n {\\\"ORTH\\\": \\\"-\\\", \\\"OP\\\": \\\"?\\\"}, {\\\"SHAPE\\\": \\\"dddd\\\"}]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"This will match phone numbers of the format \", _jsx(_components.strong, {\n          children: \"(123) 4567 8901\"\n        }), \" or \", _jsx(_components.strong, {\n          children: \"(123)\\n4567-8901\"\n        }), \". To also match formats like \", _jsx(_components.strong, {\n          children: \"(123) 456 789\"\n        }), \", you can add a second\\npattern using \", _jsx(InlineCode, {\n          children: \"'ddd'\"\n        }), \" in place of \", _jsx(InlineCode, {\n          children: \"'dddd'\"\n        }), \". By hard-coding some values, you can\\nmatch only certain, country-specific numbers. For example, here’s a pattern to\\nmatch the most common formats of\\n\", _jsx(_components.a, {\n          href: \"https://en.wikipedia.org/wiki/National_conventions_for_writing_telephone_numbers#Germany\",\n          children: \"international German numbers\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"[{\\\"ORTH\\\": \\\"+\\\"}, {\\\"ORTH\\\": \\\"49\\\"}, {\\\"ORTH\\\": \\\"(\\\", \\\"OP\\\": \\\"?\\\"}, {\\\"SHAPE\\\": \\\"dddd\\\"},\\n {\\\"ORTH\\\": \\\")\\\", \\\"OP\\\": \\\"?\\\"}, {\\\"SHAPE\\\": \\\"dddd\\\", \\\"LENGTH\\\": 6}]\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"Depending on the formats your application needs to match, creating an extensive\\nset of rules like this is often better than training a model. It’ll produce more\\npredictable results, is much easier to modify and extend, and doesn’t require\\nany training data – only a set of test cases.\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import Matcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = Matcher(nlp.vocab)\\npattern = [{\\\"ORTH\\\": \\\"(\\\"}, {\\\"SHAPE\\\": \\\"ddd\\\"}, {\\\"ORTH\\\": \\\")\\\"}, {\\\"SHAPE\\\": \\\"ddd\\\"},\\n           {\\\"ORTH\\\": \\\"-\\\", \\\"OP\\\": \\\"?\\\"}, {\\\"SHAPE\\\": \\\"ddd\\\"}]\\nmatcher.add(\\\"PHONE_NUMBER\\\", [pattern])\\n\\ndoc = nlp(\\\"Call me at (123) 456 789 or (123) 456 789!\\\")\\nprint([t.text for t in doc])\\nmatches = matcher(doc)\\nfor match_id, start, end in matches:\\n    span = doc[start:end]\\n    print(span.text)\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"example3\",\n        children: \"Example: Hashtags and emoji on social media \"\n      }), _jsx(_components.p, {\n        children: \"Social media posts, especially tweets, can be difficult to work with. They’re\\nvery short and often contain various emoji and hashtags. By only looking at the\\nplain text, you’ll lose a lot of valuable semantic information.\"\n      }), _jsxs(_components.p, {\n        children: [\"Let’s say you’ve extracted a large sample of social media posts on a specific\\ntopic, for example posts mentioning a brand name or product. As the first step\\nof your data exploration, you want to filter out posts containing certain emoji\\nand use them to assign a general sentiment score, based on whether the expressed\\nemotion is positive or negative, e.g. 😀 or 😞. You also want to find, merge and\\nlabel hashtags like \", _jsx(InlineCode, {\n          children: \"#MondayMotivation\"\n        }), \", to be able to ignore or analyze them\\nlater.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Note on sentiment analysis\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"Ultimately, sentiment analysis is not always \", _jsx(_components.em, {\n            children: \"that\"\n          }), \" easy. In addition to the\\nemoji, you’ll also want to take specific words into account and check the\\n\", _jsx(InlineCode, {\n            children: \"subtree\"\n          }), \" for intensifiers like “very”, to increase the sentiment score. At\\nsome point, you might also want to train a sentiment model. However, the\\napproach described in this example is very useful for \", _jsx(_components.strong, {\n            children: \"bootstrapping rules to\\ncollect training data\"\n          }), \". It’s also an incredibly fast way to gather first\\ninsights into your data – with about 1 million tweets, you’d be looking at a\\nprocessing time of \", _jsx(_components.strong, {\n            children: \"under 1 minute\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"By default, spaCy’s tokenizer will split emoji into separate tokens. This means\\nthat you can create a pattern for one or more emoji tokens. Valid hashtags\\nusually consist of a \", _jsx(InlineCode, {\n          children: \"#\"\n        }), \", plus a sequence of ASCII characters with no\\nwhitespace, making them easy to match as well.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\nfrom spacy.matcher import Matcher\\n\\nnlp = English()  # We only want the tokenizer, so no need to load a pipeline\\nmatcher = Matcher(nlp.vocab)\\n\\npos_emoji = [\\\"😀\\\", \\\"😃\\\", \\\"😂\\\", \\\"🤣\\\", \\\"😊\\\", \\\"😍\\\"]  # Positive emoji\\nneg_emoji = [\\\"😞\\\", \\\"😠\\\", \\\"😩\\\", \\\"😢\\\", \\\"😭\\\", \\\"😒\\\"]  # Negative emoji\\n\\n# Add patterns to match one or more emoji tokens\\npos_patterns = [[{\\\"ORTH\\\": emoji}] for emoji in pos_emoji]\\nneg_patterns = [[{\\\"ORTH\\\": emoji}] for emoji in neg_emoji]\\n\\n# Function to label the sentiment\\ndef label_sentiment(matcher, doc, i, matches):\\n    match_id, start, end = matches[i]\\n    if doc.vocab.strings[match_id] == \\\"HAPPY\\\":  # Don't forget to get string!\\n        doc.sentiment += 0.1  # Add 0.1 for positive sentiment\\n    elif doc.vocab.strings[match_id] == \\\"SAD\\\":\\n        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment\\n\\nmatcher.add(\\\"HAPPY\\\", pos_patterns, on_match=label_sentiment)  # Add positive pattern\\nmatcher.add(\\\"SAD\\\", neg_patterns, on_match=label_sentiment)  # Add negative pattern\\n\\n# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\\nmatcher.add(\\\"HASHTAG\\\", [[{\\\"ORTH\\\": \\\"#\\\"}, {\\\"IS_ASCII\\\": True}]])\\n\\ndoc = nlp(\\\"Hello world 😀 #MondayMotivation\\\")\\nmatches = matcher(doc)\\nfor match_id, start, end in matches:\\n    string_id = doc.vocab.strings[match_id]  # Look up string ID\\n    span = doc[start:end]\\n    print(string_id, span.text)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Because the \", _jsx(InlineCode, {\n          children: \"on_match\"\n        }), \" callback receives the ID of each match, you can use the\\nsame function to handle the sentiment assignment for both the positive and\\nnegative pattern. To keep it simple, we’ll either add or subtract \", _jsx(InlineCode, {\n          children: \"0.1\"\n        }), \" points –\\nthis way, the score will also reflect combinations of emoji, even positive \", _jsx(_components.em, {\n          children: \"and\"\n        }), \"\\nnegative ones.\"]\n      }), _jsxs(_components.p, {\n        children: [\"With a library like \", _jsx(_components.a, {\n          href: \"https://github.com/bcongdon/python-emojipedia\",\n          children: \"Emojipedia\"\n        }), \",\\nwe can also retrieve a short description for each emoji – for example, 😍‘s\\nofficial title is “Smiling Face With Heart-Eyes”. Assigning it to a\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components-attributes\",\n          children: \"custom attribute\"\n        }), \" on\\nthe emoji span will make it available as \", _jsx(InlineCode, {\n          children: \"span._.emoji_desc\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"from emojipedia import Emojipedia  # Installation: pip install emojipedia\\nfrom spacy.tokens import Span  # Get the global Span object\\n\\nSpan.set_extension(\\\"emoji_desc\\\", default=None)  # Register the custom attribute\\n\\ndef label_sentiment(matcher, doc, i, matches):\\n    match_id, start, end = matches[i]\\n    if doc.vocab.strings[match_id] == \\\"HAPPY\\\":  # Don't forget to get string!\\n        doc.sentiment += 0.1  # Add 0.1 for positive sentiment\\n    elif doc.vocab.strings[match_id] == \\\"SAD\\\":\\n        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment\\n    span = doc[start:end]\\n    emoji = Emojipedia.search(span[0].text)  # Get data for emoji\\n    span._.emoji_desc = emoji.title  # Assign emoji description\\n\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"To label the hashtags, we can use a\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components-attributes\",\n          children: \"custom attribute\"\n        }), \" set\\non the respective token:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import Matcher\\nfrom spacy.tokens import Token\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = Matcher(nlp.vocab)\\n\\n# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\\nmatcher.add(\\\"HASHTAG\\\", [[{\\\"ORTH\\\": \\\"#\\\"}, {\\\"IS_ASCII\\\": True}]])\\n\\n# Register token extension\\nToken.set_extension(\\\"is_hashtag\\\", default=False)\\n\\ndoc = nlp(\\\"Hello world 😀 #MondayMotivation\\\")\\nmatches = matcher(doc)\\nhashtags = []\\nfor match_id, start, end in matches:\\n    if doc.vocab.strings[match_id] == \\\"HASHTAG\\\":\\n        hashtags.append(doc[start:end])\\nwith doc.retokenize() as retokenizer:\\n    for span in hashtags:\\n        retokenizer.merge(span)\\n        for token in span:\\n            token._.is_hashtag = True\\n\\nfor token in doc:\\n    print(token.text, token._.is_hashtag)\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-phrasematcher\",\n      children: [_jsx(_components.h2, {\n        id: \"phrasematcher\",\n        children: \"Efficient phrase matching \"\n      }), _jsxs(_components.p, {\n        children: [\"If you need to match large terminology lists, you can also use the\\n\", _jsx(_components.a, {\n          href: \"/api/phrasematcher\",\n          children: _jsx(InlineCode, {\n            children: \"PhraseMatcher\"\n          })\n        }), \" and create \", _jsx(_components.a, {\n          href: \"/api/doc\",\n          children: _jsx(InlineCode, {\n            children: \"Doc\"\n          })\n        }), \" objects\\ninstead of token patterns, which is much more efficient overall. The \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \"\\npatterns can contain single or multiple tokens.\"]\n      }), _jsx(_components.h3, {\n        id: \"adding-phrase-patterns\",\n        children: \"Adding phrase patterns \"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import PhraseMatcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = PhraseMatcher(nlp.vocab)\\nterms = [\\\"Barack Obama\\\", \\\"Angela Merkel\\\", \\\"Washington, D.C.\\\"]\\n# Only run nlp.make_doc to speed things up\\npatterns = [nlp.make_doc(text) for text in terms]\\nmatcher.add(\\\"TerminologyList\\\", patterns)\\n\\ndoc = nlp(\\\"German Chancellor Angela Merkel and US President Barack Obama \\\"\\n          \\\"converse in the Oval Office inside the White House in Washington, D.C.\\\")\\nmatches = matcher(doc)\\nfor match_id, start, end in matches:\\n    span = doc[start:end]\\n    print(span.text)\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Since spaCy is used for processing both the patterns and the text to be matched,\\nyou won’t have to worry about specific tokenization – for example, you can\\nsimply pass in \", _jsx(InlineCode, {\n          children: \"nlp(\\\"Washington, D.C.\\\")\"\n        }), \" and won’t have to write a complex token\\npattern covering the exact tokenization of the term.\"]\n      }), _jsxs(Infobox, {\n        title: \"Important note on creating patterns\",\n        variant: \"warning\",\n        children: [_jsxs(_components.p, {\n          children: [\"To create the patterns, each phrase has to be processed with the \", _jsx(InlineCode, {\n            children: \"nlp\"\n          }), \" object.\\nIf you have a trained pipeline loaded, doing this in a loop or list\\ncomprehension can easily become inefficient and slow. If you \", _jsx(_components.strong, {\n            children: \"only need the\\ntokenization and lexical attributes\"\n          }), \", you can run\\n\", _jsx(_components.a, {\n            href: \"/api/language#make_doc\",\n            children: _jsx(InlineCode, {\n              children: \"nlp.make_doc\"\n            })\n          }), \" instead, which will only run the\\ntokenizer. For an additional speed boost, you can also use the\\n\", _jsx(_components.a, {\n            href: \"/api/tokenizer#pipe\",\n            children: _jsx(InlineCode, {\n              children: \"nlp.tokenizer.pipe\"\n            })\n          }), \" method, which will process the texts\\nas a stream.\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-diff\",\n            lang: \"diff\",\n            children: \"- patterns = [nlp(term) for term in LOTS_OF_TERMS]\\n+ patterns = [nlp.make_doc(term) for term in LOTS_OF_TERMS]\\n+ patterns = list(nlp.tokenizer.pipe(LOTS_OF_TERMS))\\n\"\n          })\n        })]\n      }), _jsx(_components.h3, {\n        id: \"phrasematcher-attrs\",\n        version: \"2.1\",\n        children: \"Matching on other token attributes \"\n      }), _jsxs(_components.p, {\n        children: [\"By default, the \", _jsx(InlineCode, {\n          children: \"PhraseMatcher\"\n        }), \" will match on the verbatim token text, e.g.\\n\", _jsx(InlineCode, {\n          children: \"Token.text\"\n        }), \". By setting the \", _jsx(InlineCode, {\n          children: \"attr\"\n        }), \" argument on initialization, you can change\\n\", _jsx(_components.strong, {\n          children: \"which token attribute the matcher should use\"\n        }), \" when comparing the phrase\\npattern to the matched \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \". For example, using the attribute \", _jsx(InlineCode, {\n          children: \"LOWER\"\n        }), \" lets you\\nmatch on \", _jsx(InlineCode, {\n          children: \"Token.lower\"\n        }), \" and create case-insensitive match patterns:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\nfrom spacy.matcher import PhraseMatcher\\n\\nnlp = English()\\nmatcher = PhraseMatcher(nlp.vocab, attr=\\\"LOWER\\\")\\npatterns = [nlp.make_doc(name) for name in [\\\"Angela Merkel\\\", \\\"Barack Obama\\\"]]\\nmatcher.add(\\\"Names\\\", patterns)\\n\\ndoc = nlp(\\\"angela merkel and us president barack Obama\\\")\\nfor match_id, start, end in matcher(doc):\\n    print(\\\"Matched based on lowercase token text:\\\", doc[start:end])\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Important note on creating patterns\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"The examples here use \", _jsx(_components.a, {\n            href: \"/api/language#make_doc\",\n            children: _jsx(InlineCode, {\n              children: \"nlp.make_doc\"\n            })\n          }), \" to create \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \"\\nobject patterns as efficiently as possible and without running any of the other\\npipeline components. If the token attribute you want to match on is set by a\\npipeline component, \", _jsx(_components.strong, {\n            children: \"make sure that the pipeline component runs\"\n          }), \" when you\\ncreate the pattern. For example, to match on \", _jsx(InlineCode, {\n            children: \"POS\"\n          }), \" or \", _jsx(InlineCode, {\n            children: \"LEMMA\"\n          }), \", the pattern \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \"\\nobjects need to have part-of-speech tags set by the \", _jsx(InlineCode, {\n            children: \"tagger\"\n          }), \" or \", _jsx(InlineCode, {\n            children: \"morphologizer\"\n          }), \".\\nYou can either call the \", _jsx(InlineCode, {\n            children: \"nlp\"\n          }), \" object on your pattern texts instead of\\n\", _jsx(InlineCode, {\n            children: \"nlp.make_doc\"\n          }), \", or use \", _jsx(_components.a, {\n            href: \"/api/language#select_pipes\",\n            children: _jsx(InlineCode, {\n              children: \"nlp.select_pipes\"\n            })\n          }), \" to\\ndisable components selectively.\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Another possible use case is matching number tokens like IP addresses based on\\ntheir shape. This means that you won’t have to worry about how those strings\\nwill be tokenized and you’ll be able to find tokens and combinations of tokens\\nbased on a few examples. Here, we’re matching on the shapes \", _jsx(InlineCode, {\n          children: \"ddd.d.d.d\"\n        }), \" and\\n\", _jsx(InlineCode, {\n          children: \"ddd.ddd.d.d\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\nfrom spacy.matcher import PhraseMatcher\\n\\nnlp = English()\\nmatcher = PhraseMatcher(nlp.vocab, attr=\\\"SHAPE\\\")\\nmatcher.add(\\\"IP\\\", [nlp(\\\"127.0.0.1\\\"), nlp(\\\"127.127.0.0\\\")])\\n\\ndoc = nlp(\\\"Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.\\\")\\nfor match_id, start, end in matcher(doc):\\n    print(\\\"Matched based on token shape:\\\", doc[start:end])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"In theory, the same also works for attributes like \", _jsx(InlineCode, {\n          children: \"POS\"\n        }), \". For example, a pattern\\n\", _jsx(InlineCode, {\n          children: \"nlp(\\\"I like cats\\\")\"\n        }), \" matched based on its part-of-speech tag would return a\\nmatch for “I love dogs”. You could also match on boolean flags like \", _jsx(InlineCode, {\n          children: \"IS_PUNCT\"\n        }), \"\\nto match phrases with the same sequence of punctuation and non-punctuation\\ntokens as the pattern. But this can easily get confusing and doesn’t have much\\nof an advantage over writing one or two token patterns.\"]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-dependencymatcher\",\n      children: [_jsx(_components.h2, {\n        id: \"dependencymatcher\",\n        version: \"3\",\n        model: \"parser\",\n        children: \"Dependency Matcher \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/dependencymatcher\",\n          children: _jsx(InlineCode, {\n            children: \"DependencyMatcher\"\n          })\n        }), \" lets you match patterns within\\nthe dependency parse using\\n\", _jsx(_components.a, {\n          href: \"https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.html\",\n          children: \"Semgrex\"\n        }), \"\\noperators. It requires a model containing a parser such as the\\n\", _jsx(_components.a, {\n          href: \"/api/dependencyparser\",\n          children: _jsx(InlineCode, {\n            children: \"DependencyParser\"\n          })\n        }), \". Instead of defining a list of\\nadjacent tokens as in \", _jsx(InlineCode, {\n          children: \"Matcher\"\n        }), \" patterns, the \", _jsx(InlineCode, {\n          children: \"DependencyMatcher\"\n        }), \" patterns match\\ntokens in the dependency parse and specify the relations between them.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            title: \"Example\",\n            children: \"from spacy.matcher import DependencyMatcher\\n\\n# \\\"[subject] ... initially founded\\\"\\npattern = [\\n  # anchor token: founded\\n  {\\n    \\\"RIGHT_ID\\\": \\\"founded\\\",\\n    \\\"RIGHT_ATTRS\\\": {\\\"ORTH\\\": \\\"founded\\\"}\\n  },\\n  # founded -> subject\\n  {\\n    \\\"LEFT_ID\\\": \\\"founded\\\",\\n    \\\"REL_OP\\\": \\\">\\\",\\n    \\\"RIGHT_ID\\\": \\\"subject\\\",\\n    \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": \\\"nsubj\\\"}\\n  },\\n  # \\\"founded\\\" follows \\\"initially\\\"\\n  {\\n    \\\"LEFT_ID\\\": \\\"founded\\\",\\n    \\\"REL_OP\\\": \\\";\\\",\\n    \\\"RIGHT_ID\\\": \\\"initially\\\",\\n    \\\"RIGHT_ATTRS\\\": {\\\"ORTH\\\": \\\"initially\\\"}\\n  }\\n]\\n\\nmatcher = DependencyMatcher(nlp.vocab)\\nmatcher.add(\\\"FOUNDED\\\", [pattern])\\nmatches = matcher(doc)\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"A pattern added to the dependency matcher consists of a \", _jsx(_components.strong, {\n          children: \"list of\\ndictionaries\"\n        }), \", with each dictionary describing a \", _jsx(_components.strong, {\n          children: \"token to match\"\n        }), \" and its\\n\", _jsx(_components.strong, {\n          children: \"relation to an existing token\"\n        }), \" in the pattern. Except for the first\\ndictionary, which defines an anchor token using only \", _jsx(InlineCode, {\n          children: \"RIGHT_ID\"\n        }), \" and\\n\", _jsx(InlineCode, {\n          children: \"RIGHT_ATTRS\"\n        }), \", each pattern should have the following keys:\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Name\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"LEFT_ID\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The name of the left-hand node in the relation, which has been defined in an earlier node. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"REL_OP\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"An operator that describes how the two nodes are related. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"RIGHT_ID\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"A unique name for the right-hand node in the relation. \", _jsx(_components.del, {\n                children: \"str\"\n              })]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"RIGHT_ATTRS\"\n              })\n            }), _jsxs(_components.td, {\n              children: [\"The token attributes to match for the right-hand node in the same format as patterns provided to the regular token-based \", _jsx(_components.a, {\n                href: \"/api/matcher\",\n                children: _jsx(InlineCode, {\n                  children: \"Matcher\"\n                })\n              }), \". \", _jsx(_components.del, {\n                children: \"Dict[str, Any]\"\n              })]\n            })]\n          })]\n        })]\n      }), _jsxs(_components.p, {\n        children: [\"Each additional token added to the pattern is linked to an existing token\\n\", _jsx(InlineCode, {\n          children: \"LEFT_ID\"\n        }), \" by the relation \", _jsx(InlineCode, {\n          children: \"REL_OP\"\n        }), \". The new token is given the name \", _jsx(InlineCode, {\n          children: \"RIGHT_ID\"\n        }), \"\\nand described by the attributes \", _jsx(InlineCode, {\n          children: \"RIGHT_ATTRS\"\n        }), \".\"]\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Because the unique token \", _jsx(_components.strong, {\n            children: \"names\"\n          }), \" in \", _jsx(InlineCode, {\n            children: \"LEFT_ID\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"RIGHT_ID\"\n          }), \" are used to\\nidentify tokens, the order of the dicts in the patterns is important: a token\\nname needs to be defined as \", _jsx(InlineCode, {\n            children: \"RIGHT_ID\"\n          }), \" in one dict in the pattern \", _jsx(_components.strong, {\n            children: \"before\"\n          }), \" it\\ncan be used as \", _jsx(InlineCode, {\n            children: \"LEFT_ID\"\n          }), \" in another dict.\"]\n        })\n      }), _jsx(_components.h3, {\n        id: \"dependencymatcher-operators\",\n        children: \"Dependency matcher operators \"\n      }), _jsxs(_components.p, {\n        children: [\"The following operators are supported by the \", _jsx(InlineCode, {\n          children: \"DependencyMatcher\"\n        }), \", most of which\\ncome directly from\\n\", _jsx(_components.a, {\n          href: \"https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.html\",\n          children: \"Semgrex\"\n        }), \":\"]\n      }), _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Symbol\"\n            }), _jsx(_components.th, {\n              children: \"Description\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A < B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" is the immediate dependent of \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A > B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" is the immediate head of \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A << B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" is the dependent in a chain to \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \" following dep → head paths.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A >> B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" is the head in a chain to \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \" following head → dep paths.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A . B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" immediately precedes \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A.i == B.i - 1\"\n              }), \", and both are within the same dependency tree.\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A .* B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" precedes \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A.i < B.i\"\n              }), \", and both are within the same dependency tree \", _jsx(_components.em, {\n                children: \"(not in Semgrex)\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A ; B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" immediately follows \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A.i == B.i + 1\"\n              }), \", and both are within the same dependency tree \", _jsx(_components.em, {\n                children: \"(not in Semgrex)\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A ;* B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"A\"\n              }), \" follows \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A.i > B.i\"\n              }), \", and both are within the same dependency tree \", _jsx(_components.em, {\n                children: \"(not in Semgrex)\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A $+ B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"B\"\n              }), \" is a right immediate sibling of \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \" and \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \" have the same parent and \", _jsx(InlineCode, {\n                children: \"A.i == B.i - 1\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A $- B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"B\"\n              }), \" is a left immediate sibling of \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \" and \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \" have the same parent and \", _jsx(InlineCode, {\n                children: \"A.i == B.i + 1\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A $++ B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"B\"\n              }), \" is a right sibling of \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \" and \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \" have the same parent and \", _jsx(InlineCode, {\n                children: \"A.i < B.i\"\n              }), \".\"]\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(InlineCode, {\n                children: \"A $-- B\"\n              })\n            }), _jsxs(_components.td, {\n              children: [_jsx(InlineCode, {\n                children: \"B\"\n              }), \" is a left sibling of \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \", i.e. \", _jsx(InlineCode, {\n                children: \"A\"\n              }), \" and \", _jsx(InlineCode, {\n                children: \"B\"\n              }), \" have the same parent and \", _jsx(InlineCode, {\n                children: \"A.i > B.i\"\n              }), \".\"]\n            })]\n          })]\n        })]\n      }), _jsx(_components.h3, {\n        id: \"dependencymatcher-patterns\",\n        children: \"Designing dependency matcher patterns \"\n      }), _jsx(_components.p, {\n        children: \"Let’s say we want to find sentences describing who founded what kind of company:\"\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: _jsx(_components.em, {\n            children: \"Smith founded a healthcare company in 2005.\"\n          })\n        }), \"\\n\", _jsx(_components.li, {\n          children: _jsx(_components.em, {\n            children: \"Williams initially founded an insurance company in 1987.\"\n          })\n        }), \"\\n\", _jsx(_components.li, {\n          children: _jsx(_components.em, {\n            children: \"Lee, an experienced CEO, has founded two AI startups.\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"The dependency parse for “Smith founded a healthcare company” shows types of\\nrelations and tokens we want to match:\"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Visualizing the parse\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"The \", _jsxs(_components.a, {\n            href: \"/usage/visualizers\",\n            children: [_jsx(InlineCode, {\n              children: \"displacy\"\n            }), \" visualizer\"]\n          }), \" lets you render \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \" objects\\nand their dependency parse and part-of-speech tags:\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"import spacy\\nfrom spacy import displacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Smith founded a healthcare company\\\")\\ndisplacy.serve(doc)\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(Iframe, {\n        title: \"displaCy visualization of dependencies\",\n        src: \"/images/displacy-dep-founded.html\",\n        height: 450\n      }), _jsx(_components.p, {\n        children: \"The relations we’re interested in are:\"\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"the founder is the \", _jsx(_components.strong, {\n            children: \"subject\"\n          }), \" (\", _jsx(InlineCode, {\n            children: \"nsubj\"\n          }), \") of the token with the text \", _jsx(InlineCode, {\n            children: \"founded\"\n          })]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"the company is the \", _jsx(_components.strong, {\n            children: \"object\"\n          }), \" (\", _jsx(InlineCode, {\n            children: \"dobj\"\n          }), \") of \", _jsx(InlineCode, {\n            children: \"founded\"\n          })]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"the kind of company may be an \", _jsx(_components.strong, {\n            children: \"adjective\"\n          }), \" (\", _jsx(InlineCode, {\n            children: \"amod\"\n          }), \", not shown above) or a\\n\", _jsx(_components.strong, {\n            children: \"compound\"\n          }), \" (\", _jsx(InlineCode, {\n            children: \"compound\"\n          }), \")\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"The first step is to pick an \", _jsx(_components.strong, {\n          children: \"anchor token\"\n        }), \" for the pattern. Since it’s the\\nroot of the dependency parse, \", _jsx(InlineCode, {\n          children: \"founded\"\n        }), \" is a good choice here. It is often\\neasier to construct patterns when all dependency relation operators point from\\nthe head to the children. In this example, we’ll only use \", _jsx(InlineCode, {\n          children: \">\"\n        }), \", which connects a\\nhead to an immediate dependent as \", _jsx(InlineCode, {\n          children: \"head > child\"\n        }), \".\"]\n      }), _jsx(_components.p, {\n        children: \"The simplest dependency matcher pattern will identify and name a single token in\\nthe tree:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import DependencyMatcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = DependencyMatcher(nlp.vocab)\\npattern = [\\n  {\\n    \\\"RIGHT_ID\\\": \\\"anchor_founded\\\",       # unique name\\n    \\\"RIGHT_ATTRS\\\": {\\\"ORTH\\\": \\\"founded\\\"}  # token pattern for \\\"founded\\\"\\n  }\\n]\\nmatcher.add(\\\"FOUNDED\\\", [pattern])\\ndoc = nlp(\\\"Smith founded two companies.\\\")\\nmatches = matcher(doc)\\nprint(matches) # [(4851363122962674176, [1])]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Now that we have a named anchor token (\", _jsx(InlineCode, {\n          children: \"anchor_founded\"\n        }), \"), we can add the founder\\nas the immediate dependent (\", _jsx(InlineCode, {\n          children: \">\"\n        }), \") of \", _jsx(InlineCode, {\n          children: \"founded\"\n        }), \" with the dependency label \", _jsx(InlineCode, {\n          children: \"nsubj\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Step 1\",\n          highlight: \"8,10\",\n          children: \"pattern = [\\n    {\\n        \\\"RIGHT_ID\\\": \\\"anchor_founded\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"ORTH\\\": \\\"founded\\\"}\\n    },\\n    {\\n        \\\"LEFT_ID\\\": \\\"anchor_founded\\\",\\n        \\\"REL_OP\\\": \\\">\\\",\\n        \\\"RIGHT_ID\\\": \\\"founded_subject\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": \\\"nsubj\\\"},\\n    }\\n    # ...\\n]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The direct object (\", _jsx(InlineCode, {\n          children: \"dobj\"\n        }), \") is added in the same way:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Step 2\",\n          children: \"pattern = [\\n    #...\\n    {\\n        \\\"LEFT_ID\\\": \\\"anchor_founded\\\",\\n        \\\"REL_OP\\\": \\\">\\\",\\n        \\\"RIGHT_ID\\\": \\\"founded_object\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": \\\"dobj\\\"},\\n    }\\n    # ...\\n]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"When the subject and object tokens are added, they are required to have names\\nunder the key \", _jsx(InlineCode, {\n          children: \"RIGHT_ID\"\n        }), \", which are allowed to be any unique string, e.g.\\n\", _jsx(InlineCode, {\n          children: \"founded_subject\"\n        }), \". These names can then be used as \", _jsx(InlineCode, {\n          children: \"LEFT_ID\"\n        }), \" to \", _jsx(_components.strong, {\n          children: \"link new\\ntokens into the pattern\"\n        }), \". For the final part of our pattern, we’ll specify that\\nthe token \", _jsx(InlineCode, {\n          children: \"founded_object\"\n        }), \" should have a modifier with the dependency relation\\n\", _jsx(InlineCode, {\n          children: \"amod\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"compound\"\n        }), \":\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          title: \"Step 3\",\n          highlight: \"7\",\n          children: \"pattern = [\\n    # ...\\n    {\\n        \\\"LEFT_ID\\\": \\\"founded_object\\\",\\n        \\\"REL_OP\\\": \\\">\\\",\\n        \\\"RIGHT_ID\\\": \\\"founded_object_modifier\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": {\\\"IN\\\": [\\\"amod\\\", \\\"compound\\\"]}},\\n    }\\n]\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"You can picture the process of creating a dependency matcher pattern as defining\\nan anchor token on the left and building up the pattern by linking tokens\\none-by-one on the right using relation operators. To create a valid pattern,\\neach new token needs to be linked to an existing token on its left. As for\\n\", _jsx(InlineCode, {\n          children: \"founded\"\n        }), \" in this example, a token may be linked to more than one token on its\\nright:\"]\n      }), _jsx(_components.img, {\n        src: \"/images/dep-match-diagram.svg\",\n        alt: \"Dependency matcher pattern\"\n      }), _jsx(_components.p, {\n        children: \"The full pattern comes together as shown in the example below:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.matcher import DependencyMatcher\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nmatcher = DependencyMatcher(nlp.vocab)\\n\\npattern = [\\n    {\\n        \\\"RIGHT_ID\\\": \\\"anchor_founded\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"ORTH\\\": \\\"founded\\\"}\\n    },\\n    {\\n        \\\"LEFT_ID\\\": \\\"anchor_founded\\\",\\n        \\\"REL_OP\\\": \\\">\\\",\\n        \\\"RIGHT_ID\\\": \\\"founded_subject\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": \\\"nsubj\\\"},\\n    },\\n    {\\n        \\\"LEFT_ID\\\": \\\"anchor_founded\\\",\\n        \\\"REL_OP\\\": \\\">\\\",\\n        \\\"RIGHT_ID\\\": \\\"founded_object\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": \\\"dobj\\\"},\\n    },\\n    {\\n        \\\"LEFT_ID\\\": \\\"founded_object\\\",\\n        \\\"REL_OP\\\": \\\">\\\",\\n        \\\"RIGHT_ID\\\": \\\"founded_object_modifier\\\",\\n        \\\"RIGHT_ATTRS\\\": {\\\"DEP\\\": {\\\"IN\\\": [\\\"amod\\\", \\\"compound\\\"]}},\\n    }\\n]\\n\\nmatcher.add(\\\"FOUNDED\\\", [pattern])\\ndoc = nlp(\\\"Lee, an experienced CEO, has founded two AI startups.\\\")\\nmatches = matcher(doc)\\n\\nprint(matches) # [(4851363122962674176, [6, 0, 10, 9])]\\n# Each token_id corresponds to one pattern dict\\nmatch_id, token_ids = matches[0]\\nfor i in range(len(token_ids)):\\n    print(pattern[i][\\\"RIGHT_ID\\\"] + \\\":\\\", doc[token_ids[i]].text)\\n\"\n        })\n      }), _jsxs(Infobox, {\n        title: \"Important note on speed\",\n        variant: \"warning\",\n        children: [_jsxs(_components.p, {\n          children: [\"The dependency matcher may be slow when token patterns can potentially match\\nmany tokens in the sentence or when relation operators allow longer paths in the\\ndependency parse, e.g. \", _jsx(InlineCode, {\n            children: \"<<\"\n          }), \", \", _jsx(InlineCode, {\n            children: \">>\"\n          }), \", \", _jsx(InlineCode, {\n            children: \".*\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \";*\"\n          }), \".\"]\n        }), _jsxs(_components.p, {\n          children: [\"To improve the matcher speed, try to make your token patterns and operators as\\nspecific as possible. For example, use \", _jsx(InlineCode, {\n            children: \">\"\n          }), \" instead of \", _jsx(InlineCode, {\n            children: \">>\"\n          }), \" if possible and use\\ntoken patterns that include dependency labels and other token attributes instead\\nof patterns such as \", _jsx(InlineCode, {\n            children: \"{}\"\n          }), \" that match any token in the sentence.\"]\n        })]\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-entityruler\",\n      children: [_jsx(_components.h2, {\n        id: \"entityruler\",\n        version: \"2.1\",\n        children: \"Rule-based entity recognition \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/entityruler\",\n          children: _jsx(InlineCode, {\n            children: \"EntityRuler\"\n          })\n        }), \" is a component that lets you add named\\nentities based on pattern dictionaries, which makes it easy to combine\\nrule-based and statistical named entity recognition for even more powerful\\npipelines.\"]\n      }), _jsx(_components.h3, {\n        id: \"entityruler-patterns\",\n        children: \"Entity Patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"Entity patterns are dictionaries with two keys: \", _jsx(InlineCode, {\n          children: \"\\\"label\\\"\"\n        }), \", specifying the label\\nto assign to the entity if the pattern is matched, and \", _jsx(InlineCode, {\n          children: \"\\\"pattern\\\"\"\n        }), \", the match\\npattern. The entity ruler accepts two types of patterns:\"]\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.strong, {\n              children: \"Phrase patterns\"\n            }), \" for exact string matches (string).\"]\n          }), \"\\n\", _jsx(_components.pre, {\n            children: _jsx(_components.code, {\n              className: \"language-python\",\n              lang: \"python\",\n              children: \"{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"}\\n\"\n            })\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.strong, {\n              children: \"Token patterns\"\n            }), \" with one dictionary describing one token (list).\"]\n          }), \"\\n\", _jsx(_components.pre, {\n            children: _jsx(_components.code, {\n              className: \"language-python\",\n              lang: \"python\",\n              children: \"{\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}]}\\n\"\n            })\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.h3, {\n        id: \"entityruler-usage\",\n        children: \"Using the entity ruler \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/entityruler\",\n          children: _jsx(InlineCode, {\n            children: \"EntityRuler\"\n          })\n        }), \" is a pipeline component that’s typically\\nadded via \", _jsx(_components.a, {\n          href: \"/api/language#add_pipe\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.add_pipe\"\n          })\n        }), \". When the \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object is\\ncalled on a text, it will find matches in the \", _jsx(InlineCode, {\n          children: \"doc\"\n        }), \" and add them as entities to\\nthe \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \", using the specified pattern label as the entity label. If any\\nmatches were to overlap, the pattern matching most tokens takes priority. If\\nthey also happen to be equally long, then the match occurring first in the \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \"\\nis chosen.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\n\\nnlp = English()\\nruler = nlp.add_pipe(\\\"entity_ruler\\\")\\npatterns = [{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"},\\n            {\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}]}]\\nruler.add_patterns(patterns)\\n\\ndoc = nlp(\\\"Apple is opening its first big office in San Francisco.\\\")\\nprint([(ent.text, ent.label_) for ent in doc.ents])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The entity ruler is designed to integrate with spaCy’s existing pipeline\\ncomponents and enhance the named entity recognizer. If it’s added \", _jsxs(_components.strong, {\n          children: [\"before the\\n\", _jsx(InlineCode, {\n            children: \"\\\"ner\\\"\"\n          }), \" component\"]\n        }), \", the entity recognizer will respect the existing entity\\nspans and adjust its predictions around it. This can significantly improve\\naccuracy in some cases. If it’s added \", _jsxs(_components.strong, {\n          children: [\"after the \", _jsx(InlineCode, {\n            children: \"\\\"ner\\\"\"\n          }), \" component\"]\n        }), \", the\\nentity ruler will only add spans to the \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \" if they don’t overlap with\\nexisting entities predicted by the model. To overwrite overlapping entities, you\\ncan set \", _jsx(InlineCode, {\n          children: \"overwrite_ents=True\"\n        }), \" on initialization.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\nruler = nlp.add_pipe(\\\"entity_ruler\\\")\\npatterns = [{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"MyCorp Inc.\\\"}]\\nruler.add_patterns(patterns)\\n\\ndoc = nlp(\\\"MyCorp Inc. is a company in the U.S.\\\")\\nprint([(ent.text, ent.label_) for ent in doc.ents])\\n\"\n        })\n      }), _jsx(_components.h4, {\n        id: \"entityruler-pattern-validation\",\n        version: \"2.1.8\",\n        children: \"Validating and debugging EntityRuler patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"The entity ruler can validate patterns against a JSON schema with the config\\nsetting \", _jsx(InlineCode, {\n          children: \"\\\"validate\\\"\"\n        }), \". See details under\\n\", _jsx(_components.a, {\n          href: \"#pattern-validation\",\n          children: \"Validating and debugging patterns\"\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"ruler = nlp.add_pipe(\\\"entity_ruler\\\", config={\\\"validate\\\": True})\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"entityruler-ent-ids\",\n        version: \"2.2.2\",\n        children: \"Adding IDs to patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/entityruler\",\n          children: _jsx(InlineCode, {\n            children: \"EntityRuler\"\n          })\n        }), \" can also accept an \", _jsx(InlineCode, {\n          children: \"id\"\n        }), \" attribute for each\\npattern. Using the \", _jsx(InlineCode, {\n          children: \"id\"\n        }), \" attribute allows multiple patterns to be associated with\\nthe same entity.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"from spacy.lang.en import English\\n\\nnlp = English()\\nruler = nlp.add_pipe(\\\"entity_ruler\\\")\\npatterns = [{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\", \\\"id\\\": \\\"apple\\\"},\\n            {\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}], \\\"id\\\": \\\"san-francisco\\\"},\\n            {\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"fran\\\"}], \\\"id\\\": \\\"san-francisco\\\"}]\\nruler.add_patterns(patterns)\\n\\ndoc1 = nlp(\\\"Apple is opening its first big office in San Francisco.\\\")\\nprint([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])\\n\\ndoc2 = nlp(\\\"Apple is opening its first big office in San Fran.\\\")\\nprint([(ent.text, ent.label_, ent.ent_id_) for ent in doc2.ents])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"If the \", _jsx(InlineCode, {\n          children: \"id\"\n        }), \" attribute is included in the \", _jsx(_components.a, {\n          href: \"/api/entityruler\",\n          children: _jsx(InlineCode, {\n            children: \"EntityRuler\"\n          })\n        }), \"\\npatterns, the \", _jsx(InlineCode, {\n          children: \"ent_id_\"\n        }), \" property of the matched entity is set to the \", _jsx(InlineCode, {\n          children: \"id\"\n        }), \" given\\nin the patterns. So in the example above it’s easy to identify that “San\\nFrancisco” and “San Fran” are both the same entity.\"]\n      }), _jsx(_components.h3, {\n        id: \"entityruler-files\",\n        children: \"Using pattern files \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/entityruler#to_disk\",\n          children: _jsx(InlineCode, {\n            children: \"to_disk\"\n          })\n        }), \" and\\n\", _jsx(_components.a, {\n          href: \"/api/entityruler#from_disk\",\n          children: _jsx(InlineCode, {\n            children: \"from_disk\"\n          })\n        }), \" let you save and load patterns to and\\nfrom JSONL (newline-delimited JSON) files, containing one pattern object per\\nline.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-json\",\n          lang: \"json\",\n          title: \"patterns.jsonl\",\n          children: \"{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"}\\n{\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}]}\\n\"\n        })\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"ruler.to_disk(\\\"./patterns.jsonl\\\")\\nnew_ruler = nlp.add_pipe(\\\"entity_ruler\\\").from_disk(\\\"./patterns.jsonl\\\")\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Integration with Prodigy\",\n        children: _jsxs(_components.p, {\n          children: [\"If you’re using the \", _jsx(_components.a, {\n            href: \"https://prodi.gy\",\n            children: \"Prodigy\"\n          }), \" annotation tool, you might\\nrecognize these pattern files from bootstrapping your named entity and text\\nclassification labelling. The patterns for the \", _jsx(InlineCode, {\n            children: \"EntityRuler\"\n          }), \" follow the same\\nsyntax, so you can use your existing Prodigy pattern files in spaCy, and vice\\nversa.\"]\n        })\n      }), _jsxs(_components.p, {\n        children: [\"When you save out an \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object that has an \", _jsx(InlineCode, {\n          children: \"EntityRuler\"\n        }), \" added to its\\npipeline, its patterns are automatically exported to the pipeline directory:\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"nlp = spacy.load(\\\"en_core_web_sm\\\")\\nruler = nlp.add_pipe(\\\"entity_ruler\\\")\\nruler.add_patterns([{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"}])\\nnlp.to_disk(\\\"/path/to/pipeline\\\")\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The saved pipeline now includes the \", _jsx(InlineCode, {\n          children: \"\\\"entity_ruler\\\"\"\n        }), \" in its\\n\", _jsx(_components.a, {\n          href: \"/api/data-formats#config\",\n          children: _jsx(InlineCode, {\n            children: \"config.cfg\"\n          })\n        }), \" and the pipeline directory contains a\\nfile \", _jsx(InlineCode, {\n          children: \"patterns.jsonl\"\n        }), \" with the patterns. When you load the pipeline back in, all\\npipeline components will be restored and deserialized – including the entity\\nruler. This lets you ship powerful pipeline packages with binary weights \", _jsx(_components.em, {\n          children: \"and\"\n        }), \"\\nrules included!\"]\n      }), _jsx(_components.h3, {\n        id: \"entityruler-large-phrase-patterns\",\n        version: \"2.2.4\",\n        children: \"Using a large number of phrase patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"When using a large amount of \", _jsx(_components.strong, {\n          children: \"phrase patterns\"\n        }), \" (roughly > 10000) it’s useful\\nto understand how the \", _jsx(InlineCode, {\n          children: \"add_patterns\"\n        }), \" function of the entity ruler works. For\\neach \", _jsx(_components.strong, {\n          children: \"phrase pattern\"\n        }), \", the EntityRuler calls the nlp object to construct a doc\\nobject. This happens in case you try to add the EntityRuler at the end of an\\nexisting pipeline with, for example, a POS tagger and want to extract matches\\nbased on the pattern’s POS signature. In this case you would pass a config value\\nof \", _jsx(InlineCode, {\n          children: \"\\\"phrase_matcher_attr\\\": \\\"POS\\\"\"\n        }), \" for the entity ruler.\"]\n      }), _jsxs(_components.p, {\n        children: [\"Running the full language pipeline across every pattern in a large list scales\\nlinearly and can therefore take a long time on large amounts of phrase patterns.\\nAs of spaCy v2.2.4 the \", _jsx(InlineCode, {\n          children: \"add_patterns\"\n        }), \" function has been refactored to use\\n\", _jsx(InlineCode, {\n          children: \"nlp.pipe\"\n        }), \" on all phrase patterns resulting in about a 10x-20x speed up with\\n5,000-100,000 phrase patterns respectively. Even with this speedup (but\\nespecially if you’re using an older version) the \", _jsx(InlineCode, {\n          children: \"add_patterns\"\n        }), \" function can\\nstill take a long time. An easy workaround to make this function run faster is\\ndisabling the other language pipes while adding the phrase patterns.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"ruler = nlp.add_pipe(\\\"entity_ruler\\\")\\npatterns = [{\\\"label\\\": \\\"TEST\\\", \\\"pattern\\\": str(i)} for i in range(100000)]\\nwith nlp.select_pipes(enable=\\\"tagger\\\"):\\n    ruler.add_patterns(patterns)\\n\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-spanruler\",\n      children: [_jsx(_components.h2, {\n        id: \"spanruler\",\n        version: \"3.3.1\",\n        children: \"Rule-based span matching \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/spanruler\",\n          children: _jsx(InlineCode, {\n            children: \"SpanRuler\"\n          })\n        }), \" is a generalized version of the entity ruler\\nthat lets you add spans to \", _jsx(InlineCode, {\n          children: \"doc.spans\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \" based on pattern\\ndictionaries, which makes it easy to combine rule-based and statistical pipeline\\ncomponents.\"]\n      }), _jsx(_components.h3, {\n        id: \"spanruler-patterns\",\n        children: \"Span patterns \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"#entityruler-patterns\",\n          children: \"pattern format\"\n        }), \" is the same as for the entity ruler:\"]\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.strong, {\n              children: \"Phrase patterns\"\n            }), \" for exact string matches (string).\"]\n          }), \"\\n\", _jsx(_components.pre, {\n            children: _jsx(_components.code, {\n              className: \"language-python\",\n              lang: \"python\",\n              children: \"{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"}\\n\"\n            })\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.strong, {\n              children: \"Token patterns\"\n            }), \" with one dictionary describing one token (list).\"]\n          }), \"\\n\", _jsx(_components.pre, {\n            children: _jsx(_components.code, {\n              className: \"language-python\",\n              lang: \"python\",\n              children: \"{\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}]}\\n\"\n            })\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.h3, {\n        id: \"spanruler-usage\",\n        children: \"Using the span ruler \"\n      }), _jsxs(_components.p, {\n        children: [\"The \", _jsx(_components.a, {\n          href: \"/api/spanruler\",\n          children: _jsx(InlineCode, {\n            children: \"SpanRuler\"\n          })\n        }), \" is a pipeline component that’s typically added\\nvia \", _jsx(_components.a, {\n          href: \"/api/language#add_pipe\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.add_pipe\"\n          })\n        }), \". When the \", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object is called on\\na text, it will find matches in the \", _jsx(InlineCode, {\n          children: \"doc\"\n        }), \" and add them as spans to\\n\", _jsx(InlineCode, {\n          children: \"doc.spans[\\\"ruler\\\"]\"\n        }), \", using the specified pattern label as the entity label.\\nUnlike in \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \", overlapping matches are allowed in \", _jsx(InlineCode, {\n          children: \"doc.spans\"\n        }), \", so no\\nfiltering is required, but optional filtering and sorting can be applied to the\\nspans before they’re saved.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.blank(\\\"en\\\")\\nruler = nlp.add_pipe(\\\"span_ruler\\\")\\npatterns = [{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"},\\n            {\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}]}]\\nruler.add_patterns(patterns)\\n\\ndoc = nlp(\\\"Apple is opening its first big office in San Francisco.\\\")\\nprint([(span.text, span.label_) for span in doc.spans[\\\"ruler\\\"]])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The span ruler is designed to integrate with spaCy’s existing pipeline\\ncomponents and enhance the \", _jsx(_components.a, {\n          href: \"/api/spancat\",\n          children: \"SpanCategorizer\"\n        }), \" and\\n\", _jsx(_components.a, {\n          href: \"/api/entityrecognizer\",\n          children: \"EntityRecognizer\"\n        }), \". The \", _jsx(InlineCode, {\n          children: \"overwrite\"\n        }), \" setting determines\\nwhether the existing annotation in \", _jsx(InlineCode, {\n          children: \"doc.spans\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \" is preserved.\\nBecause overlapping entities are not allowed for \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \", the entities are\\nalways filtered, using \", _jsx(_components.a, {\n          href: \"/api/top-level#util.filter_spans\",\n          children: _jsx(InlineCode, {\n            children: \"util.filter_spans\"\n          })\n        }), \"\\nby default. See the \", _jsxs(_components.a, {\n          href: \"/api/spanruler\",\n          children: [_jsx(InlineCode, {\n            children: \"SpanRuler\"\n          }), \" API docs\"]\n        }), \" for more information\\nabout how to customize the sorting and filtering of matched spans.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\n# only annotate doc.ents, not doc.spans\\nconfig = {\\\"spans_key\\\": None, \\\"annotate_ents\\\": True, \\\"overwrite\\\": False}\\nruler = nlp.add_pipe(\\\"span_ruler\\\", config=config)\\npatterns = [{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"MyCorp Inc.\\\"}]\\nruler.add_patterns(patterns)\\n\\ndoc = nlp(\\\"MyCorp Inc. is a company in the U.S.\\\")\\nprint([(ent.text, ent.label_) for ent in doc.ents])\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"spanruler-files\",\n        children: \"Using pattern files \"\n      }), _jsxs(_components.p, {\n        children: [\"You can save patterns in a JSONL file (newline-delimited JSON) to load with\\n\", _jsx(_components.a, {\n          href: \"/api/spanruler#initialize\",\n          children: _jsx(InlineCode, {\n            children: \"SpanRuler.initialize\"\n          })\n        }), \" or\\n\", _jsx(_components.a, {\n          href: \"/api/spanruler#add_patterns\",\n          children: _jsx(InlineCode, {\n            children: \"SpanRuler.add_patterns\"\n          })\n        }), \".\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-json\",\n          lang: \"json\",\n          title: \"patterns.jsonl\",\n          children: \"{\\\"label\\\": \\\"ORG\\\", \\\"pattern\\\": \\\"Apple\\\"}\\n{\\\"label\\\": \\\"GPE\\\", \\\"pattern\\\": [{\\\"LOWER\\\": \\\"san\\\"}, {\\\"LOWER\\\": \\\"francisco\\\"}]}\\n\"\n        })\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"import srsly\\n\\npatterns = srsly.read_jsonl(\\\"patterns.jsonl\\\")\\nruler = nlp.add_pipe(\\\"span_ruler\\\")\\nruler.add_patterns(patterns)\\n\"\n        })\n      }), _jsx(Infobox, {\n        title: \"Important note\",\n        variant: \"warning\",\n        children: _jsxs(_components.p, {\n          children: [\"Unlike the entity ruler, the span ruler cannot load patterns on initialization\\nwith \", _jsx(InlineCode, {\n            children: \"SpanRuler(patterns=patterns)\"\n          }), \" or directly from a JSONL file path with\\n\", _jsx(InlineCode, {\n            children: \"SpanRuler.from_disk(jsonl_path)\"\n          }), \". Patterns should be loaded from the JSONL file\\nseparately and then added through\\n\", _jsx(_components.a, {\n            href: \"/api/spanruler#initialize%5D\",\n            children: _jsx(InlineCode, {\n              children: \"SpanRuler.initialize\"\n            })\n          }), \" or\\n\", _jsx(_components.a, {\n            href: \"/api/spanruler#add_patterns\",\n            children: _jsx(InlineCode, {\n              children: \"SpanRuler.add_patterns\"\n            })\n          }), \" as shown above.\"]\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.section, {\n      id: \"section-models-rules\",\n      children: [_jsx(_components.h2, {\n        id: \"models-rules\",\n        children: \"Combining models and rules \"\n      }), _jsx(_components.p, {\n        children: \"You can combine statistical and rule-based components in a variety of ways.\\nRule-based components can be used to improve the accuracy of statistical models,\\nby presetting tags, entities or sentence boundaries for specific tokens. The\\nstatistical models will usually respect these preset annotations, which\\nsometimes improves the accuracy of other decisions. You can also use rule-based\\ncomponents after a statistical model to correct common errors. Finally,\\nrule-based components can reference the attributes set by statistical models, in\\norder to implement more abstract logic.\"\n      }), _jsx(_components.h3, {\n        id: \"models-rules-ner\",\n        children: \"Example: Expanding named entities \"\n      }), _jsxs(_components.p, {\n        children: [\"When using a trained\\n\", _jsx(_components.a, {\n          href: \"/usage/linguistic-features/#named-entities\",\n          children: \"named entity recognition\"\n        }), \" model to\\nextract information from your texts, you may find that the predicted span only\\nincludes parts of the entity you’re looking for. Sometimes, this happens if\\nstatistical model predicts entities incorrectly. Other times, it happens if the\\nway the entity type was defined in the original training corpus doesn’t match\\nwhat you need for your application.\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Where corpora come from\"\n        }), \"\\n\", _jsx(_components.p, {\n          children: \"Corpora used to train pipelines from scratch are often produced in academia.\\nThey contain text from various sources with linguistic features labeled\\nmanually by human annotators (following a set of specific guidelines). The\\ncorpora are then distributed with evaluation data, so other researchers can\\nbenchmark their algorithms and everyone can report numbers on the same data.\\nHowever, most applications need to learn information that isn’t contained in\\nany available corpus.\"\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"For example, the corpus spaCy’s \", _jsx(_components.a, {\n          href: \"/models/en\",\n          children: \"English pipelines\"\n        }), \" were trained on\\ndefines a \", _jsx(InlineCode, {\n          children: \"PERSON\"\n        }), \" entity as just the \", _jsx(_components.strong, {\n          children: \"person name\"\n        }), \", without titles like “Mr.”\\nor “Dr.”. This makes sense, because it makes it easier to resolve the entity\\ntype back to a knowledge base. But what if your application needs the full\\nnames, \", _jsx(_components.em, {\n          children: \"including\"\n        }), \" the titles?\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Dr. Alex Smith chaired first board meeting of Acme Corp Inc.\\\")\\nprint([(ent.text, ent.label_) for ent in doc.ents])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"While you could try and teach the model a new definition of the \", _jsx(InlineCode, {\n          children: \"PERSON\"\n        }), \" entity\\nby \", _jsx(_components.a, {\n          href: \"/usage/training/#example-train-ner\",\n          children: \"updating it\"\n        }), \" with more examples of spans\\nthat include the title, this might not be the most efficient approach. The\\nexisting model was trained on over 2 million words, so in order to completely\\nchange the definition of an entity type, you might need a lot of training\\nexamples. However, if you already have the predicted \", _jsx(InlineCode, {\n          children: \"PERSON\"\n        }), \" entities, you can\\nuse a rule-based approach that checks whether they come with a title and if so,\\nexpands the entity span by one token. After all, what all titles in this example\\nhave in common is that \", _jsx(_components.em, {\n          children: \"if\"\n        }), \" they occur, they occur in the \", _jsx(_components.strong, {\n          children: \"previous token\"\n        }), \"\\nright before the person entity.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          highlight: \"9-13\",\n          children: \"from spacy.language import Language\\nfrom spacy.tokens import Span\\n\\n@Language.component(\\\"expand_person_entities\\\")\\ndef expand_person_entities(doc):\\n    new_ents = []\\n    for ent in doc.ents:\\n        # Only check for title if it's a person and not the first token\\n        if ent.label_ == \\\"PERSON\\\" and ent.start != 0:\\n            prev_token = doc[ent.start - 1]\\n            if prev_token.text in (\\\"Dr\\\", \\\"Dr.\\\", \\\"Mr\\\", \\\"Mr.\\\", \\\"Ms\\\", \\\"Ms.\\\"):\\n                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\\n                new_ents.append(new_ent)\\n            else:\\n                new_ents.append(ent)\\n        else:\\n            new_ents.append(ent)\\n    doc.ents = new_ents\\n    return doc\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"The above function takes a \", _jsx(InlineCode, {\n          children: \"Doc\"\n        }), \" object, modifies its \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \" and returns it.\\nUsing the \", _jsx(_components.a, {\n          href: \"/api/language#component\",\n          children: _jsx(InlineCode, {\n            children: \"@Language.component\"\n          })\n        }), \" decorator, we can\\nregister it as a \", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines\",\n          children: \"pipeline component\"\n        }), \" so it can run\\nautomatically when processing a text. We can use\\n\", _jsx(_components.a, {\n          href: \"/api/language#add_pipe\",\n          children: _jsx(InlineCode, {\n            children: \"nlp.add_pipe\"\n          })\n        }), \" to add it to the current pipeline.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.language import Language\\nfrom spacy.tokens import Span\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\n\\n@Language.component(\\\"expand_person_entities\\\")\\ndef expand_person_entities(doc):\\n    new_ents = []\\n    for ent in doc.ents:\\n        if ent.label_ == \\\"PERSON\\\" and ent.start != 0:\\n            prev_token = doc[ent.start - 1]\\n            if prev_token.text in (\\\"Dr\\\", \\\"Dr.\\\", \\\"Mr\\\", \\\"Mr.\\\", \\\"Ms\\\", \\\"Ms.\\\"):\\n                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\\n                new_ents.append(new_ent)\\n        else:\\n            new_ents.append(ent)\\n    doc.ents = new_ents\\n    return doc\\n\\n# Add the component after the named entity recognizer\\nnlp.add_pipe(\\\"expand_person_entities\\\", after=\\\"ner\\\")\\n\\ndoc = nlp(\\\"Dr. Alex Smith chaired first board meeting of Acme Corp Inc.\\\")\\nprint([(ent.text, ent.label_) for ent in doc.ents])\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"An alternative approach would be to use an\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines/#custom-components-attributes\",\n          children: \"extension attribute\"\n        }), \"\\nlike \", _jsx(InlineCode, {\n          children: \"._.person_title\"\n        }), \" and add it to \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" objects (which includes entity spans\\nin \", _jsx(InlineCode, {\n          children: \"doc.ents\"\n        }), \"). The advantage here is that the entity text stays intact and can\\nstill be used to look up the name in a knowledge base. The following function\\ntakes a \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" object, checks the previous token if it’s a \", _jsx(InlineCode, {\n          children: \"PERSON\"\n        }), \" entity and\\nreturns the title if one is found. The \", _jsx(InlineCode, {\n          children: \"Span.doc\"\n        }), \" attribute gives us easy access\\nto the span’s parent document.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"def get_person_title(span):\\n    if span.label_ == \\\"PERSON\\\" and span.start != 0:\\n        prev_token = span.doc[span.start - 1]\\n        if prev_token.text in (\\\"Dr\\\", \\\"Dr.\\\", \\\"Mr\\\", \\\"Mr.\\\", \\\"Ms\\\", \\\"Ms.\\\"):\\n            return prev_token.text\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"We can now use the \", _jsx(_components.a, {\n          href: \"/api/span#set_extension\",\n          children: _jsx(InlineCode, {\n            children: \"Span.set_extension\"\n          })\n        }), \" method to add\\nthe custom extension attribute \", _jsx(InlineCode, {\n          children: \"\\\"person_title\\\"\"\n        }), \", using \", _jsx(InlineCode, {\n          children: \"get_person_title\"\n        }), \" as the\\ngetter function.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.tokens import Span\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\n\\ndef get_person_title(span):\\n    if span.label_ == \\\"PERSON\\\" and span.start != 0:\\n        prev_token = span.doc[span.start - 1]\\n        if prev_token.text in (\\\"Dr\\\", \\\"Dr.\\\", \\\"Mr\\\", \\\"Mr.\\\", \\\"Ms\\\", \\\"Ms.\\\"):\\n            return prev_token.text\\n\\n# Register the Span extension as 'person_title'\\nSpan.set_extension(\\\"person_title\\\", getter=get_person_title)\\n\\ndoc = nlp(\\\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\\\")\\nprint([(ent.text, ent.label_, ent._.person_title) for ent in doc.ents])\\n\"\n        })\n      }), _jsx(_components.h3, {\n        id: \"models-rules-pos-dep\",\n        children: \"Example: Using entities, part-of-speech tags and the dependency parse \"\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Linguistic features\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"This example makes extensive use of part-of-speech tag and dependency\\nattributes and related \", _jsx(InlineCode, {\n            children: \"Doc\"\n          }), \", \", _jsx(InlineCode, {\n            children: \"Token\"\n          }), \" and \", _jsx(InlineCode, {\n            children: \"Span\"\n          }), \" methods. For an introduction\\non this, see the guide on \", _jsx(_components.a, {\n            href: \"/usage/linguistic-features/\",\n            children: \"linguistic features\"\n          }), \".\\nAlso see the label schemes in the \", _jsx(_components.a, {\n            href: \"/models\",\n            children: \"models directory\"\n          }), \" for details on\\nthe labels.\"]\n        }), \"\\n\"]\n      }), _jsxs(_components.p, {\n        children: [\"Let’s say you want to parse professional biographies and extract the person\\nnames and company names, and whether it’s a company they’re \", _jsx(_components.em, {\n          children: \"currently\"\n        }), \" working\\nat, or a \", _jsx(_components.em, {\n          children: \"previous\"\n        }), \" company. One approach could be to try and train a named\\nentity recognizer to predict \", _jsx(InlineCode, {\n          children: \"CURRENT_ORG\"\n        }), \" and \", _jsx(InlineCode, {\n          children: \"PREVIOUS_ORG\"\n        }), \" – but this\\ndistinction is very subtle and something the entity recognizer may struggle to\\nlearn. Nothing about “Acme Corp Inc.” is inherently “current” or “previous”.\"]\n      }), _jsxs(_components.p, {\n        children: [\"However, the syntax of the sentence holds some very important clues: we can\\ncheck for trigger words like “work”, whether they’re \", _jsx(_components.strong, {\n          children: \"past tense\"\n        }), \" or \", _jsx(_components.strong, {\n          children: \"present\\ntense\"\n        }), \", whether company names are attached to it and whether the person is the\\nsubject. All of this information is available in the part-of-speech tags and the\\ndependency parse.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\ndoc = nlp(\\\"Alex Smith worked at Acme Corp Inc.\\\")\\nprint([(ent.text, ent.label_) for ent in doc.ents])\\n\"\n        })\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"nsubj\"\n            }), \": Nominal subject.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"prep\"\n            }), \": Preposition.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"pobj\"\n            }), \": Object of preposition.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"NNP\"\n            }), \": Proper noun, singular.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"VBD\"\n            }), \": Verb, past tense.\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(InlineCode, {\n              children: \"IN\"\n            }), \": Conjunction, subordinating or preposition.\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.img, {\n        src: \"/images/displacy-model-rules.svg\",\n        alt: \"Visualization of dependency parse\",\n        title: \"[`spacy.displacy`](/api/top-level#displacy) visualization with `options={'fine_grained': True}` to output the fine-grained part-of-speech tags, i.e. `Token.tag_`\"\n      }), _jsxs(_components.p, {\n        children: [\"In this example, “worked” is the root of the sentence and is a past tense verb.\\nIts subject is “Alex Smith”, the person who worked. “at Acme Corp Inc.” is a\\nprepositional phrase attached to the verb “worked”. To extract this\\nrelationship, we can start by looking at the predicted \", _jsx(InlineCode, {\n          children: \"PERSON\"\n        }), \" entities, find\\ntheir heads and check whether they’re attached to a trigger word like “work”.\\nNext, we can check for prepositional phrases attached to the head and whether\\nthey contain an \", _jsx(InlineCode, {\n          children: \"ORG\"\n        }), \" entity. Finally, to determine whether the company\\naffiliation is current, we can check the head’s part-of-speech tag.\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          children: \"person_entities = [ent for ent in doc.ents if ent.label_ == \\\"PERSON\\\"]\\nfor ent in person_entities:\\n    # Because the entity is a span, we need to use its root token. The head\\n    # is the syntactic governor of the person, e.g. the verb\\n    head = ent.root.head\\n    if head.lemma_ == \\\"work\\\":\\n        # Check if the children contain a preposition\\n        preps = [token for token in head.children if token.dep_ == \\\"prep\\\"]\\n        for prep in preps:\\n            # Check if tokens part of ORG entities are in the preposition's\\n            # children, e.g. at -> Acme Corp Inc.\\n            orgs = [token for token in prep.children if token.ent_type_ == \\\"ORG\\\"]\\n            # If the verb is in past tense, the company was a previous company\\n            print({\\\"person\\\": ent, \\\"orgs\\\": orgs, \\\"past\\\": head.tag_ == \\\"VBD\\\"})\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"To apply this logic automatically when we process a text, we can add it to the\\n\", _jsx(InlineCode, {\n          children: \"nlp\"\n        }), \" object as a\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines/#custom-components\",\n          children: \"custom pipeline component\"\n        }), \". The\\nabove logic also expects that entities are merged into single tokens. spaCy\\nships with a handy built-in \", _jsx(InlineCode, {\n          children: \"merge_entities\"\n        }), \" that takes care of that. Instead of\\njust printing the result, you could also write it to\\n\", _jsx(_components.a, {\n          href: \"/usage/processing-pipelines#custom-components-attributes\",\n          children: \"custom attributes\"\n        }), \" on\\nthe entity \", _jsx(InlineCode, {\n          children: \"Span\"\n        }), \" – for example \", _jsx(InlineCode, {\n          children: \"._.orgs\"\n        }), \" or \", _jsx(InlineCode, {\n          children: \"._.prev_orgs\"\n        }), \" and\\n\", _jsx(InlineCode, {\n          children: \"._.current_orgs\"\n        }), \".\"]\n      }), _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.h4, {\n          children: \"Merging entities\"\n        }), \"\\n\", _jsxs(_components.p, {\n          children: [\"Under the hood, entities are merged using the\\n\", _jsx(_components.a, {\n            href: \"/api/doc#retokenize\",\n            children: _jsx(InlineCode, {\n              children: \"Doc.retokenize\"\n            })\n          }), \" context manager:\"]\n        }), \"\\n\", _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            className: \"language-python\",\n            lang: \"python\",\n            children: \"with doc.retokenize() as retokenizer:\\n  for ent in doc.ents:\\n      retokenizer.merge(ent)\\n\"\n          })\n        }), \"\\n\"]\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          executable: \"true\",\n          children: \"import spacy\\nfrom spacy.language import Language\\nfrom spacy import displacy\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\n\\n@Language.component(\\\"extract_person_orgs\\\")\\ndef extract_person_orgs(doc):\\n    person_entities = [ent for ent in doc.ents if ent.label_ == \\\"PERSON\\\"]\\n    for ent in person_entities:\\n        head = ent.root.head\\n        if head.lemma_ == \\\"work\\\":\\n            preps = [token for token in head.children if token.dep_ == \\\"prep\\\"]\\n            for prep in preps:\\n                orgs = [token for token in prep.children if token.ent_type_ == \\\"ORG\\\"]\\n                print({'person': ent, 'orgs': orgs, 'past': head.tag_ == \\\"VBD\\\"})\\n    return doc\\n\\n# To make the entities easier to work with, we'll merge them into single tokens\\nnlp.add_pipe(\\\"merge_entities\\\")\\nnlp.add_pipe(\\\"extract_person_orgs\\\")\\n\\ndoc = nlp(\\\"Alex Smith worked at Acme Corp Inc.\\\")\\n# If you're not in a Jupyter / IPython environment, use displacy.serve\\ndisplacy.render(doc, options={\\\"fine_grained\\\": True})\\n\"\n        })\n      }), _jsx(_components.p, {\n        children: \"If you change the sentence structure above, for example to “was working”, you’ll\\nnotice that our current logic fails and doesn’t correctly detect the company as\\na past organization. That’s because the root is a participle and the tense\\ninformation is in the attached auxiliary “was”:\"\n      }), _jsx(_components.img, {\n        src: \"/images/displacy-model-rules2.svg\",\n        alt: \"Visualization of dependency parse\"\n      }), _jsx(_components.p, {\n        children: \"To solve this, we can adjust the rules to also check for the above construction:\"\n      }), _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          lang: \"python\",\n          highlight: \"10-12\",\n          children: \"@Language.component(\\\"extract_person_orgs\\\")\\ndef extract_person_orgs(doc):\\n    person_entities = [ent for ent in doc.ents if ent.label_ == \\\"PERSON\\\"]\\n    for ent in person_entities:\\n        head = ent.root.head\\n        if head.lemma_ == \\\"work\\\":\\n            preps = [token for token in head.children if token.dep_ == \\\"prep\\\"]\\n            for prep in preps:\\n                orgs = [t for t in prep.children if t.ent_type_ == \\\"ORG\\\"]\\n                aux = [token for token in head.children if token.dep_ == \\\"aux\\\"]\\n                past_aux = any(t.tag_ == \\\"VBD\\\" for t in aux)\\n                past = head.tag_ == \\\"VBD\\\" or head.tag_ == \\\"VBG\\\" and past_aux\\n                print({'person': ent, 'orgs': orgs, 'past': past})\\n    return doc\\n\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"In your final rule-based system, you may end up with \", _jsx(_components.strong, {\n          children: \"several different code\\npaths\"\n        }), \" to cover the types of constructions that occur in your data.\"]\n      })]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{"title":"Rule-based matching","teaser":"Find phrases and tokens, and match entities","menu":[["Token Matcher","matcher"],["Phrase Matcher","phrasematcher"],["Dependency Matcher","dependencymatcher"],["Entity Ruler","entityruler"],["Span Ruler","spanruler"],["Models & Rules","models-rules"]]},"scope":{}},"sectionTitle":"Usage Documentation","theme":"blue","section":"usage","apiDetails":{"stringName":null,"baseClass":null,"trainable":null},"isIndex":false,"next":{"slug":"/usage/processing-pipelines","title":"Processing Pipelines"}},"__N_SSG":true}